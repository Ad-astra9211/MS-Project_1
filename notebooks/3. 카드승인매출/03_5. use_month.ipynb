{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "18bc46ab-10d7-4880-b2cf-66721a926cd7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when, col, to_date\n",
    "from pyspark.sql import functions as F\n",
    "from matplotlib import font_manager\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import warnings\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8321e8bf-4642-43c0-9d88-46a54de55de5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "!apt-get update -qq\n",
    "!apt-get install -y fonts-nanum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "80ce7b84-61bb-4fb3-8f0e-219a1b804a35",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "font_dirs = [\"/usr/share/fonts/truetype/nanum/\"]\n",
    "font_files = font_manager.findSystemFonts(fontpaths=font_dirs)\n",
    " \n",
    "for font_file in font_files:\n",
    "    font_manager.fontManager.addfont(font_file)\n",
    " \n",
    "plt.rc('font', family='NanumGothic')\n",
    "plt.rc('axes', unicode_minus=False)\n",
    " \n",
    "pd.Series([-1,2,3]).plot(title='í…ŒìŠ¤íŠ¸', figsize=(3,2))\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8e63dcd3-09b5-4793-923b-e9ee657386ed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ì²˜ë¦¬í•  # íƒ€ê²Ÿ í…Œì´ë¸” ê°€ì ¸ì˜¤ê¸°\n",
    "ps_df = spark.read.table(\"database_03_cache.use_month_df\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5d63f60a-f091-4c15-9c30-d90cfee768a0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 1. ê²°ì¸¡ì¹˜ ì²˜ë¦¬ & ë°ì´í„°í˜• ë³€í™˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "81917fcc-7e12-4a0a-805f-642af385ee2c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when, col, to_date\n",
    "\n",
    "#for c in ps_df.columns[2:]:\n",
    "#    ps_df = ps_df.withColumn(c, col(c).cast(\"float\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7a77cf2c-3357-4cea-931d-96b13397df9d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Databricks data profile. Run in Databricks to view."
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1.subcommand+json": {
       "baseErrorDetails": null,
       "bindings": {},
       "collapsed": false,
       "command": "%python\nif hasattr(dbutils, \"data\") and hasattr(dbutils.data, \"summarize\"):\n  # setup\n  __data_summary_display_orig = display\n  __data_summary_dfs = []\n  def __data_summary_display_new(df):\n    # add only when result is going to be table type\n    __data_summary_df_modules = [\"pandas.core.frame\", \"databricks.koalas.frame\", \"pyspark.sql.dataframe\", \"pyspark.pandas.frame\", \"pyspark.sql.connect.dataframe\", \"pyspark.sql.classic.dataframe\"]\n    if (type(df).__module__ in __data_summary_df_modules and type(df).__name__ == 'DataFrame') or isinstance(df, list):\n      __data_summary_dfs.append(df)\n  display = __data_summary_display_new\n\n  def __data_summary_user_code_fn():\n    import base64\n    exec(base64.standard_b64decode(\"ZGlzcGxheShkYXRlX2RmKQ==\").decode())\n\n  try:\n    # run user code\n    __data_summary_user_code_fn()\n\n    # run on valid tableResultIndex\n    if len(__data_summary_dfs) > 0:\n      # run summarize\n      if type(__data_summary_dfs[0]).__module__ == \"databricks.koalas.frame\":\n        # koalas dataframe\n        dbutils.data.summarize(__data_summary_dfs[0].to_spark())\n      elif type(__data_summary_dfs[0]).__module__ == \"pandas.core.frame\":\n        # pandas dataframe\n        dbutils.data.summarize(spark.createDataFrame(__data_summary_dfs[0]))\n      else:\n        dbutils.data.summarize(__data_summary_dfs[0])\n    else:\n        displayHTML(\"dataframe no longer exists. If you're using dataframe.display(), use display(dataframe) instead.\")\n\n  finally:\n    display = __data_summary_display_orig\n    del __data_summary_display_new\n    del __data_summary_display_orig\n    del __data_summary_dfs\n    del __data_summary_user_code_fn\nelse:\n  print(\"This DBR version does not support data profiles.\")",
       "commandTitle": "ë°ì´í„° í”„ë¡œí•„ 1",
       "commandType": "auto",
       "commandVersion": 0,
       "commentThread": [],
       "commentsVisible": false,
       "contentSha256Hex": null,
       "customPlotOptions": {},
       "datasetPreviewNameToCmdIdMap": {},
       "diffDeletes": [],
       "diffInserts": [],
       "displayType": "table",
       "error": null,
       "errorDetails": null,
       "errorSummary": null,
       "errorTraceType": null,
       "finishTime": 0,
       "globalVars": {},
       "guid": "",
       "height": "auto",
       "hideCommandCode": false,
       "hideCommandResult": false,
       "iPythonMetadata": null,
       "inputWidgets": {},
       "isLockedInExamMode": false,
       "latestUser": "a user",
       "latestUserId": null,
       "listResultMetadata": null,
       "metadata": {
        "byteLimit": 2048000,
        "rowLimit": 10000
       },
       "nuid": "8462e18c-fc6d-4d82-84b7-be56f5c2e54b",
       "origId": 0,
       "parentHierarchy": [],
       "pivotAggregation": null,
       "pivotColumns": null,
       "position": 4.0,
       "resultDbfsErrorMessage": null,
       "resultDbfsStatus": "INLINED_IN_TREE",
       "results": null,
       "showCommandTitle": false,
       "startTime": 0,
       "state": "input",
       "streamStates": {},
       "subcommandOptions": null,
       "submitTime": 0,
       "subtype": "tableResultSubCmd.dataSummary",
       "tableResultIndex": 0,
       "tableResultSettingsMap": {},
       "useConsistentColors": false,
       "version": "CommandV1",
       "width": "auto",
       "workflows": null,
       "xColumns": null,
       "yColumns": null
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(ps_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c40667df-d7fa-4246-b2b5-4312f87567d6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 2. ì»¬ëŸ¼ ì„¸ë¶„í™”"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dd355f98-26c8-4fae-bb3b-7f414d6adce0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### 2.0 ê¸°ê°„ í¬í•¨/ë¶ˆí¬í•¨ êµ¬ë¶„\n",
    "\n",
    "- ì „ì²´ : 48ê°œ\n",
    "- ë¶„ë¥˜ : ê¸°ê°„ í¬í•¨ëœ ì»¬ëŸ¼ (46ê°œ) / ë¶ˆí¬í•¨ëœ ì»¬ëŸ¼ (2ê°œ[ê³µí†µì¹¼ëŸ¼])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6f1722d2-eb67-4ec4-b42f-3903a11a5ca4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "ps_columns = ps_df.columns\n",
    "len(ps_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d835288e-1fca-4afc-abeb-ffb0c7420d7c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "### Step 1: ê¸°ê°„ì´ í¬í•¨ëœ ì»¬ëŸ¼ ì •ê·œí‘œí˜„ì‹\n",
    "period_pattern = re.compile(r'^(.*)_(B\\d+M|R\\d+M)$') # Bë‚˜ Rê¸°ê°„\n",
    "\n",
    "### Step 2: prefix-period ë”•ì…”ë„ˆë¦¬ ë§Œë“¤ê¸°\n",
    "prefix_period_map = {}  # ê¸°ê°„ë³„ë¡œ prefixë¥¼ ì €ì¥í•  ë”•ì…”ë„ˆë¦¬\n",
    "non_matching_cols = []  # ê¸°ê°„ì´ í¬í•¨ë˜ì§€ ì•Šì€ ì»¬ëŸ¼\n",
    "\n",
    "for col in ps_columns:\n",
    "    match = period_pattern.match(col)\n",
    "    # prefix(ex.'ì´ìš©ê¸ˆì•¡_ì‹ ìš©')ì™€ ê¸°ê°„(ex.'R12M')ì„ ë”°ë¡œ ì €ì¥\n",
    "    if match:\n",
    "        prefix = match.group(1) # ex. 'ì´ìš©ê¸ˆì•¡_ì‹ ìš©'\n",
    "        period = match.group(2) # ex. 'R12M'\n",
    "        if prefix not in prefix_period_map:\n",
    "            prefix_period_map[prefix] = {}\n",
    "        prefix_period_map[prefix][period] = col\n",
    "    else:\n",
    "        non_matching_cols.append(col)\n",
    "\n",
    "print(48 - len(non_matching_cols), prefix_period_map)\n",
    "print()\n",
    "print(len(non_matching_cols), non_matching_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "79c80f77-9299-4ba1-aeb3-92eef1792413",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### 2.1 ê¸°ê°„ í¬í•¨ ì»¬ëŸ¼\n",
    "(prefix_period_map ì— ë‹´ê²¨ì ¸ ìˆìŒ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4a3a6776-6c7b-462c-a0e1-0c3dc5fd9038",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "> **ê¸°ê°„ unique ì •ë¦¬**\n",
    "| ê¸°ê°„ì½”ë“œ   | ì˜ë¯¸                                               |\n",
    "| ------ | ------------------------------------------------ |\n",
    "| `B0M`  | **í˜„ì¬ ê¸°ì¤€ ì‹œì ** (ì˜ˆ: ë¶„ì„ ê¸°ì¤€ì´ 2025ë…„ 5ì›”ì´ë©´ â†’ 2025ë…„ 5ì›”)   |\n",
    "| `B1M`  | **1ê°œì›” ì „** ê¸°ì¤€ (ì˜ˆ: 2025ë…„ 4ì›”)                       |\n",
    "| `B2M`  | **2ê°œì›” ì „** ê¸°ì¤€ (ì˜ˆ: 2025ë…„ 3ì›”)                       |\n",
    "| `R3M`  | **ìµœê·¼ 3ê°œì›” í‰ê· ì¹˜** (ì˜ˆ: 2025ë…„ 3\\~5ì›”ì˜ í‰ê·  ì´ìš©ê¸ˆì•¡)         |\n",
    "| `R6M`  | **ìµœê·¼ 6ê°œì›” í‰ê· ì¹˜** (ì˜ˆ: 2024ë…„ 12ì›”\\~2025ë…„ 5ì›”ì˜ í‰ê·  ì´ìš©ê¸ˆì•¡) |\n",
    "| `R12M` | **ìµœê·¼ 12ê°œì›” í‰ê· ì¹˜** (ì˜ˆ: 2024ë…„ 6ì›”\\~2025ë…„ 5ì›”ì˜ í‰ê·  ì´ìš©ê¸ˆì•¡) |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ae910147-46dd-4ea2-a219-82a740997137",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ëª¨ë“  prefixì— ëŒ€í•´ period keyë§Œ ëª¨ì•„ì„œ setìœ¼ë¡œ ì¤‘ë³µ ì œê±°\n",
    "all_periods = set()\n",
    "\n",
    "for periods in prefix_period_map.values():\n",
    "    all_periods.update(periods.keys())\n",
    "\n",
    "# ë³´ê¸° ì¢‹ê²Œ ìˆ«ì ì •ë ¬ (ì˜ˆ: B0M, B1M, ..., R3M, R6M, R12M)\n",
    "sorted_periods = sorted(\n",
    "    list(all_periods),\n",
    "    key=lambda x: (x[0], int(re.search(r'\\d+', x).group()))\n",
    ")\n",
    "\n",
    "print(\"âœ” ì‚¬ìš©ëœ ê¸°ê°„ë“¤:\", sorted_periods)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e40b1718-b767-41f0-a871-df3c47d9ad34",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "ps_columns = [col for col in ps_columns if col not in non_matching_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b17d721f-26f0-40bc-b416-6cf7d11b101c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "period_df = ps_df.select(ps_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f61f9b6c-c790-493a-b457-2578b41dd7d4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Databricks data profile. Run in Databricks to view."
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1.subcommand+json": {
       "baseErrorDetails": null,
       "bindings": {},
       "collapsed": false,
       "command": "%python\nif hasattr(dbutils, \"data\") and hasattr(dbutils.data, \"summarize\"):\n  # setup\n  __data_summary_display_orig = display\n  __data_summary_dfs = []\n  def __data_summary_display_new(df):\n    # add only when result is going to be table type\n    __data_summary_df_modules = [\"pandas.core.frame\", \"databricks.koalas.frame\", \"pyspark.sql.dataframe\", \"pyspark.pandas.frame\", \"pyspark.sql.connect.dataframe\", \"pyspark.sql.classic.dataframe\"]\n    if (type(df).__module__ in __data_summary_df_modules and type(df).__name__ == 'DataFrame') or isinstance(df, list):\n      __data_summary_dfs.append(df)\n  display = __data_summary_display_new\n\n  def __data_summary_user_code_fn():\n    import base64\n    exec(base64.standard_b64decode(\"ZGlzcGxheShwZXJpb2RfZGYp\").decode())\n\n  try:\n    # run user code\n    __data_summary_user_code_fn()\n\n    # run on valid tableResultIndex\n    if len(__data_summary_dfs) > 0:\n      # run summarize\n      if type(__data_summary_dfs[0]).__module__ == \"databricks.koalas.frame\":\n        # koalas dataframe\n        dbutils.data.summarize(__data_summary_dfs[0].to_spark())\n      elif type(__data_summary_dfs[0]).__module__ == \"pandas.core.frame\":\n        # pandas dataframe\n        dbutils.data.summarize(spark.createDataFrame(__data_summary_dfs[0]))\n      else:\n        dbutils.data.summarize(__data_summary_dfs[0])\n    else:\n        displayHTML(\"dataframe no longer exists. If you're using dataframe.display(), use display(dataframe) instead.\")\n\n  finally:\n    display = __data_summary_display_orig\n    del __data_summary_display_new\n    del __data_summary_display_orig\n    del __data_summary_dfs\n    del __data_summary_user_code_fn\nelse:\n  print(\"This DBR version does not support data profiles.\")",
       "commandTitle": "ë°ì´í„° í”„ë¡œí•„ 1",
       "commandType": "auto",
       "commandVersion": 0,
       "commentThread": [],
       "commentsVisible": false,
       "contentSha256Hex": null,
       "customPlotOptions": {},
       "datasetPreviewNameToCmdIdMap": {},
       "diffDeletes": [],
       "diffInserts": [],
       "displayType": "table",
       "error": null,
       "errorDetails": null,
       "errorSummary": null,
       "errorTraceType": null,
       "finishTime": 0,
       "globalVars": {},
       "guid": "",
       "height": "auto",
       "hideCommandCode": false,
       "hideCommandResult": false,
       "iPythonMetadata": null,
       "inputWidgets": {},
       "isLockedInExamMode": false,
       "latestUser": "a user",
       "latestUserId": null,
       "listResultMetadata": null,
       "metadata": {
        "byteLimit": 2048000,
        "rowLimit": 10000
       },
       "nuid": "b216152d-0887-45b7-80dd-6d18874ac211",
       "origId": 0,
       "parentHierarchy": [],
       "pivotAggregation": null,
       "pivotColumns": null,
       "position": 6.970588235294114,
       "resultDbfsErrorMessage": null,
       "resultDbfsStatus": "INLINED_IN_TREE",
       "results": null,
       "showCommandTitle": false,
       "startTime": 0,
       "state": "input",
       "streamStates": {},
       "subcommandOptions": null,
       "submitTime": 0,
       "subtype": "tableResultSubCmd.dataSummary",
       "tableResultIndex": 0,
       "tableResultSettingsMap": {},
       "useConsistentColors": false,
       "version": "CommandV1",
       "width": "auto",
       "workflows": null,
       "xColumns": null,
       "yColumns": null
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(period_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3c329c81-0723-4fb6-91b4-81b257785114",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### ğŸ’¡ ë°ì´í„° ì¶”ì¶œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7217c7da-422b-4a7f-845a-2fb1caa80f7a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "### ë°ì´í„° ë² ì´ìŠ¤ ì‚¬ìš© ì„¤ì •\n",
    "ps_period_df = period_df.cache()\n",
    "spark.sql(\"USE database_03_cache\")\n",
    "print(\"í˜„ì¬ ë°ì´í„°ë² ì´ìŠ¤ë¥¼ 'database_03_cache'ë¡œ ì„¤ì •\")\n",
    "\n",
    "### ì €ì¥í•  í…Œì´ë¸” ê°’ ì…ë ¥\n",
    "# ps_period_df.write.mode(\"overwrite\").saveAsTable(\"_period_df\")\n",
    "print(\"ì´ìš©ê¸ˆì•¡(ê¸°ê°„ í¬í•¨) ê´€ë ¨ í…Œì´ë¸” ìƒì„± ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1d992609-6a9f-4d67-85c7-ca85d1aa0f78",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### ğŸ’¡ë‹¤ì‹œ ë¶ˆëŸ¬ì˜¤ê¸°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "28e94960-c195-420a-ab5e-bfc09c1b2226",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "### ì €ì¥í•œ í…Œì´ë¸” ê°’ ì…ë ¥\n",
    "# ps_period_df = spark.read.table(\"database_03_cache.ps_period_df\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4ec28720-219c-452d-a486-a444e44ea449",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Databricks data profile. Run in Databricks to view."
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1.subcommand+json": {
       "baseErrorDetails": null,
       "bindings": {},
       "collapsed": false,
       "command": "%python\nif hasattr(dbutils, \"data\") and hasattr(dbutils.data, \"summarize\"):\n  # setup\n  __data_summary_display_orig = display\n  __data_summary_dfs = []\n  def __data_summary_display_new(df):\n    # add only when result is going to be table type\n    __data_summary_df_modules = [\"pandas.core.frame\", \"databricks.koalas.frame\", \"pyspark.sql.dataframe\", \"pyspark.pandas.frame\", \"pyspark.sql.connect.dataframe\", \"pyspark.sql.classic.dataframe\"]\n    if (type(df).__module__ in __data_summary_df_modules and type(df).__name__ == 'DataFrame') or isinstance(df, list):\n      __data_summary_dfs.append(df)\n  display = __data_summary_display_new\n\n  def __data_summary_user_code_fn():\n    import base64\n    exec(base64.standard_b64decode(\"ZGlzcGxheShwc19wZXJpb2RfZGYp\").decode())\n\n  try:\n    # run user code\n    __data_summary_user_code_fn()\n\n    # run on valid tableResultIndex\n    if len(__data_summary_dfs) > 0:\n      # run summarize\n      if type(__data_summary_dfs[0]).__module__ == \"databricks.koalas.frame\":\n        # koalas dataframe\n        dbutils.data.summarize(__data_summary_dfs[0].to_spark())\n      elif type(__data_summary_dfs[0]).__module__ == \"pandas.core.frame\":\n        # pandas dataframe\n        dbutils.data.summarize(spark.createDataFrame(__data_summary_dfs[0]))\n      else:\n        dbutils.data.summarize(__data_summary_dfs[0])\n    else:\n        displayHTML(\"dataframe no longer exists. If you're using dataframe.display(), use display(dataframe) instead.\")\n\n  finally:\n    display = __data_summary_display_orig\n    del __data_summary_display_new\n    del __data_summary_display_orig\n    del __data_summary_dfs\n    del __data_summary_user_code_fn\nelse:\n  print(\"This DBR version does not support data profiles.\")",
       "commandTitle": "ë°ì´í„° í”„ë¡œí•„ 1",
       "commandType": "auto",
       "commandVersion": 0,
       "commentThread": [],
       "commentsVisible": false,
       "contentSha256Hex": null,
       "customPlotOptions": {},
       "datasetPreviewNameToCmdIdMap": {},
       "diffDeletes": [],
       "diffInserts": [],
       "displayType": "table",
       "error": null,
       "errorDetails": null,
       "errorSummary": null,
       "errorTraceType": null,
       "finishTime": 0,
       "globalVars": {},
       "guid": "",
       "height": "auto",
       "hideCommandCode": false,
       "hideCommandResult": false,
       "iPythonMetadata": null,
       "inputWidgets": {},
       "isLockedInExamMode": false,
       "latestUser": "a user",
       "latestUserId": null,
       "listResultMetadata": null,
       "metadata": {
        "byteLimit": 2048000,
        "rowLimit": 10000
       },
       "nuid": "5b40b8c9-1434-41fd-93a5-ddbffd22221d",
       "origId": 0,
       "parentHierarchy": [],
       "pivotAggregation": null,
       "pivotColumns": null,
       "position": 1.0,
       "resultDbfsErrorMessage": null,
       "resultDbfsStatus": "INLINED_IN_TREE",
       "results": null,
       "showCommandTitle": false,
       "startTime": 0,
       "state": "input",
       "streamStates": {},
       "subcommandOptions": null,
       "submitTime": 0,
       "subtype": "tableResultSubCmd.dataSummary",
       "tableResultIndex": 0,
       "tableResultSettingsMap": {},
       "useConsistentColors": false,
       "version": "CommandV1",
       "width": "auto",
       "workflows": null,
       "xColumns": null,
       "yColumns": null
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(ps_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "815f87a6-b5e8-4f58-b50c-7593b2ec0ee9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "ps_df = spark.read.table(\"database_03_cache.use_month_df\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f981fa1d-7116-42c7-9e40-5ea66e46d384",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "ps_period_numeric_cols = [c for c in ps_df.columns if c not in ['ê¸°ì¤€ë…„ì›”', 'ë°œê¸‰íšŒì›ë²ˆí˜¸']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ac94ba28-d9e1-496d-84ea-c6ee71ced80f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ì „ì²´ ë ˆì½”ë“œ ìˆ˜\n",
    "total_count = ps_df.count()\n",
    "total_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "94bb5bfa-edbe-4751-9d0d-9981e567cfb9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### í¬ì†Œ(sparse)í•œ ì»¬ëŸ¼ ì²˜ë¦¬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8b43b35e-1bca-4a5e-afc4-c0ac71b190d2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, when, sum as Fsum\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "# 1. ì „ì²´ ìˆ˜ì¹˜í˜• ì»¬ëŸ¼ ì¤‘ 0 ë¹„ìœ¨ ê¸°ì¤€ìœ¼ë¡œ ë‚˜ëˆ„ê¸°\n",
    "zero_ratio_cols = []     # 0ì´ 70% ì´ìƒ, 100% ë¯¸ë§Œì¸ ì»¬ëŸ¼\n",
    "zero_only_cols = []      # ì „ë¶€ 0ì¸ ì»¬ëŸ¼\n",
    "\n",
    "\n",
    "# ì „ë¶€ 0ì¸ ì»¬ëŸ¼ íƒì§€\n",
    "sum_df = ps_df.select([Fsum(col(c)).alias(c) for c in ps_period_numeric_cols])\n",
    "for c in sum_df.columns:\n",
    "    if sum_df.select(c).collect()[0][0] == 0:\n",
    "        zero_only_cols.append(c)\n",
    "print(len(zero_only_cols))\n",
    "\n",
    "# ì¼ë¶€ë§Œ 0ì¸ í¬ì†Œ ì»¬ëŸ¼ íƒì§€\n",
    "for c in ps_period_numeric_cols:\n",
    "    if c not in zero_only_cols:\n",
    "        zero_count = ps_df.filter(col(c) == 0).count()\n",
    "        ratio = zero_count / total_count\n",
    "        if 0.7 <= ratio < 1.0:\n",
    "            zero_ratio_cols.append(c)\n",
    "print(len(zero_ratio_cols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "45479e9f-bdc1-4141-9116-345c330abb56",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 2. ì „ë¶€ 0ì¸ ì»¬ëŸ¼ ì œê±°\n",
    "ap_df = ps_df.drop(*zero_only_cols)\n",
    "print(len(ap_df.columns))\n",
    "\n",
    "# 3. ì´ì§„í™”ëœ ì»¬ëŸ¼ ì¶”ê°€\n",
    "for zcol in zero_ratio_cols:\n",
    "    ap_df = ap_df.withColumn(f\"{zcol}_ì‚¬ìš©ì—¬ë¶€\", when(col(zcol) > 0, 1).otherwise(0))\n",
    "print(len(ap_df.columns))\n",
    "\n",
    "# 4. ë²¡í„°í™”\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[f\"{z}_ì‚¬ìš©ì—¬ë¶€\" for z in zero_ratio_cols],\n",
    "    outputCol=\"ì‚¬ìš©íŒ¨í„´ë²¡í„°\"\n",
    ")\n",
    "ap_df = assembler.transform(ap_df)\n",
    "print(len(ap_df.columns))\n",
    "\n",
    "# 5. ì´ì§„í™”ëœ ì»¬ëŸ¼ ì œê±°í•˜ê³  ë²¡í„°ë§Œ ë‚¨ê¹€\n",
    "ap_df = ap_df.drop(*[f\"{z}_ì‚¬ìš©ì—¬ë¶€\" for z in zero_ratio_cols])\n",
    "print(len(ap_df.columns))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8f33938d-a3e0-4447-99ac-cec970e3f965",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(ap_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d0f6214f-af37-431b-91ce-08a61317413d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(len(ap_df.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7895ff8c-0f09-4b94-b9ed-bee4afe11b30",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### ì´ìƒì¹˜ ì²˜ë¦¬/ìŠ¤ì¼€ì¼ë§ - ë¡œê·¸ ë³€í™˜\n",
    "í…Œì´ë¸” ë¶„ì„ì—ì„œ box plot í™•ì¸ ê²°ê³¼ ëŒ€ë¶€ë¶„ positive skewë¡œ ì´ìƒì¹˜ ë§ì€ ë¶„í¬ì„. ë”°ë¼ì„œ ë¡œê·¸ ë³€í™˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3fa55e20-32d4-415d-8972-12fff9832457",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ìˆ˜ì¹˜í˜• ì»¬ëŸ¼ - ì»¬ëŸ¼ ë°”ë€Œì—ˆìœ¼ë¯€ë¡œ, ì—…ë°ì´íŠ¸\n",
    "ap_numeric_cols = [c for c, t in ap_df.dtypes if t in (\"int\", \"bigint\", \"float\", \"double\")]\n",
    "\n",
    "len(ap_numeric_cols)\n",
    "ap_numeric_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "db77e587-9de4-459f-bb64-285c120e855a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Databricks data profile. Run in Databricks to view."
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1.subcommand+json": {
       "baseErrorDetails": null,
       "bindings": {},
       "collapsed": false,
       "command": "%python\nif hasattr(dbutils, \"data\") and hasattr(dbutils.data, \"summarize\"):\n  # setup\n  __data_summary_display_orig = display\n  __data_summary_dfs = []\n  def __data_summary_display_new(df):\n    # add only when result is going to be table type\n    __data_summary_df_modules = [\"pandas.core.frame\", \"databricks.koalas.frame\", \"pyspark.sql.dataframe\", \"pyspark.pandas.frame\", \"pyspark.sql.connect.dataframe\", \"pyspark.sql.classic.dataframe\"]\n    if (type(df).__module__ in __data_summary_df_modules and type(df).__name__ == 'DataFrame') or isinstance(df, list):\n      __data_summary_dfs.append(df)\n  display = __data_summary_display_new\n\n  def __data_summary_user_code_fn():\n    import base64\n    exec(base64.standard_b64decode(\"ZnJvbSBweXNwYXJrLnNxbCBpbXBvcnQgU3BhcmtTZXNzaW9uCmZyb20gcHlzcGFyay5zcWwuZnVuY3Rpb25zIGltcG9ydCBsb2cxcCwgY29sCmZyb20gcHlzcGFyay5zcWwudHlwZXMgaW1wb3J0IEludGVnZXJUeXBlLCBMb25nVHlwZSwgRmxvYXRUeXBlLCBEb3VibGVUeXBlLCBEYXRlVHlwZSwgU3RyaW5nVHlwZSAjIO2VhOyalO2VnCDtg4DsnoUg7J6E7Y+s7Yq4CgojIDEuIOybkOuzuCDthYzsnbTruJQg7J296riwCnBzX2RmID0gc3BhcmsucmVhZC50YWJsZSgiZGF0YWJhc2VfMDNfY2FjaGUudXNlX21vbnRoX2RmIikKCiMgMi4g66Gc6re4IOuzgO2ZmOydhCDsoIHsmqntlaAg7Iir7J6Q7ZiVIOy7rOufvCDrpqzsiqTtirgg7ZWE7YSw66eBCiMgJ+q4sOykgOuFhOyblCfqs7wgJ+uwnOq4ie2ajOybkOuyiO2YuCfrpbwg7KCc7Jm47ZWY6rOgLCDsi6TsoJwg642w7J207YSwIO2DgOyeheydtCDsiKvsnpDtmJXsnbgg7Lus65+866eMIOyEoO2DnQpudW1lcmljX2NvbHNfdG9fdHJhbnNmb3JtID0gW10KZm9yIGZpZWxkIGluIHBzX2RmLnNjaGVtYToKICAgICMg7KCc7Jm47ZWgIOy7rOufvCDsnbTrpoQg66qp66GdCiAgICBleGNsdWRlX2NvbHMgPSBbJ+q4sOykgOuFhOyblCcsICfrsJzquIntmozsm5DrsojtmLgnXQoKICAgICMg7Lus65+8IOydtOumhOydtCDsoJzsmbgg66qp66Gd7JeQIOyXhuqzoCwg642w7J207YSwIO2DgOyeheydtCDsiKvsnpDtmJXsnbgg6rK97JqwCiAgICBpZiBmaWVsZC5uYW1lIG5vdCBpbiBleGNsdWRlX2NvbHMgYW5kIFwKICAgICAgIGlzaW5zdGFuY2UoZmllbGQuZGF0YVR5cGUsIChJbnRlZ2VyVHlwZSwgTG9uZ1R5cGUsIEZsb2F0VHlwZSwgRG91YmxlVHlwZSkpOgogICAgICAgIG51bWVyaWNfY29sc190b190cmFuc2Zvcm0uYXBwZW5kKGZpZWxkLm5hbWUpCgojIDMuIOuhnOq3uCDrs4DtmZgg7KCB7JqpCnBzX3BlcmlvZF9kZiA9IHBzX2RmCgpmb3IgY29sX25hbWUgaW4gbnVtZXJpY19jb2xzX3RvX3RyYW5zZm9ybToKICAgIHBzX3BlcmlvZF9kZiA9IHBzX3BlcmlvZF9kZi53aXRoQ29sdW1uKGNvbF9uYW1lLCBsb2cxcChjb2woY29sX25hbWUpKSkKCiMgNC4g6rKw6rO8IO2ZleyduApwcmludCgiLS0tIOuzgO2ZmOuQnCBEYXRhRnJhbWUg7Iqk7YKk66eIICjtmZXsnbjsmqkpIC0tLSIpCnBzX3BlcmlvZF9kZi5wcmludFNjaGVtYSgpICMg67OA7ZmY65CcIOy7rOufvOydmCDtg4DsnoXsnbQgRG91YmxlVHlwZeycvOuhnCDrs4Dqsr3rkJjsl4jripTsp4Ag7ZmV7J24CgpwcmludCgiLS0tIOuzgO2ZmOuQnCBEYXRhRnJhbWUgKOydvOu2gCkgLS0tIikKZGlzcGxheShwc19wZXJpb2RfZGYpICMgRGF0YWJyaWNrc+ydmCBkaXNwbGF5IO2VqOyImOuhnCDqsrDqs7wg7ZmV7J24\").decode())\n\n  try:\n    # run user code\n    __data_summary_user_code_fn()\n\n    # run on valid tableResultIndex\n    if len(__data_summary_dfs) > 0:\n      # run summarize\n      if type(__data_summary_dfs[0]).__module__ == \"databricks.koalas.frame\":\n        # koalas dataframe\n        dbutils.data.summarize(__data_summary_dfs[0].to_spark())\n      elif type(__data_summary_dfs[0]).__module__ == \"pandas.core.frame\":\n        # pandas dataframe\n        dbutils.data.summarize(spark.createDataFrame(__data_summary_dfs[0]))\n      else:\n        dbutils.data.summarize(__data_summary_dfs[0])\n    else:\n        displayHTML(\"dataframe no longer exists. If you're using dataframe.display(), use display(dataframe) instead.\")\n\n  finally:\n    display = __data_summary_display_orig\n    del __data_summary_display_new\n    del __data_summary_display_orig\n    del __data_summary_dfs\n    del __data_summary_user_code_fn\nelse:\n  print(\"This DBR version does not support data profiles.\")",
       "commandTitle": "ë°ì´í„° í”„ë¡œí•„ 1",
       "commandType": "auto",
       "commandVersion": 0,
       "commentThread": [],
       "commentsVisible": false,
       "contentSha256Hex": null,
       "customPlotOptions": {},
       "datasetPreviewNameToCmdIdMap": {},
       "diffDeletes": [],
       "diffInserts": [],
       "displayType": "table",
       "error": null,
       "errorDetails": null,
       "errorSummary": null,
       "errorTraceType": null,
       "finishTime": 0,
       "globalVars": {},
       "guid": "",
       "height": "auto",
       "hideCommandCode": false,
       "hideCommandResult": false,
       "iPythonMetadata": null,
       "inputWidgets": {},
       "isLockedInExamMode": false,
       "latestUser": "a user",
       "latestUserId": null,
       "listResultMetadata": null,
       "metadata": {
        "byteLimit": 2048000,
        "rowLimit": 10000
       },
       "nuid": "da79156a-929a-486e-bacd-ae83663bf3f7",
       "origId": 0,
       "parentHierarchy": [],
       "pivotAggregation": null,
       "pivotColumns": null,
       "position": 7.0299719887955145,
       "resultDbfsErrorMessage": null,
       "resultDbfsStatus": "INLINED_IN_TREE",
       "results": null,
       "showCommandTitle": false,
       "startTime": 0,
       "state": "input",
       "streamStates": {},
       "subcommandOptions": null,
       "submitTime": 0,
       "subtype": "tableResultSubCmd.dataSummary",
       "tableResultIndex": 0,
       "tableResultSettingsMap": {},
       "useConsistentColors": false,
       "version": "CommandV1",
       "width": "auto",
       "workflows": null,
       "xColumns": null,
       "yColumns": null
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# from pyspark.sql.functions import log1p, col\n",
    "# # ë¡œê·¸ ë³€í™˜\n",
    "# for col_name in ap_numeric_cols:\n",
    "#     ap_df = ap_df.withColumn(col_name, log1p(col(col_name)))\n",
    "\n",
    "from pyspark.sql.functions import log1p, col, when\n",
    "\n",
    "# ë¡œê·¸ ë³€í™˜ (ìŒìˆ˜ê°’ì€ 0ìœ¼ë¡œ ëŒ€ì²´ í›„ log1p ì ìš©)\n",
    "for col_name in ap_numeric_cols:\n",
    "    ps_period_df = ap_df.withColumn(\n",
    "        col_name,\n",
    "        log1p(when(col(col_name) < 0, 0).otherwise(col(col_name)))\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d40809a9-b7b3-496f-bab2-b6cd3af2b1b8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### ìƒê´€ê´€ê³„ ë¶„ì„ (fin)\n",
    "pyspark.ml.stat.Correlationì€ **ë²¡í„° ì—´**(ì•„ë˜ ì½”ë“œì—ì„œ featuresë³€ìˆ˜)ì—ì—ì„œë§Œ ì‘ë™í•˜ë¯€ë¡œ<br>\n",
    "â†’ ë°˜ë“œì‹œ VectorAssembler ì‚¬ìš©í•´ì•¼ í•¨"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "89696b52-6a48-4ea8-9d64-b3c45a73c672",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**ìƒê´€ê´€ê³„ ìœ í˜• ì„¤ë©**\n",
    "\n",
    "| êµ¬ë¶„    | í”¼ì–´ìŠ¨ (Pearson)          | ìŠ¤í”¼ì–´ë§Œ (Spearman)                        |\n",
    "| ----- | ---------------------- | -------------------------------------- |\n",
    "| ì •ì˜    | ë³€ìˆ˜ ê°„ì˜ **ì„ í˜• ê´€ê³„** ì¸¡ì •     | ë³€ìˆ˜ ê°„ì˜ **ìˆœìœ„ ê¸°ë°˜(ëª¨ë…¸í†¤) ê´€ê³„** ì¸¡ì •             |\n",
    "| ì „ì œ ì¡°ê±´ | ì—°ì†í˜• ë³€ìˆ˜ + ì •ê·œë¶„í¬ ê·¼ì²˜       | ìˆœìœ„ë¡œ ë°”ê¿”ë„ ì˜ë¯¸ ìˆëŠ” ë°ì´í„°                      |\n",
    "| ë¯¼ê°ë„   | ì´ìƒì¹˜ì— ë¯¼ê°                | ì´ìƒì¹˜ì— ê°•ê±´                                |\n",
    "| ì‚¬ìš© ì˜ˆ  | ì†Œë¹„ê¸ˆì•¡ì²˜ëŸ¼ **ì •ëŸ‰ì ì¸ ê°’ ê°„ ê´€ê³„** | **ë¹„ì„ í˜•ì ì´ì§€ë§Œ ë‹¨ì¡°ì ì¸ ê´€ê³„** (ex. ë§Œì¡±ë„ ë“±ê¸‰ vs ì†Œë¹„ ë“±ê¸‰) |\n",
    "\n",
    "âœ… ìš°ë¦¬ ë¶„ì„ ëª©ì ì—”?\n",
    "- \"ì†Œë¹„ ê¸ˆì•¡ì˜ ì ˆëŒ€ í¬ê¸°\"ë¥¼ ë¶„ì„í•˜ê³  ì‹¶ë‹¤ë©´ â†’ í”¼ì–´ìŠ¨ (ì†Œë¹„ í¬ê¸°ì— ë”°ë¥¸ ìƒí’ˆ ì¶”ì²œ)\n",
    "-  \"ì–´ë””ì— ë” ë§ì´ ì“°ëŠ”ì§€ ì„±í–¥\"ì„ ë³´ê³  ì‹¶ë‹¤ë©´ â†’ ìŠ¤í”¼ì–´ë§Œ(ì†Œë¹„ ì„±í–¥ ê¸°ë°˜ í´ëŸ¬ìŠ¤í„°ë§)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d3498712-1aea-4879-885b-ff5c1cd79d59",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "from pyspark.ml.stat import Correlation\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# í†µê³„ì ìœ¼ë¡œ ìœ ì˜í•œ ìƒ˜í”Œ í¬ê¸° ê³„ì‚°\n",
    "def calculate_sample_size(population_size, confidence_level=0.95, margin_error=0.05):\n",
    "    \"\"\"\n",
    "    í†µê³„ì ìœ¼ë¡œ ìœ ì˜í•œ ìƒ˜í”Œ í¬ê¸° ê³„ì‚°\n",
    "    \"\"\"\n",
    "    z_score = 2.576  # 99% ì‹ ë¢°ë„\n",
    "    p = 0.5  # ìµœëŒ€ ë¶„ì‚°\n",
    "    \n",
    "    n = (z_score**2 * p * (1-p)) / (margin_error**2)\n",
    "    n_adjusted = n / (1 + (n-1)/population_size)\n",
    "    \n",
    "    return int(n_adjusted)\n",
    "\n",
    "# ë°ì´í„° ë¡œë“œ\n",
    "ps_df = ps_period_df\n",
    "total_count = ps_df.count()\n",
    "\n",
    "# í†µê³„ì  ìƒ˜í”Œ í¬ê¸° ê³„ì‚°\n",
    "sample_size = calculate_sample_size(total_count)\n",
    "sample_fraction = sample_size / total_count\n",
    "\n",
    "print(f\"ì „ì²´ ë°ì´í„°: {total_count:,}\")\n",
    "print(f\"í•„ìš” ìƒ˜í”Œ í¬ê¸°: {sample_size:,}\")\n",
    "print(f\"ìƒ˜í”Œë§ ë¹„ìœ¨: {sample_fraction:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1a105b9d-3fa9-4291-af52-448f5497ecd6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 2. ì¸µí™” ìƒ˜í”Œë§ (ê¸°ì¤€ë…„ì›”ë³„ë¡œ ê· ë“±í•˜ê²Œ)\n",
    "def stratified_sampling(df, strata_col=\"ê¸°ì¤€ë…„ì›”\", sample_fraction=0.01):\n",
    "    \"\"\"\n",
    "    ì¸µí™” ìƒ˜í”Œë§ìœ¼ë¡œ ëŒ€í‘œì„± ìˆëŠ” ìƒ˜í”Œ ìƒì„±\n",
    "    \"\"\"\n",
    "    # ê° ì¸µ(ê¸°ì¤€ë…„ì›”)ë³„ ìƒ˜í”Œë§\n",
    "    strata_samples = []\n",
    "    \n",
    "    for month in df.select(strata_col).distinct().collect():\n",
    "        month_value = month[strata_col]\n",
    "        month_df = df.filter(col(strata_col) == month_value)\n",
    "        month_sample = month_df.sample(fraction=sample_fraction, seed=42)\n",
    "        strata_samples.append(month_sample)\n",
    "    \n",
    "    # ëª¨ë“  ì¸µ í•©ì¹˜ê¸°\n",
    "    final_sample = strata_samples[0]\n",
    "    for sample in strata_samples[1:]:\n",
    "        final_sample = final_sample.union(sample)\n",
    "    \n",
    "    return final_sample\n",
    "\n",
    "# ì¸µí™” ìƒ˜í”Œë§ ì‹¤í–‰\n",
    "print(\"=== ì¸µí™” ìƒ˜í”Œë§ ì‹¤í–‰ ===\")\n",
    "sampled_df = stratified_sampling(ps_df, sample_fraction=0.005)  # 0.5%\n",
    "sampled_count = sampled_df.count()\n",
    "print(f\"ìƒ˜í”Œ ë°ì´í„°: {sampled_count:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "96800681-3726-4674-9ce3-61f01660a19d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 3. ë¹ ë¥¸ ìƒê´€ê´€ê³„ ë¶„ì„ (í”¼ì³ ìˆ˜ ì œí•œ ì—†ìŒ)\n",
    "def fast_correlation_analysis(df):\n",
    "\n",
    "    # ìˆ˜ì¹˜í˜• ì»¬ëŸ¼ ì„ íƒ\n",
    "    numeric_cols = [col_name for col_name, data_type in df.dtypes\n",
    "                    if data_type in ['int', 'bigint', 'float', 'double']]\n",
    "    \n",
    "    # í‚¤ ì»¬ëŸ¼ ì œì™¸\n",
    "    exclude_cols = ['ê¸°ì¤€ë…„ì›”', 'ë°œê¸‰íšŒì›ë²ˆí˜¸']\n",
    "    analysis_cols = [col for col in numeric_cols if col not in exclude_cols]\n",
    "    \n",
    "    print(f\"ë¶„ì„í•  í”¼ì²˜ ìˆ˜: {len(analysis_cols)}\")\n",
    "    \n",
    "    # null ì²˜ë¦¬\n",
    "    df_filled = df.fillna(0, subset=analysis_cols)\n",
    "    \n",
    "    # ë²¡í„°í™”\n",
    "    assembler = VectorAssembler(inputCols=analysis_cols, outputCol=\"features\")\n",
    "    vector_df = assembler.transform(df_filled).select(\"features\")\n",
    "    \n",
    "    # ìºì‹±\n",
    "    vector_df.cache()\n",
    "    vector_df.count()\n",
    "    \n",
    "    # ìƒê´€ê´€ê³„ ê³„ì‚°\n",
    "    print(\"ìƒê´€ê´€ê³„ ê³„ì‚° ì¤‘...\")\n",
    "    correlation_matrix = Correlation.corr(vector_df, \"features\", method=\"pearson\").head()[0]\n",
    "    \n",
    "    return correlation_matrix, analysis_cols\n",
    "\n",
    "# ë¹ ë¥¸ ë¶„ì„ ì‹¤í–‰ (ëª¨ë“  í”¼ì²˜ ì‚¬ìš©)\n",
    "correlation_matrix, feature_names = fast_correlation_analysis(sampled_df)\n",
    "print(\"ìƒê´€ê´€ê³„ ê³„ì‚° ì™„ë£Œ!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "53805341-e8b1-46ed-9dbc-fcd3e91bc8e5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 4. ê²°ê³¼ ë¶„ì„ ë° ì‹œê°í™”\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# ìƒê´€ê´€ê³„ ë§¤íŠ¸ë¦­ìŠ¤ë¥¼ numpy ë°°ì—´ë¡œ ë³€í™˜\n",
    "corr_array = correlation_matrix.toArray()\n",
    "\n",
    "# ë†’ì€ ìƒê´€ê´€ê³„ ì°¾ê¸°\n",
    "high_correlations = []\n",
    "for i in range(len(feature_names)):\n",
    "    for j in range(i+1, len(feature_names)):\n",
    "        corr_value = corr_array[i][j]\n",
    "        if abs(corr_value) > 0.7:  # 0.7 ì´ìƒ\n",
    "            high_correlations.append({\n",
    "                'feature1': feature_names[i],\n",
    "                'feature2': feature_names[j],\n",
    "                'correlation': corr_value\n",
    "            })\n",
    "\n",
    "# ê²°ê³¼ ì¶œë ¥\n",
    "print(f\"\\n=== ë†’ì€ ìƒê´€ê´€ê³„ ({len(high_correlations)}ê°œ) ===\")\n",
    "high_correlations_sorted = sorted(high_correlations, \n",
    "                                 key=lambda x: abs(x['correlation']), \n",
    "                                 reverse=True)\n",
    "\n",
    "for corr in high_correlations_sorted[:10]:\n",
    "    print(f\"{corr['feature1']} â†” {corr['feature2']}: {corr['correlation']:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8b8c0483-5ddc-463a-b0d1-d2bcd908d70e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 5.íˆíŠ¸ë§µ ìƒì„±\n",
    "try:\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    \n",
    "    # ìƒê´€ê´€ê³„ ë§¤íŠ¸ë¦­ìŠ¤ë¥¼ DataFrameìœ¼ë¡œ ë³€í™˜\n",
    "    corr_df = pd.DataFrame(corr_array, \n",
    "                          index=feature_names, \n",
    "                          columns=feature_names)\n",
    "    \n",
    "    print(f\"DataFrame í¬ê¸°: {corr_df.shape}\")\n",
    "    print(f\"DataFrame ì¸ë±ìŠ¤ ìˆ˜: {len(corr_df.index)}\")\n",
    "    print(f\"DataFrame ì»¬ëŸ¼ ìˆ˜: {len(corr_df.columns)}\")\n",
    "    \n",
    "    # í° íˆíŠ¸ë§µì„ ìœ„í•œ ì„¤ì •\n",
    "    plt.figure(figsize=(20, 18))  # í¬ê¸° ì¦ê°€\n",
    "    \n",
    "    # íˆíŠ¸ë§µ ìƒì„± (ë¼ë²¨ í¬ê¸° ì¡°ì •)\n",
    "    sns.heatmap(corr_df, \n",
    "                annot=False,  # ìˆ«ì í‘œì‹œ ë„ê¸° (ë„ˆë¬´ ë§ì•„ì„œ)\n",
    "                cmap='coolwarm', \n",
    "                center=0,\n",
    "                square=True, \n",
    "                fmt='.2f',\n",
    "                xticklabels=True,  # xì¶• ë¼ë²¨ í‘œì‹œ\n",
    "                yticklabels=True,  # yì¶• ë¼ë²¨ í‘œì‹œ\n",
    "                cbar_kws={'shrink': 0.8})\n",
    "    \n",
    "    # ë¼ë²¨ í¬ê¸° ì¡°ì •\n",
    "    plt.xticks(rotation=45, ha='right', fontsize=8)\n",
    "    plt.yticks(rotation=0, fontsize=8)\n",
    "    plt.title('Feature Correlation Heatmap (All Features)', fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # ìƒê´€ê´€ê³„ê°€ ë†’ì€ í”¼ì²˜ë“¤ë§Œ ë³„ë„ íˆíŠ¸ë§µ\n",
    "    print(\"\\n=== ë†’ì€ ìƒê´€ê´€ê³„ í”¼ì²˜ë“¤ë§Œ íˆíŠ¸ë§µ ===\")\n",
    "    \n",
    "    # ë†’ì€ ìƒê´€ê´€ê³„ë¥¼ ê°€ì§„ í”¼ì²˜ë“¤ ì°¾ê¸°\n",
    "    high_corr_features = set()\n",
    "    threshold = 0.7\n",
    "    \n",
    "    for i in range(len(feature_names)):\n",
    "        for j in range(i+1, len(feature_names)):\n",
    "            if abs(corr_array[i][j]) > threshold:\n",
    "                high_corr_features.add(feature_names[i])\n",
    "                high_corr_features.add(feature_names[j])\n",
    "    \n",
    "    if high_corr_features:\n",
    "        high_corr_features = list(high_corr_features)\n",
    "        print(f\"ë†’ì€ ìƒê´€ê´€ê³„ í”¼ì²˜ ìˆ˜: {len(high_corr_features)}\")\n",
    "        \n",
    "        # ì„œë¸Œì…‹ íˆíŠ¸ë§µ\n",
    "        corr_subset = corr_df.loc[high_corr_features, high_corr_features]\n",
    "        \n",
    "        plt.figure(figsize=(12, 10))\n",
    "        sns.heatmap(corr_subset, \n",
    "                    annot=False, \n",
    "                    cmap='coolwarm', \n",
    "                    center=0,\n",
    "                    square=True, \n",
    "                    fmt='.2f',\n",
    "                    xticklabels=True,\n",
    "                    yticklabels=True)\n",
    "        plt.xticks(rotation=45, ha='right')\n",
    "        plt.yticks(rotation=0)\n",
    "        plt.title(f'High Correlation Features Heatmap (>{threshold})')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"ë†’ì€ ìƒê´€ê´€ê³„ë¥¼ ê°€ì§„ í”¼ì²˜ê°€ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "        \n",
    "except ImportError:\n",
    "    print(\"matplotlib/seabornì´ ì—†ì–´ íˆíŠ¸ë§µì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "except Exception as e:\n",
    "    print(f\"íˆíŠ¸ë§µ ìƒì„± ì¤‘ ì˜¤ë¥˜: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "65aed736-a3e9-4bf0-9d0d-0d74fb07c1c2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 6. ë‹¤ì¤‘ê³µì„ ì„± ê²€ì‚¬\n",
    "def check_multicollinearity(corr_matrix, feature_names, threshold=0.9):\n",
    "    \"\"\"\n",
    "    ë‹¤ì¤‘ê³µì„ ì„± ê²€ì‚¬\n",
    "    \"\"\"\n",
    "    corr_array = corr_matrix.toArray()\n",
    "    multicollinear_pairs = []\n",
    "    \n",
    "    for i in range(len(feature_names)):\n",
    "        for j in range(i+1, len(feature_names)):\n",
    "            corr_value = abs(corr_array[i][j])\n",
    "            if corr_value > threshold:\n",
    "                multicollinear_pairs.append({\n",
    "                    'feature1': feature_names[i],\n",
    "                    'feature2': feature_names[j],\n",
    "                    'correlation': corr_array[i][j]\n",
    "                })\n",
    "    \n",
    "    return multicollinear_pairs\n",
    "\n",
    "# ë‹¤ì¤‘ê³µì„ ì„± ê²€ì‚¬\n",
    "multicollinear = check_multicollinearity(correlation_matrix, feature_names, 0.9)\n",
    "\n",
    "print(f\"\\n=== ë‹¤ì¤‘ê³µì„ ì„± ìœ„í—˜ ({len(multicollinear)}ê°œ) ===\")\n",
    "for pair in multicollinear:\n",
    "    print(f\"âš ï¸ {pair['feature1']} â†” {pair['feature2']}: {pair['correlation']:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b9bbcd5a-a7cc-4f52-8d99-056278e28496",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### ê¸°ê°„ë³„ ìƒê´€ê´€ê³„ ë¶„ì„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "28f0b55a-27e5-4b17-bb87-029476a854bb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ê¸°ê°„ë³„ ì»¬ëŸ¼ ë¶„ë¥˜ í•¨ìˆ˜\n",
    "def classify_columns_by_period(df_columns):\n",
    "    \"\"\"\n",
    "    ì»¬ëŸ¼ëª…ì„ ê¸°ê°„ë³„ë¡œ ë¶„ë¥˜í•˜ëŠ” í•¨ìˆ˜\n",
    "    \"\"\"\n",
    "    period_groups = {\n",
    "        'R3M': [],\n",
    "        'R6M': [],\n",
    "        'R12M': []\n",
    "    }\n",
    "    \n",
    "    # ê¸°ê°„ íŒ¨í„´ ì •ì˜\n",
    "    period_patterns = {\n",
    "        'R3M': re.compile(r'.*_R3M$'),\n",
    "        'R6M': re.compile(r'.*_R6M$'), \n",
    "        'R12M': re.compile(r'.*_R12M$')\n",
    "    }\n",
    "    \n",
    "    for col_name in df_columns:\n",
    "        # ì œì™¸í•  ì»¬ëŸ¼ë“¤\n",
    "        if col_name in ['ê¸°ì¤€ë…„ì›”', 'ë°œê¸‰íšŒì›ë²ˆí˜¸']:\n",
    "            continue\n",
    "            \n",
    "        # ê° ê¸°ê°„ íŒ¨í„´ì— ë§¤ì¹­ë˜ëŠ”ì§€ í™•ì¸\n",
    "        for period, pattern in period_patterns.items():\n",
    "            if pattern.match(col_name):\n",
    "                period_groups[period].append(col_name)\n",
    "                break\n",
    "    \n",
    "    return period_groups\n",
    "\n",
    "# ps_period_dfì˜ ì»¬ëŸ¼ ë¶„ë¥˜\n",
    "column_groups = classify_columns_by_period(ps_period_df.columns)\n",
    "\n",
    "# ê²°ê³¼ í™•ì¸\n",
    "print(\"=== ê¸°ê°„ë³„ ì»¬ëŸ¼ ë¶„ë¥˜ ê²°ê³¼ ===\")\n",
    "for period, cols in column_groups.items():\n",
    "    print(f\"{period}: {len(cols)}ê°œ ì»¬ëŸ¼\")\n",
    "    if len(cols) > 0:\n",
    "        print(f\"  ì˜ˆì‹œ: {cols[:3]}...\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "914592fe-9dab-4cd4-8865-9dd40b8afb01",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ê¸°ê°„ë³„ ìƒê´€ê´€ê³„ ë¶„ì„ í•¨ìˆ˜ (ê¸°ì¡´ ì½”ë“œ êµ¬ì¡° ìœ ì§€)\n",
    "def analyze_correlation_by_period(period_columns, period_name):\n",
    "    \"\"\"\n",
    "    íŠ¹ì • ê¸°ê°„ì˜ ì»¬ëŸ¼ë“¤ì— ëŒ€í•´ ìƒê´€ê´€ê³„ ë¶„ì„ì„ ìˆ˜í–‰í•˜ëŠ” í•¨ìˆ˜\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"=== {period_name} ê¸°ê°„ ìƒê´€ê´€ê³„ ë¶„ì„ ===\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    if len(period_columns) == 0:\n",
    "        print(f\"{period_name} ê¸°ê°„ì— í•´ë‹¹í•˜ëŠ” ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "        return None, None\n",
    "    \n",
    "    print(f\"ë¶„ì„ ëŒ€ìƒ ì»¬ëŸ¼ ìˆ˜: {len(period_columns)}\")\n",
    "    \n",
    "    # 1. ì¸µí™” ìƒ˜í”Œë§ (ê¸°ì¡´ ì½”ë“œì™€ ë™ì¼í•œ ë°©ì‹)\n",
    "    def stratified_sampling(df, strata_col=\"ê¸°ì¤€ë…„ì›”\", sample_fraction=0.005):\n",
    "        strata_samples = []\n",
    "        \n",
    "        for month in df.select(strata_col).distinct().collect():\n",
    "            month_value = month[strata_col]\n",
    "            month_df = df.filter(col(strata_col) == month_value)\n",
    "            month_sample = month_df.sample(fraction=sample_fraction, seed=42)\n",
    "            strata_samples.append(month_sample)\n",
    "        \n",
    "        # ëª¨ë“  ì¸µ í•©ì¹˜ê¸°\n",
    "        final_sample = strata_samples[0]\n",
    "        for sample in strata_samples[1:]:\n",
    "            final_sample = final_sample.union(sample)\n",
    "        \n",
    "        return final_sample\n",
    "    \n",
    "    # ìƒ˜í”Œë§ ì‹¤í–‰\n",
    "    sampled_df = stratified_sampling(ps_period_df, sample_fraction=0.005)\n",
    "    sampled_count = sampled_df.count()\n",
    "    print(f\"ìƒ˜í”Œ ë°ì´í„°: {sampled_count:,}\")\n",
    "    \n",
    "    # 2. í•´ë‹¹ ê¸°ê°„ ì»¬ëŸ¼ë§Œ ì„ íƒ + ê¸°ì¤€ë…„ì›” (ì¸µí™”ìƒ˜í”Œë§ì„ ìœ„í•´ í•„ìš”í–ˆë˜ ì»¬ëŸ¼)\n",
    "    period_df = sampled_df.select(period_columns)\n",
    "    \n",
    "    # 3. ë¹ ë¥¸ ìƒê´€ê´€ê³„ ë¶„ì„ (ê¸°ì¡´ í•¨ìˆ˜ì™€ ë™ì¼í•œ ë¡œì§)\n",
    "    def fast_correlation_analysis(df, analysis_cols):\n",
    "        print(f\"ë¶„ì„í•  í”¼ì²˜ ìˆ˜: {len(analysis_cols)}\")\n",
    "        \n",
    "        # null ì²˜ë¦¬\n",
    "        df_filled = df.fillna(0, subset=analysis_cols)\n",
    "        \n",
    "        # ë²¡í„°í™”\n",
    "        assembler = VectorAssembler(inputCols=analysis_cols, outputCol=\"features\")\n",
    "        vector_df = assembler.transform(df_filled).select(\"features\")\n",
    "        \n",
    "        # ìºì‹±\n",
    "        vector_df.cache()\n",
    "        vector_df.count()\n",
    "        \n",
    "        # ìƒê´€ê´€ê³„ ê³„ì‚°\n",
    "        print(\"ìƒê´€ê´€ê³„ ê³„ì‚° ì¤‘...\")\n",
    "        correlation_matrix = Correlation.corr(vector_df, \"features\", method=\"pearson\").head()[0]\n",
    "        \n",
    "        return correlation_matrix, analysis_cols\n",
    "    \n",
    "    # ìƒê´€ê´€ê³„ ë¶„ì„ ì‹¤í–‰\n",
    "    correlation_matrix, feature_names = fast_correlation_analysis(period_df, period_columns)\n",
    "    print(\"ìƒê´€ê´€ê³„ ê³„ì‚° ì™„ë£Œ!\")\n",
    "    \n",
    "    # 4. ê²°ê³¼ ë¶„ì„ (ê¸°ì¡´ ì½”ë“œì™€ ë™ì¼)\n",
    "    corr_array = correlation_matrix.toArray()\n",
    "    \n",
    "    # ë†’ì€ ìƒê´€ê´€ê³„ ì°¾ê¸°\n",
    "    high_correlations = []\n",
    "    for i in range(len(feature_names)):\n",
    "        for j in range(i+1, len(feature_names)):\n",
    "            corr_value = corr_array[i][j]\n",
    "            if abs(corr_value) > 0.7:\n",
    "                high_correlations.append({\n",
    "                    'feature1': feature_names[i],\n",
    "                    'feature2': feature_names[j],\n",
    "                    'correlation': corr_value\n",
    "                })\n",
    "    \n",
    "    # ê²°ê³¼ ì¶œë ¥\n",
    "    print(f\"\\n=== ë†’ì€ ìƒê´€ê´€ê³„ ({len(high_correlations)}ê°œ) ===\")\n",
    "    high_correlations_sorted = sorted(high_correlations, \n",
    "                                     key=lambda x: abs(x['correlation']), \n",
    "                                     reverse=True)\n",
    "    \n",
    "    for corr in high_correlations_sorted[:10]:\n",
    "        print(f\"{corr['feature1']} â†” {corr['feature2']}: {corr['correlation']:.3f}\")\n",
    "    \n",
    "    # 5. ë‹¤ì¤‘ê³µì„ ì„± ê²€ì‚¬ (ê¸°ì¡´ ì½”ë“œì™€ ë™ì¼)\n",
    "    def check_multicollinearity(corr_matrix, feature_names, threshold=0.9):\n",
    "        corr_array = corr_matrix.toArray()\n",
    "        multicollinear_pairs = []\n",
    "        \n",
    "        for i in range(len(feature_names)):\n",
    "            for j in range(i+1, len(feature_names)):\n",
    "                corr_value = abs(corr_array[i][j])\n",
    "                if corr_value > threshold:\n",
    "                    multicollinear_pairs.append({\n",
    "                        'feature1': feature_names[i],\n",
    "                        'feature2': feature_names[j],\n",
    "                        'correlation': corr_array[i][j]\n",
    "                    })\n",
    "        \n",
    "        return multicollinear_pairs\n",
    "    \n",
    "    multicollinear = check_multicollinearity(correlation_matrix, feature_names, 0.9)\n",
    "    \n",
    "    print(f\"\\n=== ë‹¤ì¤‘ê³µì„ ì„± ìœ„í—˜ ({len(multicollinear)}ê°œ) ===\")\n",
    "    for pair in multicollinear:\n",
    "        print(f\"âš ï¸ {pair['feature1']} â†” {pair['feature2']}: {pair['correlation']:.3f}\")\n",
    "    \n",
    "    return correlation_matrix, feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2b3711c8-cc97-44d9-ad48-2a688ac60a3f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ê° ê¸°ê°„ë³„ë¡œ ìƒê´€ê´€ê³„ ë¶„ì„ ì‹¤í–‰\n",
    "correlation_results = {}\n",
    "\n",
    "for period in ['R3M', 'R6M', 'R12M']:\n",
    "    if len(column_groups[period]) > 0:\n",
    "        corr_matrix, feature_names = analyze_correlation_by_period(\n",
    "            column_groups[period], \n",
    "            period\n",
    "        )\n",
    "        \n",
    "        if corr_matrix is not None:\n",
    "            correlation_results[period] = {\n",
    "                'matrix': corr_matrix,\n",
    "                'features': feature_names\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "497a828d-847b-41f3-8916-e1cbc306958a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 5. íˆíŠ¸ë§µ ìƒì„± (ê¸°ì¡´ ì½”ë“œì™€ ë™ì¼í•œ ë°©ì‹)\n",
    "def create_period_heatmaps():\n",
    "    for period, data in correlation_results.items():\n",
    "        try:\n",
    "            print(f\"\\n=== {period} íˆíŠ¸ë§µ ìƒì„± ===\")\n",
    "            \n",
    "            # ìƒê´€ê´€ê³„ ë§¤íŠ¸ë¦­ìŠ¤ë¥¼ DataFrameìœ¼ë¡œ ë³€í™˜\n",
    "            corr_array = data['matrix'].toArray()\n",
    "            feature_names = data['features']\n",
    "            \n",
    "            corr_df = pd.DataFrame(corr_array, \n",
    "                                  index=feature_names, \n",
    "                                  columns=feature_names)\n",
    "            \n",
    "            print(f\"DataFrame í¬ê¸°: {corr_df.shape}\")\n",
    "            \n",
    "            # í° íˆíŠ¸ë§µì„ ìœ„í•œ ì„¤ì •\n",
    "            plt.figure(figsize=(20, 18))\n",
    "            \n",
    "            # íˆíŠ¸ë§µ ìƒì„± (ë¼ë²¨ í¬ê¸° ì¡°ì •)\n",
    "            sns.heatmap(corr_df, \n",
    "                        annot=False,  # ìˆ«ì í‘œì‹œ ë„ê¸°\n",
    "                        cmap='coolwarm', \n",
    "                        center=0,\n",
    "                        square=True, \n",
    "                        fmt='.2f',\n",
    "                        xticklabels=True,\n",
    "                        yticklabels=True,\n",
    "                        cbar_kws={'shrink': 0.8})\n",
    "            \n",
    "            # ë¼ë²¨ í¬ê¸° ì¡°ì •\n",
    "            plt.xticks(rotation=45, ha='right', fontsize=8)\n",
    "            plt.yticks(rotation=0, fontsize=8)\n",
    "            plt.title(f'{period} ê¸°ê°„ Feature Correlation Heatmap', fontsize=16)\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            \n",
    "            # ìƒê´€ê´€ê³„ê°€ ë†’ì€ í”¼ì²˜ë“¤ë§Œ ë³„ë„ íˆíŠ¸ë§µ\n",
    "            print(f\"\\n=== {period} ë†’ì€ ìƒê´€ê´€ê³„ í”¼ì²˜ë“¤ë§Œ íˆíŠ¸ë§µ ===\")\n",
    "            \n",
    "            high_corr_features = set()\n",
    "            threshold = 0.7\n",
    "            \n",
    "            for i in range(len(feature_names)):\n",
    "                for j in range(i+1, len(feature_names)):\n",
    "                    if abs(corr_array[i][j]) > threshold:\n",
    "                        high_corr_features.add(feature_names[i])\n",
    "                        high_corr_features.add(feature_names[j])\n",
    "            \n",
    "            if high_corr_features:\n",
    "                high_corr_features = list(high_corr_features)\n",
    "                print(f\"ë†’ì€ ìƒê´€ê´€ê³„ í”¼ì²˜ ìˆ˜: {len(high_corr_features)}\")\n",
    "                \n",
    "                # ì„œë¸Œì…‹ íˆíŠ¸ë§µ\n",
    "                corr_subset = corr_df.loc[high_corr_features, high_corr_features]\n",
    "                \n",
    "                plt.figure(figsize=(12, 10))\n",
    "                sns.heatmap(corr_subset, \n",
    "                            annot=False, \n",
    "                            cmap='coolwarm', \n",
    "                            center=0,\n",
    "                            square=True, \n",
    "                            fmt='.2f',\n",
    "                            xticklabels=True,\n",
    "                            yticklabels=True)\n",
    "                plt.xticks(rotation=45, ha='right')\n",
    "                plt.yticks(rotation=0)\n",
    "                plt.title(f'{period} High Correlation Features Heatmap (>{threshold})')\n",
    "                plt.tight_layout()\n",
    "                plt.show()\n",
    "            else:\n",
    "                print(\"ë†’ì€ ìƒê´€ê´€ê³„ë¥¼ ê°€ì§„ í”¼ì²˜ê°€ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"{period} íˆíŠ¸ë§µ ìƒì„± ì¤‘ ì˜¤ë¥˜: {str(e)}\")\n",
    "\n",
    "# íˆíŠ¸ë§µ ìƒì„± ì‹¤í–‰\n",
    "create_period_heatmaps()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ac67756c-fbaa-44f3-9bd4-3ed499fbe190",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ê¸°ê°„ë³„ ë¶„ì„ ê²°ê³¼ ìš”ì•½\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"=== ê¸°ê°„ë³„ ìƒê´€ê´€ê³„ ë¶„ì„ ê²°ê³¼ ìš”ì•½ ===\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for period in ['R3M', 'R6M', 'R12M']:\n",
    "    print(f\"\\n[{period}]\")\n",
    "    if period in correlation_results:\n",
    "        n_features = len(correlation_results[period]['features'])\n",
    "        print(f\"  - ë¶„ì„ëœ í”¼ì²˜ ìˆ˜: {n_features}\")\n",
    "        print(f\"  - ìƒê´€ê´€ê³„ ë§¤íŠ¸ë¦­ìŠ¤ í¬ê¸°: {n_features}x{n_features}\")\n",
    "        print(f\"  - ìƒíƒœ: âœ… ë¶„ì„ ì™„ë£Œ\")\n",
    "    else:\n",
    "        n_features = len(column_groups[period])\n",
    "        if n_features == 0:\n",
    "            print(f\"  - í•´ë‹¹ ê¸°ê°„ ì»¬ëŸ¼ ì—†ìŒ\")\n",
    "        else:\n",
    "            print(f\"  - í”¼ì²˜ ìˆ˜: {n_features}\")\n",
    "            print(f\"  - ìƒíƒœ: âŒ ë¶„ì„ ì‹¤íŒ¨\")\n",
    "\n",
    "print(f\"\\nì „ì²´ ë¶„ì„ ì™„ë£Œëœ ê¸°ê°„: {len(correlation_results)}ê°œ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2bf53c1b-007d-45d9-8880-05096c4e915a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### ì „ì²´ ìƒê´€ê´€ê³„ ë¶„ì„ (GPU)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4e5073e9-7944-4356-9e48-d4dbf7a76db5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "ìƒ˜í”Œë§ ì—†ì´ ì „ì²´ í–‰ì„ ë¶„ì„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "062a5194-f22f-460e-a55c-56c10cbe90f4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pyspark.sql.functions import col\n",
    "import time\n",
    "\n",
    "print(\"=== GPUë¡œ ìƒê´€ê´€ê³„ ë¶„ì„ ===\")\n",
    "\n",
    "# 1. GPU ë©”ëª¨ë¦¬ ìµœì í™” ì„¤ì •\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"ì‚¬ìš© ë””ë°”ì´ìŠ¤: {device}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    gpu_props = torch.cuda.get_device_properties(0)\n",
    "    total_memory = gpu_props.total_memory / 1024**3\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU ì´ ë©”ëª¨ë¦¬: {total_memory:.1f} GB\")\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "# 2. ì „ì²´ ë°ì´í„° ë¡œë“œ\n",
    "ps_df = ps_period_df\n",
    "numeric_cols = [col_name for col_name, data_type in ps_df.dtypes\n",
    "                if data_type in ['int', 'bigint', 'float', 'double']]\n",
    "\n",
    "total_rows = ps_df.count()\n",
    "n_features = len(numeric_cols)\n",
    "print(f\"ì „ì²´ ë°ì´í„° ìˆ˜: {total_rows:,}\")\n",
    "print(f\"ë¶„ì„í•  í”¼ì²˜ ìˆ˜: {n_features}\")\n",
    "\n",
    "def compute_large_chunk_correlation(df, numeric_cols, device):\n",
    "    \"\"\"\n",
    "    ëŒ€ìš©ëŸ‰ ì²­í¬ë¡œ GPU ìƒê´€ê´€ê³„ ê³„ì‚° (ë©”ëª¨ë¦¬ í™œìš©ë„ ê·¹ëŒ€í™”)\n",
    "    \"\"\"\n",
    "    print(\"\\n=== ëŒ€ìš©ëŸ‰ ì²­í¬ GPU ì²˜ë¦¬ ì‹œì‘ ===\")\n",
    "    \n",
    "    # í›¨ì”¬ í° ì²­í¬ í¬ê¸° ì„¤ì • (Tesla T4 16GB ê¸°ì¤€)\n",
    "    if torch.cuda.is_available():\n",
    "        # 16GB GPUì—ì„œ ì•ˆì „í•˜ê²Œ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” í¬ê¸°\n",
    "        # ìƒê´€ê´€ê³„ ë§¤íŠ¸ë¦­ìŠ¤ ê³„ì‚° ì‹œ ì¤‘ê°„ ê²°ê³¼ë¬¼ ê³ ë ¤í•˜ì—¬ ë³´ìˆ˜ì ìœ¼ë¡œ ì„¤ì •\n",
    "        large_chunk_size = 2000000  # 200ë§Œ í–‰ë¶€í„° ì‹œì‘\n",
    "        \n",
    "        # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¶”ì •\n",
    "        estimated_memory_gb = (large_chunk_size * n_features * 4) / 1024**3  # float32 ê¸°ì¤€\n",
    "        print(f\"ì²­í¬ë‹¹ ì˜ˆìƒ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {estimated_memory_gb:.2f} GB\")\n",
    "        \n",
    "        # GPU ë©”ëª¨ë¦¬ì˜ 70% ì´ìƒ ì‚¬ìš©í•˜ë„ë¡ ì¡°ì •\n",
    "        target_memory_usage = total_memory * 0.7  # 70% ì‚¬ìš© ëª©í‘œ\n",
    "        optimal_chunk_size = int((target_memory_usage * 1024**3) / (n_features * 4 * 3))  # ì•ˆì „ ë§ˆì§„\n",
    "        \n",
    "        # ìµœì¢… ì²­í¬ í¬ê¸° ê²°ì • (ìµœì†Œ 100ë§Œ, ìµœëŒ€ 500ë§Œ)\n",
    "        final_chunk_size = max(1000000, min(optimal_chunk_size, 5000000))\n",
    "        \n",
    "    else:\n",
    "        final_chunk_size = 1000000  # CPUì˜ ê²½ìš°\n",
    "    \n",
    "    print(f\"ìµœì¢… ì²­í¬ í¬ê¸°: {final_chunk_size:,} í–‰\")\n",
    "    \n",
    "    # ì²­í¬ ê°œìˆ˜ ê³„ì‚°\n",
    "    n_rows = df.count()\n",
    "    n_chunks = max(1, (n_rows + final_chunk_size - 1) // final_chunk_size)\n",
    "    \n",
    "    print(f\"ì´ {n_chunks}ê°œ ì²­í¬ë¡œ ë¶„í• \")\n",
    "    \n",
    "    if n_chunks > 10:\n",
    "        print(\"âš ï¸ ì²­í¬ ê°œìˆ˜ê°€ ë§ìŠµë‹ˆë‹¤. ì²­í¬ í¬ê¸°ë¥¼ ë” ëŠ˜ë ¤ë³´ê² ìŠµë‹ˆë‹¤.\")\n",
    "        final_chunk_size = max(final_chunk_size, n_rows // 5)  # ìµœëŒ€ 5ê°œ ì²­í¬ë¡œ ì œí•œ\n",
    "        n_chunks = max(1, (n_rows + final_chunk_size - 1) // final_chunk_size)\n",
    "        print(f\"ì¡°ì •ëœ ì²­í¬ í¬ê¸°: {final_chunk_size:,} í–‰\")\n",
    "        print(f\"ì¡°ì •ëœ ì²­í¬ ê°œìˆ˜: {n_chunks}ê°œ\")\n",
    "    \n",
    "    # ìƒê´€ê´€ê³„ ë§¤íŠ¸ë¦­ìŠ¤ ëˆ„ì ì„ ìœ„í•œ ë³€ìˆ˜ë“¤\n",
    "    correlation_sum = None\n",
    "    total_weight = 0\n",
    "    \n",
    "    for chunk_idx in range(n_chunks):\n",
    "        chunk_start_time = time.time()\n",
    "        \n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"ì²­í¬ {chunk_idx + 1}/{n_chunks} ì²˜ë¦¬ ì¤‘...\")\n",
    "        \n",
    "        # ì²­í¬ ë°ì´í„° ì¶”ì¶œ (ë” íš¨ìœ¨ì ì¸ ë°©ë²•)\n",
    "        if n_chunks == 1:\n",
    "            # ì „ì²´ ë°ì´í„°ë¥¼ í•œ ë²ˆì— ì²˜ë¦¬\n",
    "            chunk_df = df\n",
    "        else:\n",
    "            # ë¶„í•  ì²˜ë¦¬\n",
    "            chunk_fraction = 1.0 / n_chunks\n",
    "            chunk_df = df.sample(fraction=chunk_fraction, seed=42 + chunk_idx)\n",
    "        \n",
    "        # Pandasë¡œ ë³€í™˜\n",
    "        print(\"  PySpark â†’ Pandas ë³€í™˜ ì¤‘...\")\n",
    "        conversion_start = time.time()\n",
    "        chunk_pdf = chunk_df.select(*numeric_cols).fillna(0).toPandas()\n",
    "        conversion_time = time.time() - conversion_start\n",
    "        \n",
    "        actual_chunk_size = len(chunk_pdf)\n",
    "        print(f\"  ì‹¤ì œ ì²­í¬ í¬ê¸°: {actual_chunk_size:,} x {len(chunk_pdf.columns)}\")\n",
    "        print(f\"  ë³€í™˜ ì‹œê°„: {conversion_time:.2f}ì´ˆ\")\n",
    "        \n",
    "        if actual_chunk_size == 0:\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            # GPU ë©”ëª¨ë¦¬ ìƒíƒœ í™•ì¸\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "                initial_memory = torch.cuda.memory_allocated() / 1024**3\n",
    "                available_memory = (torch.cuda.get_device_properties(0).total_memory - torch.cuda.memory_allocated()) / 1024**3\n",
    "                print(f\"  ì‚¬ìš© ê°€ëŠ¥ GPU ë©”ëª¨ë¦¬: {available_memory:.2f} GB\")\n",
    "            \n",
    "            # GPU í…ì„œë¡œ ë³€í™˜\n",
    "            print(\"  GPU í…ì„œ ë³€í™˜ ì¤‘...\")\n",
    "            tensor_start = time.time()\n",
    "            chunk_tensor = torch.tensor(chunk_pdf.values, dtype=torch.float32).to(device)\n",
    "            tensor_time = time.time() - tensor_start\n",
    "            \n",
    "            if torch.cuda.is_available():\n",
    "                after_tensor_memory = torch.cuda.memory_allocated() / 1024**3\n",
    "                memory_used = after_tensor_memory - initial_memory\n",
    "                print(f\"  GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {memory_used:.2f} GB\")\n",
    "                print(f\"  í…ì„œ ë³€í™˜ ì‹œê°„: {tensor_time:.2f}ì´ˆ\")\n",
    "            \n",
    "            # ìƒê´€ê´€ê³„ ê³„ì‚°\n",
    "            print(\"  ìƒê´€ê´€ê³„ ê³„ì‚° ì¤‘...\")\n",
    "            corr_start = time.time()\n",
    "            chunk_correlation = torch.corrcoef(chunk_tensor.T)\n",
    "            corr_time = time.time() - corr_start\n",
    "            print(f\"  ìƒê´€ê´€ê³„ ê³„ì‚° ì‹œê°„: {corr_time:.2f}ì´ˆ\")\n",
    "            \n",
    "            # CPUë¡œ ì´ë™í•˜ì—¬ ëˆ„ì \n",
    "            chunk_corr_cpu = chunk_correlation.cpu().numpy()\n",
    "            \n",
    "            # ê°€ì¤‘ í‰ê· ìœ¼ë¡œ ëˆ„ì \n",
    "            weight = actual_chunk_size\n",
    "            if correlation_sum is None:\n",
    "                correlation_sum = chunk_corr_cpu * weight\n",
    "            else:\n",
    "                correlation_sum += chunk_corr_cpu * weight\n",
    "            total_weight += weight\n",
    "            \n",
    "            # ë©”ëª¨ë¦¬ ì •ë¦¬\n",
    "            del chunk_tensor, chunk_correlation, chunk_pdf\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "            \n",
    "            chunk_total_time = time.time() - chunk_start_time\n",
    "            print(f\"  ì²­í¬ ì´ ì²˜ë¦¬ ì‹œê°„: {chunk_total_time:.2f}ì´ˆ\")\n",
    "            print(f\"  ì§„í–‰ë¥ : {(chunk_idx + 1) / n_chunks * 100:.1f}%\")\n",
    "            \n",
    "            # ë‚¨ì€ ì‹œê°„ ì¶”ì •\n",
    "            if chunk_idx > 0:\n",
    "                avg_time_per_chunk = (time.time() - start_time) / (chunk_idx + 1)\n",
    "                remaining_time = avg_time_per_chunk * (n_chunks - chunk_idx - 1)\n",
    "                print(f\"  ì˜ˆìƒ ë‚¨ì€ ì‹œê°„: {remaining_time:.1f}ì´ˆ ({remaining_time/60:.1f}ë¶„)\")\n",
    "            \n",
    "        except RuntimeError as e:\n",
    "            if \"out of memory\" in str(e):\n",
    "                print(f\"  âŒ GPU ë©”ëª¨ë¦¬ ë¶€ì¡±! í˜„ì¬ ì²­í¬ í¬ê¸°: {actual_chunk_size:,}\")\n",
    "                print(\"  ë” ì‘ì€ ì²­í¬ë¡œ ì¬ì‹œë„í•˜ê±°ë‚˜ CPUë¡œ fallbackì´ í•„ìš”í•©ë‹ˆë‹¤.\")\n",
    "                raise e\n",
    "            else:\n",
    "                raise e\n",
    "    \n",
    "    # ìµœì¢… ìƒê´€ê´€ê³„ ë§¤íŠ¸ë¦­ìŠ¤ ê³„ì‚°\n",
    "    if total_weight > 0:\n",
    "        final_correlation = correlation_sum / total_weight\n",
    "        return final_correlation, numeric_cols\n",
    "    else:\n",
    "        return None, numeric_cols\n",
    "\n",
    "# 3. ëŒ€ìš©ëŸ‰ ì²­í¬ ìƒê´€ê´€ê³„ ë¶„ì„ ì‹¤í–‰\n",
    "print(f\"\\nğŸš€ ëŒ€ìš©ëŸ‰ ì²­í¬ë¡œ ì „ì²´ ë°ì´í„° {total_rows:,}í–‰ ë¶„ì„ ì‹œì‘\")\n",
    "start_time = time.time()\n",
    "\n",
    "try:\n",
    "    correlation_matrix, feature_names = compute_large_chunk_correlation(\n",
    "        ps_df, \n",
    "        numeric_cols, \n",
    "        device\n",
    "    )\n",
    "    \n",
    "    end_time = time.time()\n",
    "    total_time = end_time - start_time\n",
    "    print(f\"\\nâ±ï¸ ì „ì²´ ì²˜ë¦¬ ì‹œê°„: {total_time:.2f}ì´ˆ ({total_time/60:.1f}ë¶„)\")\n",
    "    print(f\"ğŸ“Š ì²˜ë¦¬ ì†ë„: {total_rows/total_time:,.0f} í–‰/ì´ˆ\")\n",
    "    \n",
    "    # 4. ê²°ê³¼ ë¶„ì„\n",
    "    if correlation_matrix is not None:\n",
    "        print(f\"\\n=== âœ… ëŒ€ìš©ëŸ‰ ì²­í¬ ìƒê´€ê´€ê³„ ë¶„ì„ ê²°ê³¼ âœ… ===\")\n",
    "        print(f\"ë¶„ì„ëœ ë°ì´í„°: {total_rows:,}í–‰\")\n",
    "        print(f\"ë¶„ì„ëœ í”¼ì²˜: {len(feature_names)}ê°œ\")\n",
    "        print(f\"ìƒê´€ê´€ê³„ ë§¤íŠ¸ë¦­ìŠ¤ í¬ê¸°: {correlation_matrix.shape}\")\n",
    "        \n",
    "        # ë†’ì€ ìƒê´€ê´€ê³„ ë¶„ì„\n",
    "        def analyze_correlations(corr_matrix, features, threshold=0.7):\n",
    "            high_corr = []\n",
    "            n = len(features)\n",
    "            for i in range(n):\n",
    "                for j in range(i+1, n):\n",
    "                    corr_val = corr_matrix[i, j]\n",
    "                    if abs(corr_val) > threshold:\n",
    "                        high_corr.append({\n",
    "                            'feature1': features[i],\n",
    "                            'feature2': features[j],\n",
    "                            'correlation': float(corr_val)\n",
    "                        })\n",
    "            return high_corr\n",
    "        \n",
    "        # ë†’ì€ ìƒê´€ê´€ê³„ ì¶œë ¥\n",
    "        high_correlations = analyze_correlations(correlation_matrix, feature_names, 0.7)\n",
    "        print(f\"\\në†’ì€ ìƒê´€ê´€ê³„ (|r| > 0.7): {len(high_correlations)}ê°œ\")\n",
    "        \n",
    "        for pair in sorted(high_correlations, key=lambda x: abs(x['correlation']), reverse=True)[:20]:\n",
    "            print(f\"  {pair['feature1']} â†” {pair['feature2']}: {pair['correlation']:.3f}\")\n",
    "        \n",
    "        # ë‹¤ì¤‘ê³µì„ ì„± ìœ„í—˜\n",
    "        multicollinear = analyze_correlations(correlation_matrix, feature_names, 0.9)\n",
    "        print(f\"\\në‹¤ì¤‘ê³µì„ ì„± ìœ„í—˜ (|r| > 0.9): {len(multicollinear)}ê°œ\")\n",
    "        \n",
    "        for pair in sorted(multicollinear, key=lambda x: abs(x['correlation']), reverse=True):\n",
    "            print(f\"  âš ï¸ {pair['feature1']} â†” {pair['feature2']}: {pair['correlation']:.3f}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"âŒ ì˜¤ë¥˜ ë°œìƒ: {str(e)}\")\n",
    "    print(\"CPUë¡œ fallbackì„ ì‹œë„í•˜ê±°ë‚˜ ì²­í¬ í¬ê¸°ë¥¼ ë” ì¤„ì—¬ë³´ì„¸ìš”.\")\n",
    "\n",
    "# 5. ë©”ëª¨ë¦¬ ì •ë¦¬\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    final_memory = torch.cuda.memory_allocated() / 1024**3\n",
    "    peak_memory = torch.cuda.max_memory_allocated() / 1024**3\n",
    "    print(f\"\\nğŸ§¹ ìµœì¢… GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {final_memory:.2f} GB\")\n",
    "    print(f\"ğŸ“ˆ ìµœëŒ€ GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {peak_memory:.2f} GB\")\n",
    "\n",
    "print(\"\\nâœ… GPU ìƒê´€ê´€ê³„ ë¶„ì„ ì™„ë£Œ!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0bdd5c27-0e0e-4d96-a603-9f78e5bac07a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### ê¸°ê°„ë³„ ìƒê´€ê´€ê³„ ë¶„ì„ (GPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aac7d9c6-fd2a-4d8a-85ff-896dd9c39842",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pyspark.sql.functions import col\n",
    "import time\n",
    "import re\n",
    "\n",
    "print(\"=== ê¸°ê°„ë³„ ì»¬ëŸ¼ ë¶„ë¥˜ í›„ GPU ìƒê´€ê´€ê³„ ë¶„ì„ ===\")\n",
    "\n",
    "# 1. GPU ë©”ëª¨ë¦¬ ìµœì í™” ì„¤ì •\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"ì‚¬ìš© ë””ë°”ì´ìŠ¤: {device}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    gpu_props = torch.cuda.get_device_properties(0)\n",
    "    total_memory = gpu_props.total_memory / 1024**3\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU ì´ ë©”ëª¨ë¦¬: {total_memory:.1f} GB\")\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "# 2. ë°ì´í„° ë¡œë“œ\n",
    "ps_df = ps_period_df\n",
    "print(f\"ì „ì²´ ë°ì´í„° ìˆ˜: {ps_df.count():,}\")\n",
    "\n",
    "# 3. ê¸°ê°„ë³„ ì»¬ëŸ¼ ë¶„ë¥˜ í•¨ìˆ˜ (ê°œì„ ëœ ë²„ì „)\n",
    "def classify_columns_by_period(df_columns):\n",
    "    \"\"\"\n",
    "    ì»¬ëŸ¼ëª…ì„ ê¸°ê°„ë³„ë¡œ ë¶„ë¥˜í•˜ëŠ” í•¨ìˆ˜\n",
    "    \"\"\"\n",
    "    period_groups = {\n",
    "        'R3M': [],\n",
    "        'R6M': [],\n",
    "        'R12M': []\n",
    "    }\n",
    "    \n",
    "    # ê¸°ê°„ íŒ¨í„´ ì •ì˜ (ë” í¬ê´„ì ìœ¼ë¡œ)\n",
    "    period_patterns = {\n",
    "        'R3M': re.compile(r'.*_R3M$'),\n",
    "        'R6M': re.compile(r'.*_R6M$'),\n",
    "        'R12M': re.compile(r'.*_R12M$')\n",
    "    }\n",
    "    \n",
    "    non_period_cols = []  # ê¸°ê°„ì´ ì—†ëŠ” ì»¬ëŸ¼ë“¤\n",
    "    \n",
    "    for col_name in df_columns:\n",
    "        # ì œì™¸í•  ì»¬ëŸ¼ë“¤\n",
    "        if col_name in ['ê¸°ì¤€ë…„ì›”', 'ë°œê¸‰íšŒì›ë²ˆí˜¸']:\n",
    "            continue\n",
    "            \n",
    "        # ê° ê¸°ê°„ íŒ¨í„´ì— ë§¤ì¹­ë˜ëŠ”ì§€ í™•ì¸\n",
    "        matched = False\n",
    "        for period, pattern in period_patterns.items():\n",
    "            if pattern.match(col_name):\n",
    "                period_groups[period].append(col_name)\n",
    "                matched = True\n",
    "                break\n",
    "        \n",
    "        if not matched:\n",
    "            non_period_cols.append(col_name)\n",
    "    \n",
    "    period_groups['non_period'] = non_period_cols\n",
    "    return period_groups\n",
    "\n",
    "# 4. ì»¬ëŸ¼ ë¶„ë¥˜ ì‹¤í–‰\n",
    "column_groups = classify_columns_by_period(ps_df.columns)\n",
    "\n",
    "print(\"=== ê¸°ê°„ë³„ ì»¬ëŸ¼ ë¶„ë¥˜ ê²°ê³¼ ===\")\n",
    "for period, cols in column_groups.items():\n",
    "    print(f\"{period}: {len(cols)}ê°œ ì»¬ëŸ¼\")\n",
    "    if len(cols) > 0:\n",
    "        print(f\"  ì˜ˆì‹œ: {cols[:3]}...\")\n",
    "print()\n",
    "\n",
    "# 5. GPU ìƒê´€ê´€ê³„ ë¶„ì„ í•¨ìˆ˜ (ê¸°ê°„ë³„ ì ìš©)\n",
    "def compute_period_correlation_gpu(df, period_columns, period_name, device):\n",
    "    \"\"\"\n",
    "    íŠ¹ì • ê¸°ê°„ì˜ ì»¬ëŸ¼ë“¤ì— ëŒ€í•´ GPU ìƒê´€ê´€ê³„ ê³„ì‚°\n",
    "    \"\"\"\n",
    "    if len(period_columns) == 0:\n",
    "        print(f\"âš ï¸ {period_name}: ë¶„ì„í•  ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "        return None, []\n",
    "    \n",
    "    print(f\"\\n=== {period_name} ê¸°ê°„ GPU ìƒê´€ê´€ê³„ ë¶„ì„ ===\")\n",
    "    print(f\"ë¶„ì„ ì»¬ëŸ¼ ìˆ˜: {len(period_columns)}\")\n",
    "    \n",
    "    # ìˆ˜ì¹˜í˜• ì»¬ëŸ¼ë§Œ í•„í„°ë§\n",
    "    numeric_period_cols = []\n",
    "    for col_name in period_columns:\n",
    "        col_type = dict(df.dtypes)[col_name]\n",
    "        if col_type in ['int', 'bigint', 'float', 'double']:\n",
    "            numeric_period_cols.append(col_name)\n",
    "    \n",
    "    if len(numeric_period_cols) == 0:\n",
    "        print(f\"âš ï¸ {period_name}: ìˆ˜ì¹˜í˜• ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "        return None, []\n",
    "    \n",
    "    print(f\"ìˆ˜ì¹˜í˜• ì»¬ëŸ¼ ìˆ˜: {len(numeric_period_cols)}\")\n",
    "    \n",
    "    # ë©”ëª¨ë¦¬ íš¨ìœ¨ì ì¸ ì²­í¬ í¬ê¸° ê³„ì‚°\n",
    "    n_features = len(numeric_period_cols)\n",
    "    if torch.cuda.is_available():\n",
    "        # í”¼ì²˜ ìˆ˜ì— ë”°ë¥¸ ë™ì  ì²­í¬ í¬ê¸° ì¡°ì •\n",
    "        if n_features <= 10:\n",
    "            chunk_size = 5000000  # í”¼ì²˜ê°€ ì ìœ¼ë©´ ë” í° ì²­í¬\n",
    "        elif n_features <= 20:\n",
    "            chunk_size = 3000000\n",
    "        elif n_features <= 50:\n",
    "            chunk_size = 2000000\n",
    "        else:\n",
    "            chunk_size = 1000000  # í”¼ì²˜ê°€ ë§ìœ¼ë©´ ì‘ì€ ì²­í¬\n",
    "    else:\n",
    "        chunk_size = 500000\n",
    "    \n",
    "    print(f\"ì²­í¬ í¬ê¸°: {chunk_size:,}\")\n",
    "    \n",
    "    # ì „ì²´ ë°ì´í„° ì²˜ë¦¬\n",
    "    total_rows = df.count()\n",
    "    n_chunks = max(1, (total_rows + chunk_size - 1) // chunk_size)\n",
    "    print(f\"ì´ {n_chunks}ê°œ ì²­í¬ë¡œ ë¶„í• \")\n",
    "    \n",
    "    correlation_sum = None\n",
    "    total_weight = 0\n",
    "    \n",
    "    for chunk_idx in range(n_chunks):\n",
    "        chunk_start_time = time.time()\n",
    "        print(f\"\\n  ì²­í¬ {chunk_idx + 1}/{n_chunks} ì²˜ë¦¬ ì¤‘...\")\n",
    "        \n",
    "        # ì²­í¬ ë°ì´í„° ì¶”ì¶œ\n",
    "        if n_chunks == 1:\n",
    "            chunk_df = df\n",
    "        else:\n",
    "            chunk_fraction = 1.0 / n_chunks\n",
    "            chunk_df = df.sample(fraction=chunk_fraction, seed=42 + chunk_idx)\n",
    "        \n",
    "        # Pandasë¡œ ë³€í™˜\n",
    "        chunk_pdf = chunk_df.select(*numeric_period_cols).fillna(0).toPandas()\n",
    "        actual_chunk_size = len(chunk_pdf)\n",
    "        \n",
    "        if actual_chunk_size == 0:\n",
    "            continue\n",
    "        \n",
    "        print(f\"    ì‹¤ì œ ì²­í¬ í¬ê¸°: {actual_chunk_size:,} x {len(chunk_pdf.columns)}\")\n",
    "        \n",
    "        try:\n",
    "            # GPU í…ì„œë¡œ ë³€í™˜\n",
    "            chunk_tensor = torch.tensor(chunk_pdf.values, dtype=torch.float32).to(device)\n",
    "            \n",
    "            if torch.cuda.is_available():\n",
    "                memory_used = torch.cuda.memory_allocated() / 1024**3\n",
    "                print(f\"    GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {memory_used:.2f} GB\")\n",
    "            \n",
    "            # ìƒê´€ê´€ê³„ ê³„ì‚°\n",
    "            chunk_correlation = torch.corrcoef(chunk_tensor.T)\n",
    "            chunk_corr_cpu = chunk_correlation.cpu().numpy()\n",
    "            \n",
    "            # ê°€ì¤‘ í‰ê· ìœ¼ë¡œ ëˆ„ì \n",
    "            weight = actual_chunk_size\n",
    "            if correlation_sum is None:\n",
    "                correlation_sum = chunk_corr_cpu * weight\n",
    "            else:\n",
    "                correlation_sum += chunk_corr_cpu * weight\n",
    "            total_weight += weight\n",
    "            \n",
    "            # ë©”ëª¨ë¦¬ ì •ë¦¬\n",
    "            del chunk_tensor, chunk_correlation, chunk_pdf\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "            \n",
    "            chunk_time = time.time() - chunk_start_time\n",
    "            print(f\"    ì²­í¬ ì²˜ë¦¬ ì‹œê°„: {chunk_time:.2f}ì´ˆ\")\n",
    "            \n",
    "        except RuntimeError as e:\n",
    "            if \"out of memory\" in str(e):\n",
    "                print(f\"    âš ï¸ GPU ë©”ëª¨ë¦¬ ë¶€ì¡±, ì²­í¬ í¬ê¸° ì¡°ì • í•„ìš”\")\n",
    "                return None, numeric_period_cols\n",
    "            else:\n",
    "                raise e\n",
    "    \n",
    "    # ìµœì¢… ìƒê´€ê´€ê³„ ë§¤íŠ¸ë¦­ìŠ¤ ê³„ì‚°\n",
    "    if total_weight > 0:\n",
    "        final_correlation = correlation_sum / total_weight\n",
    "        return final_correlation, numeric_period_cols\n",
    "    else:\n",
    "        return None, numeric_period_cols\n",
    "\n",
    "# 6. ìƒê´€ê´€ê³„ ë¶„ì„ í•¨ìˆ˜\n",
    "def analyze_correlations(corr_matrix, features, threshold=0.7):\n",
    "    \"\"\"ë†’ì€ ìƒê´€ê´€ê³„ ë¶„ì„\"\"\"\n",
    "    high_corr = []\n",
    "    n = len(features)\n",
    "    for i in range(n):\n",
    "        for j in range(i+1, n):\n",
    "            corr_val = corr_matrix[i, j]\n",
    "            if abs(corr_val) > threshold:\n",
    "                high_corr.append({\n",
    "                    'feature1': features[i],\n",
    "                    'feature2': features[j],\n",
    "                    'correlation': float(corr_val)\n",
    "                })\n",
    "    return high_corr\n",
    "\n",
    "# 7. ê° ê¸°ê°„ë³„ë¡œ ìƒê´€ê´€ê³„ ë¶„ì„ ì‹¤í–‰\n",
    "period_results = {}\n",
    "analysis_start_time = time.time()\n",
    "\n",
    "for period_name, period_cols in column_groups.items():\n",
    "    if period_name == 'non_period':\n",
    "        continue  # ê¸°ê°„ì´ ì—†ëŠ” ì»¬ëŸ¼ì€ ë³„ë„ ì²˜ë¦¬\n",
    "    \n",
    "    if len(period_cols) == 0:\n",
    "        continue\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"ğŸ” {period_name} ê¸°ê°„ ë¶„ì„ ì‹œì‘\")\n",
    "    \n",
    "    period_start_time = time.time()\n",
    "    \n",
    "    # GPU ìƒê´€ê´€ê³„ ë¶„ì„\n",
    "    correlation_matrix, feature_names = compute_period_correlation_gpu(\n",
    "        ps_df, period_cols, period_name, device\n",
    "    )\n",
    "    \n",
    "    if correlation_matrix is not None:\n",
    "        period_time = time.time() - period_start_time\n",
    "        \n",
    "        print(f\"\\n=== {period_name} ë¶„ì„ ê²°ê³¼ ===\")\n",
    "        print(f\"ì²˜ë¦¬ ì‹œê°„: {period_time:.2f}ì´ˆ\")\n",
    "        print(f\"ë¶„ì„ëœ í”¼ì²˜: {len(feature_names)}ê°œ\")\n",
    "        print(f\"ìƒê´€ê´€ê³„ ë§¤íŠ¸ë¦­ìŠ¤ í¬ê¸°: {correlation_matrix.shape}\")\n",
    "        \n",
    "        # ë†’ì€ ìƒê´€ê´€ê³„ ë¶„ì„\n",
    "        high_correlations = analyze_correlations(correlation_matrix, feature_names, 0.7)\n",
    "        print(f\"ë†’ì€ ìƒê´€ê´€ê³„ (|r| > 0.7): {len(high_correlations)}ê°œ\")\n",
    "        \n",
    "        # ìƒìœ„ 10ê°œ ì¶œë ¥\n",
    "        for pair in sorted(high_correlations, key=lambda x: abs(x['correlation']), reverse=True)[:100]:\n",
    "            print(f\"  {pair['feature1']} â†” {pair['feature2']}: {pair['correlation']:.3f}\")\n",
    "        \n",
    "        # ë‹¤ì¤‘ê³µì„ ì„± ìœ„í—˜\n",
    "        multicollinear = analyze_correlations(correlation_matrix, feature_names, 0.9)\n",
    "        print(f\"ë‹¤ì¤‘ê³µì„ ì„± ìœ„í—˜ (|r| > 0.9): {len(multicollinear)}ê°œ\")\n",
    "        \n",
    "        for pair in sorted(multicollinear, key=lambda x: abs(x['correlation']), reverse=True)[:50]:\n",
    "            print(f\"  âš ï¸ {pair['feature1']} â†” {pair['feature2']}: {pair['correlation']:.3f}\")\n",
    "        \n",
    "        # ê²°ê³¼ ì €ì¥\n",
    "        period_results[period_name] = {\n",
    "            'correlation_matrix': correlation_matrix,\n",
    "            'feature_names': feature_names,\n",
    "            'high_correlations': high_correlations,\n",
    "            'multicollinear': multicollinear,\n",
    "            'processing_time': period_time\n",
    "        }\n",
    "    \n",
    "    else:\n",
    "        print(f\"âŒ {period_name} ë¶„ì„ ì‹¤íŒ¨\")\n",
    "\n",
    "# 8. ì „ì²´ ê²°ê³¼ ìš”ì•½\n",
    "total_analysis_time = time.time() - analysis_start_time\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"ğŸ¯ ì „ì²´ ê¸°ê°„ë³„ ìƒê´€ê´€ê³„ ë¶„ì„ ì™„ë£Œ!\")\n",
    "print(f\"ì´ ì²˜ë¦¬ ì‹œê°„: {total_analysis_time:.2f}ì´ˆ ({total_analysis_time/60:.1f}ë¶„)\")\n",
    "\n",
    "print(f\"\\n=== ê¸°ê°„ë³„ ë¶„ì„ ìš”ì•½ ===\")\n",
    "for period_name, results in period_results.items():\n",
    "    print(f\"{period_name}:\")\n",
    "    print(f\"  - í”¼ì²˜ ìˆ˜: {len(results['feature_names'])}\")\n",
    "    print(f\"  - ë†’ì€ ìƒê´€ê´€ê³„: {len(results['high_correlations'])}ê°œ\")\n",
    "    print(f\"  - ë‹¤ì¤‘ê³µì„ ì„± ìœ„í—˜: {len(results['multicollinear'])}ê°œ\")\n",
    "    print(f\"  - ì²˜ë¦¬ ì‹œê°„: {results['processing_time']:.2f}ì´ˆ\")\n",
    "\n",
    "# 9. ê¸°ê°„ ê°„ ë¹„êµ ë¶„ì„ (ì˜µì…˜)\n",
    "print(f\"\\n=== ê¸°ê°„ ê°„ ìƒê´€ê´€ê³„ íŒ¨í„´ ë¹„êµ ===\")\n",
    "for period_name, results in period_results.items():\n",
    "    if len(results['high_correlations']) > 0:\n",
    "        print(f\"\\n{period_name} ì£¼ìš” ìƒê´€ê´€ê³„:\")\n",
    "        # ê°€ì¥ ë†’ì€ ìƒê´€ê´€ê³„ 3ê°œ\n",
    "        top_corrs = sorted(results['high_correlations'], \n",
    "                          key=lambda x: abs(x['correlation']), reverse=True)[:3]\n",
    "        for i, pair in enumerate(top_corrs, 1):\n",
    "            print(f\"  {i}. {pair['feature1']} â†” {pair['feature2']}: {pair['correlation']:.3f}\")\n",
    "\n",
    "# 10. ë©”ëª¨ë¦¬ ì •ë¦¬\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    final_memory = torch.cuda.memory_allocated() / 1024**3\n",
    "    peak_memory = torch.cuda.max_memory_allocated() / 1024**3\n",
    "    print(f\"\\nğŸ§¹ ìµœì¢… GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {final_memory:.2f} GB\")\n",
    "    print(f\"ğŸ“ˆ ìµœëŒ€ GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {peak_memory:.2f} GB\")\n",
    "\n",
    "print(\"\\nâœ… ê¸°ê°„ë³„ GPU ìƒê´€ê´€ê³„ ë¶„ì„ ì™„ë£Œ!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fb3421e2-f9a5-4d82-b67a-96e264e6f1a9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### ë‹¤ì¤‘ê³µì‹ ì„± ì œê±°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "039aa5ae-6081-41b8-a2b5-0261a3559da5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def analyze_multicollinearity_and_recommend_drops(ps_period_df, table_name):\n",
    "    \"\"\"\n",
    "    ë‹¤ì¤‘ê³µì„ ì„± ë¶„ì„ í›„ ì‚­ì œí•  ì»¬ëŸ¼ ì¶”ì²œ\n",
    "    \"\"\"\n",
    "    print(f\"=== {table_name} ë‹¤ì¤‘ê³µì„ ì„± í•´ê²° ì „ëµ ===\")\n",
    "    \n",
    "    # ê° í…Œì´ë¸”ë³„ ì‚­ì œ ì¶”ì²œ ì»¬ëŸ¼ ì •ì˜\n",
    "    if table_name == \"count_df\":\n",
    "        drop_recommendations = {\n",
    "            # 1. ì™„ì „ ì¤‘ë³µ (ìƒê´€ê³„ìˆ˜ 1.0)\n",
    "            \"ì™„ì „_ì¤‘ë³µ\": [\n",
    "                \"í• ë¶€ê±´ìˆ˜_ë¶€ë¶„_12M_R12M\"  # ì´ìš©ê±´ìˆ˜_ë¶€ë¶„ë¬´ì´ì_R12Mê³¼ ì™„ì „ ë™ì¼\n",
    "            ],\n",
    "            \n",
    "            # 2. ì‹ ìš©ì¹´ë“œ ê´€ë ¨ ì¤‘ë³µ (ì‹ ìš© > ì‹ íŒ > ì¼ì‹œë¶ˆ ìˆœìœ¼ë¡œ í¬ê´„ì )\n",
    "            \"ì‹ ìš©ì¹´ë“œ_ì¤‘ë³µ\": [\n",
    "                \"ì´ìš©ê±´ìˆ˜_ì‹ íŒ_B0M\",      # ì‹ ìš©ì¹´ë“œì˜ í•˜ìœ„ ê°œë…\n",
    "                \"ì´ìš©ê±´ìˆ˜_ì¼ì‹œë¶ˆ_B0M\",     # ì‹ íŒì˜ í•˜ìœ„ ê°œë…\n",
    "                \"ì´ìš©ê±´ìˆ˜_ì‹ íŒ_R3M\", \n",
    "                \"ì´ìš©ê±´ìˆ˜_ì¼ì‹œë¶ˆ_R3M\",\n",
    "                \"ì´ìš©ê±´ìˆ˜_ì‹ íŒ_R6M\",\n",
    "                \"ì´ìš©ê±´ìˆ˜_ì¼ì‹œë¶ˆ_R6M\", \n",
    "                \"ì´ìš©ê±´ìˆ˜_ì‹ íŒ_R12M\",\n",
    "                \"ì´ìš©ê±´ìˆ˜_ì¼ì‹œë¶ˆ_R12M\"\n",
    "            ],\n",
    "            \n",
    "            # 3. í• ë¶€ ê´€ë ¨ ì¤‘ë³µ (ì „ì²´ í• ë¶€ê°€ ë¬´ì´ì í• ë¶€ë¥¼ í¬í•¨)\n",
    "            \"í• ë¶€_ì¤‘ë³µ\": [\n",
    "                \"ì´ìš©ê±´ìˆ˜_í• ë¶€_ë¬´ì´ì_B0M\",   # ì „ì²´ í• ë¶€ì— í¬í•¨ë¨\n",
    "                \"ì´ìš©ê±´ìˆ˜_í• ë¶€_ë¬´ì´ì_R3M\",\n",
    "                \"ì´ìš©ê±´ìˆ˜_í• ë¶€_ë¬´ì´ì_R6M\"\n",
    "            ],\n",
    "            \n",
    "            # 4. ìŠ¹ì¸ê±°ì ˆ ê´€ë ¨ ì¤‘ë³µ (ì „ì²´ê°€ ì„¸ë¶€ì‚¬í•­ í¬í•¨)\n",
    "            \"ìŠ¹ì¸ê±°ì ˆ_ì¤‘ë³µ\": [\n",
    "                \"ìŠ¹ì¸ê±°ì ˆê±´ìˆ˜_í•œë„ì´ˆê³¼_B0M\"   # ì „ì²´ ìŠ¹ì¸ê±°ì ˆì— í¬í•¨ë¨\n",
    "            ],\n",
    "            \n",
    "            # 5. í˜ì´ ê´€ë ¨ ì¤‘ë³µ (ì˜¨ë¼ì¸í˜ì´ê°€ ê°„í¸ê²°ì œ í¬í•¨)\n",
    "            \"í˜ì´_ì¤‘ë³µ\": [\n",
    "                \"ì´ìš©ê±´ìˆ˜_ê°„í¸ê²°ì œ_R3M\",      # ì˜¨ë¼ì¸í˜ì´ì— í¬í•¨ë¨\n",
    "                \"ì´ìš©ê±´ìˆ˜_ê°„í¸ê²°ì œ_R6M\"\n",
    "            ],\n",
    "            \n",
    "            # 6. í• ë¶€ê¸°ê°„ ê´€ë ¨ ì¤‘ë³µ\n",
    "            \"í• ë¶€ê¸°ê°„_ì¤‘ë³µ\": [\n",
    "                \"í• ë¶€ê±´ìˆ˜_ìœ ì´ì_14M_R12M\"    # 14M ì „ì²´ì— í¬í•¨ë¨\n",
    "            ]\n",
    "        }\n",
    "        \n",
    "    elif table_name == \"use_month_df\":\n",
    "        drop_recommendations = {\n",
    "            # 1. ì‹ ìš©ì¹´ë“œ ê´€ë ¨ ì¤‘ë³µ (ì‹ ìš© > ì‹ íŒ > ì¼ì‹œë¶ˆ)\n",
    "            \"ì‹ ìš©ì¹´ë“œ_ì¤‘ë³µ\": [\n",
    "                \"ì´ìš©ê°œì›”ìˆ˜_ì‹ íŒ_R3M\",\n",
    "                \"ì´ìš©ê°œì›”ìˆ˜_ì¼ì‹œë¶ˆ_R3M\", \n",
    "                \"ì´ìš©ê°œì›”ìˆ˜_ì‹ íŒ_R6M\",\n",
    "                \"ì´ìš©ê°œì›”ìˆ˜_ì¼ì‹œë¶ˆ_R6M\",\n",
    "                \"ì´ìš©ê°œì›”ìˆ˜_ì‹ íŒ_R12M\", \n",
    "                \"ì´ìš©ê°œì›”ìˆ˜_ì¼ì‹œë¶ˆ_R12M\"\n",
    "            ],\n",
    "            \n",
    "            # 2. í• ë¶€ ê´€ë ¨ ì¤‘ë³µ\n",
    "            \"í• ë¶€_ì¤‘ë³µ\": [\n",
    "                \"ì´ìš©ê°œì›”ìˆ˜_í• ë¶€_ë¬´ì´ì_R3M\",\n",
    "                \"ì´ìš©ê°œì›”ìˆ˜_í• ë¶€_ë¬´ì´ì_R6M\", \n",
    "                \"ì´ìš©ê°œì›”ìˆ˜_í• ë¶€_ë¬´ì´ì_R12M\"\n",
    "            ],\n",
    "            \n",
    "            # 3. ì „ì²´/ê²°ì œì¼ ì¤‘ë³µ (ì „ì²´ê°€ ë” í¬ê´„ì )\n",
    "            \"ì „ì²´ê²°ì œ_ì¤‘ë³µ\": [\n",
    "                \"ì´ìš©ê°œì›”ìˆ˜_ê²°ì œì¼_R3M\",\n",
    "                \"ì´ìš©ê°œì›”ìˆ˜_ê²°ì œì¼_R6M\"\n",
    "            ],\n",
    "            \n",
    "            # 4. í˜ì´ ê´€ë ¨ ì¤‘ë³µ\n",
    "            \"í˜ì´_ì¤‘ë³µ\": [\n",
    "                \"ì´ìš©ê°œì›”ìˆ˜_ê°„í¸ê²°ì œ_R6M\",    # ì˜¨ë¼ì¸í˜ì´ì— í¬í•¨\n",
    "                \"ì´ìš©ê°œì›”ìˆ˜_Aí˜ì´_R6M\"        # ì˜¤í”„ë¼ì¸í˜ì´ì— í¬í•¨\n",
    "            ],\n",
    "            \n",
    "            # 5. ì˜¤í”„ë¼ì¸ ì¤‘ë³µ (ì‹ ìš©ì¹´ë“œ ì‚¬ìš©ì´ ëŒ€ë¶€ë¶„ ì˜¤í”„ë¼ì¸ í¬í•¨)\n",
    "            \"ì˜¤í”„ë¼ì¸_ì¤‘ë³µ\": [\n",
    "                \"ì´ìš©ê°œì›”ìˆ˜_ì˜¤í”„ë¼ì¸_R6M\"     # ì‹ ìš©ì¹´ë“œ ì‚¬ìš©ì— í¬í•¨ë¨\n",
    "            ]\n",
    "        }\n",
    "    \n",
    "    # ì‚­ì œí•  ì „ì²´ ì»¬ëŸ¼ ë¦¬ìŠ¤íŠ¸ ìƒì„±\n",
    "    all_drops = []\n",
    "    for category, columns in drop_recommendations.items():\n",
    "        all_drops.extend(columns)\n",
    "    \n",
    "    print(f\"\\nğŸ“‹ ì‚­ì œ ì¶”ì²œ ì»¬ëŸ¼ ({len(all_drops)}ê°œ):\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    for category, columns in drop_recommendations.items():\n",
    "        print(f\"\\nğŸ”¸ {category}:\")\n",
    "        for i, col in enumerate(columns, 1):\n",
    "            print(f\"   {i}. {col}\")\n",
    "    \n",
    "    # ì‚­ì œ ì „í›„ ë¹„êµ\n",
    "    original_cols = len(ps_df.columns)\n",
    "    remaining_cols = original_cols - len(all_drops)\n",
    "    \n",
    "    print(f\"\\nğŸ“Š ì‚­ì œ ì „í›„ ë¹„êµ:\")\n",
    "    print(f\"   ì›ë³¸ ì»¬ëŸ¼ ìˆ˜: {original_cols}\")\n",
    "    print(f\"   ì‚­ì œ ì»¬ëŸ¼ ìˆ˜: {len(all_drops)}\")\n",
    "    print(f\"   ë‚¨ì€ ì»¬ëŸ¼ ìˆ˜: {remaining_cols}\")\n",
    "    print(f\"   ì‚­ì œ ë¹„ìœ¨: {len(all_drops)/original_cols*100:.1f}%\")\n",
    "    \n",
    "    return all_drops, drop_recommendations\n",
    "\n",
    "def apply_multicollinearity_solution(ps_df, table_name):\n",
    "    \"\"\"\n",
    "    ë‹¤ì¤‘ê³µì„ ì„± í•´ê²°ì„ ìœ„í•œ ì»¬ëŸ¼ ì‚­ì œ ì‹¤í–‰\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"ğŸ”§ {table_name} ë‹¤ì¤‘ê³µì„ ì„± í•´ê²° ì‹¤í–‰\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # ì‚­ì œ ì¶”ì²œ ë¶„ì„\n",
    "    drop_columns, drop_categories = analyze_multicollinearity_and_recommend_drops(ps_df, table_name)\n",
    "    \n",
    "    # ì‹¤ì œ ì¡´ì¬í•˜ëŠ” ì»¬ëŸ¼ë§Œ í•„í„°ë§\n",
    "    existing_drops = [col for col in drop_columns if col in ps_df.columns]\n",
    "    missing_cols = [col for col in drop_columns if col not in ps_df.columns]\n",
    "    \n",
    "    if missing_cols:\n",
    "        print(f\"\\nâš ï¸ ì¡´ì¬í•˜ì§€ ì•ŠëŠ” ì»¬ëŸ¼ë“¤ ({len(missing_cols)}ê°œ):\")\n",
    "        for col in missing_cols:\n",
    "            print(f\"   - {col}\")\n",
    "    \n",
    "    print(f\"\\nâœ… ì‹¤ì œ ì‚­ì œí•  ì»¬ëŸ¼ë“¤ ({len(existing_drops)}ê°œ):\")\n",
    "    for i, col in enumerate(existing_drops, 1):\n",
    "        print(f\"   {i:2d}. {col}\")\n",
    "    \n",
    "    # ì»¬ëŸ¼ ì‚­ì œ ì‹¤í–‰\n",
    "    if existing_drops:\n",
    "        ps_df_cleaned = ps_df.drop(*existing_drops)\n",
    "        \n",
    "        print(f\"\\nğŸ¯ ì‚­ì œ ì™„ë£Œ!\")\n",
    "        print(f\"   ì‚­ì œ ì „: {len(ps_df.columns)} ì»¬ëŸ¼\")\n",
    "        print(f\"   ì‚­ì œ í›„: {len(ps_df_cleaned.columns)} ì»¬ëŸ¼\")\n",
    "        print(f\"   ì‹¤ì œ ì‚­ì œ: {len(existing_drops)} ì»¬ëŸ¼\")\n",
    "        \n",
    "        return ps_df_cleaned, existing_drops\n",
    "    else:\n",
    "        print(\"\\nâŒ ì‚­ì œí•  ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "        return ps_df, []\n",
    "\n",
    "def validate_multicollinearity_reduction(ps_df_original, ps_df_cleaned, table_name):\n",
    "    \"\"\"\n",
    "    ë‹¤ì¤‘ê³µì„ ì„± í•´ê²° íš¨ê³¼ ê²€ì¦\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"ğŸ” {table_name} ë‹¤ì¤‘ê³µì„ ì„± í•´ê²° íš¨ê³¼ ê²€ì¦\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # ê°„ë‹¨í•œ ìƒê´€ê´€ê³„ ì²´í¬ (ìƒ˜í”Œë§)\n",
    "    sample_df = ps_df_cleaned.sample(fraction=0.01, seed=42)\n",
    "    numeric_cols = [col_name for col_name, data_type in sample_df.dtypes\n",
    "                    if data_type in ['int', 'bigint', 'float', 'double']]\n",
    "    \n",
    "    if len(numeric_cols) > 1:\n",
    "        # Pandas ë³€í™˜ í›„ ìƒê´€ê´€ê³„ ê³„ì‚°\n",
    "        sample_pdf = sample_df.select(*numeric_cols).fillna(0).toPandas()\n",
    "        corr_matrix = sample_pdf.corr()\n",
    "        \n",
    "        # ë‹¤ì¤‘ê³µì„ ì„± ìœ„í—˜ ì¹´ìš´íŠ¸\n",
    "        high_corr_count = 0\n",
    "        very_high_corr_count = 0\n",
    "        \n",
    "        n = len(numeric_cols)\n",
    "        for i in range(n):\n",
    "            for j in range(i+1, n):\n",
    "                corr_val = abs(corr_matrix.iloc[i, j])\n",
    "                if corr_val > 0.9:\n",
    "                    very_high_corr_count += 1\n",
    "                elif corr_val > 0.7:\n",
    "                    high_corr_count += 1\n",
    "        \n",
    "        print(f\"ğŸ“Š ì •ë¦¬ í›„ ìƒê´€ê´€ê³„ í˜„í™© (ìƒ˜í”Œ ê¸°ì¤€):\")\n",
    "        print(f\"   ë¶„ì„ ì»¬ëŸ¼ ìˆ˜: {len(numeric_cols)}\")\n",
    "        print(f\"   ë‹¤ì¤‘ê³µì„ ì„± ìœ„í—˜ (>0.9): {very_high_corr_count}ê°œ\")\n",
    "        print(f\"   ë†’ì€ ìƒê´€ê´€ê³„ (0.7-0.9): {high_corr_count}ê°œ\")\n",
    "        \n",
    "        if very_high_corr_count == 0:\n",
    "            print(\"   âœ… ë‹¤ì¤‘ê³µì„ ì„± ìœ„í—˜ì´ í•´ê²°ë˜ì—ˆìŠµë‹ˆë‹¤!\")\n",
    "        else:\n",
    "            print(\"   âš ï¸ ì¼ë¶€ ë‹¤ì¤‘ê³µì„ ì„±ì´ ë‚¨ì•„ìˆìŠµë‹ˆë‹¤.\")\n",
    "    \n",
    "    return ps_df_cleaned\n",
    "\n",
    "# ì‹¤í–‰ ì½”ë“œ\n",
    "print(\"ğŸš€ ë‹¤ì¤‘ê³µì„ ì„± í•´ê²° í”„ë¡œì„¸ìŠ¤ ì‹œì‘\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# count_df ì²˜ë¦¬\n",
    "print(\"\\n1ï¸âƒ£ COUNT_DF ì²˜ë¦¬\")\n",
    "ps_df_count_cleaned, dropped_count = apply_multicollinearity_solution(ps_df, \"count_df\")\n",
    "ps_df_count_final = validate_multicollinearity_reduction(ps_df, ps_df_count_cleaned, \"count_df\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "\n",
    "# use_month_df ì²˜ë¦¬  \n",
    "print(\"\\n2ï¸âƒ£ USE_MONTH_DF ì²˜ë¦¬\")\n",
    "ps_df_month_cleaned, dropped_month = apply_multicollinearity_solution(ps_df, \"use_month_df\")\n",
    "ps_df_month_final = validate_multicollinearity_reduction(ps_df, ps_df_month_cleaned, \"use_month_df\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ğŸ¯ ì „ì²´ ë‹¤ì¤‘ê³µì„ ì„± í•´ê²° ì™„ë£Œ!\")\n",
    "print(f\"   COUNT_DF: {len(dropped_count)}ê°œ ì»¬ëŸ¼ ì‚­ì œ\")\n",
    "print(f\"   USE_MONTH_DF: {len(dropped_month)}ê°œ ì»¬ëŸ¼ ì‚­ì œ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cca316b4-5a46-4778-993e-1aac43d8a779",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pyspark.sql.functions import col\n",
    "import time\n",
    "import re\n",
    "\n",
    "print(\"=== ê¸°ê°„ë³„ ì»¬ëŸ¼ ë¶„ë¥˜ í›„ GPU ìƒê´€ê´€ê³„ ë¶„ì„ ===\")\n",
    "\n",
    "# 1. GPU ë©”ëª¨ë¦¬ ìµœì í™” ì„¤ì •\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"ì‚¬ìš© ë””ë°”ì´ìŠ¤: {device}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    gpu_props = torch.cuda.get_device_properties(0)\n",
    "    total_memory = gpu_props.total_memory / 1024**3\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU ì´ ë©”ëª¨ë¦¬: {total_memory:.1f} GB\")\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "# 2. ë°ì´í„° ë¡œë“œ\n",
    "ps_df = ps_df_month_final\n",
    "print(f\"ì „ì²´ ë°ì´í„° ìˆ˜: {ps_df.count():,}\")\n",
    "\n",
    "# 3. ê¸°ê°„ë³„ ì»¬ëŸ¼ ë¶„ë¥˜ í•¨ìˆ˜ (ê°œì„ ëœ ë²„ì „)\n",
    "def classify_columns_by_period(df_columns):\n",
    "    \"\"\"\n",
    "    ì»¬ëŸ¼ëª…ì„ ê¸°ê°„ë³„ë¡œ ë¶„ë¥˜í•˜ëŠ” í•¨ìˆ˜\n",
    "    \"\"\"\n",
    "    period_groups = {\n",
    "        'R3M': [],\n",
    "        'R6M': [],\n",
    "        'R12M': []\n",
    "    }\n",
    "    \n",
    "    # ê¸°ê°„ íŒ¨í„´ ì •ì˜ (ë” í¬ê´„ì ìœ¼ë¡œ)\n",
    "    period_patterns = {\n",
    "        'R3M': re.compile(r'.*_R3M$'),\n",
    "        'R6M': re.compile(r'.*_R6M$'),\n",
    "        'R12M': re.compile(r'.*_R12M$')\n",
    "    }\n",
    "    \n",
    "    non_period_cols = []  # ê¸°ê°„ì´ ì—†ëŠ” ì»¬ëŸ¼ë“¤\n",
    "    \n",
    "    for col_name in df_columns:\n",
    "        # ì œì™¸í•  ì»¬ëŸ¼ë“¤\n",
    "        if col_name in ['ê¸°ì¤€ë…„ì›”', 'ë°œê¸‰íšŒì›ë²ˆí˜¸']:\n",
    "            continue\n",
    "            \n",
    "        # ê° ê¸°ê°„ íŒ¨í„´ì— ë§¤ì¹­ë˜ëŠ”ì§€ í™•ì¸\n",
    "        matched = False\n",
    "        for period, pattern in period_patterns.items():\n",
    "            if pattern.match(col_name):\n",
    "                period_groups[period].append(col_name)\n",
    "                matched = True\n",
    "                break\n",
    "        \n",
    "        if not matched:\n",
    "            non_period_cols.append(col_name)\n",
    "    \n",
    "    period_groups['non_period'] = non_period_cols\n",
    "    return period_groups\n",
    "\n",
    "# 4. ì»¬ëŸ¼ ë¶„ë¥˜ ì‹¤í–‰\n",
    "column_groups = classify_columns_by_period(ps_df.columns)\n",
    "\n",
    "print(\"=== ê¸°ê°„ë³„ ì»¬ëŸ¼ ë¶„ë¥˜ ê²°ê³¼ ===\")\n",
    "for period, cols in column_groups.items():\n",
    "    print(f\"{period}: {len(cols)}ê°œ ì»¬ëŸ¼\")\n",
    "    if len(cols) > 0:\n",
    "        print(f\"  ì˜ˆì‹œ: {cols[:3]}...\")\n",
    "print()\n",
    "\n",
    "# 5. GPU ìƒê´€ê´€ê³„ ë¶„ì„ í•¨ìˆ˜ (ê¸°ê°„ë³„ ì ìš©)\n",
    "def compute_period_correlation_gpu(df, period_columns, period_name, device):\n",
    "    \"\"\"\n",
    "    íŠ¹ì • ê¸°ê°„ì˜ ì»¬ëŸ¼ë“¤ì— ëŒ€í•´ GPU ìƒê´€ê´€ê³„ ê³„ì‚°\n",
    "    \"\"\"\n",
    "    if len(period_columns) == 0:\n",
    "        print(f\"âš ï¸ {period_name}: ë¶„ì„í•  ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "        return None, []\n",
    "    \n",
    "    print(f\"\\n=== {period_name} ê¸°ê°„ GPU ìƒê´€ê´€ê³„ ë¶„ì„ ===\")\n",
    "    print(f\"ë¶„ì„ ì»¬ëŸ¼ ìˆ˜: {len(period_columns)}\")\n",
    "    \n",
    "    # ìˆ˜ì¹˜í˜• ì»¬ëŸ¼ë§Œ í•„í„°ë§\n",
    "    numeric_period_cols = []\n",
    "    for col_name in period_columns:\n",
    "        col_type = dict(df.dtypes)[col_name]\n",
    "        if col_type in ['int', 'bigint', 'float', 'double']:\n",
    "            numeric_period_cols.append(col_name)\n",
    "    \n",
    "    if len(numeric_period_cols) == 0:\n",
    "        print(f\"âš ï¸ {period_name}: ìˆ˜ì¹˜í˜• ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "        return None, []\n",
    "    \n",
    "    print(f\"ìˆ˜ì¹˜í˜• ì»¬ëŸ¼ ìˆ˜: {len(numeric_period_cols)}\")\n",
    "    \n",
    "    # ë©”ëª¨ë¦¬ íš¨ìœ¨ì ì¸ ì²­í¬ í¬ê¸° ê³„ì‚°\n",
    "    n_features = len(numeric_period_cols)\n",
    "    if torch.cuda.is_available():\n",
    "        # í”¼ì²˜ ìˆ˜ì— ë”°ë¥¸ ë™ì  ì²­í¬ í¬ê¸° ì¡°ì •\n",
    "        if n_features <= 10:\n",
    "            chunk_size = 5000000  # í”¼ì²˜ê°€ ì ìœ¼ë©´ ë” í° ì²­í¬\n",
    "        elif n_features <= 20:\n",
    "            chunk_size = 3000000\n",
    "        elif n_features <= 50:\n",
    "            chunk_size = 2000000\n",
    "        else:\n",
    "            chunk_size = 1000000  # í”¼ì²˜ê°€ ë§ìœ¼ë©´ ì‘ì€ ì²­í¬\n",
    "    else:\n",
    "        chunk_size = 500000\n",
    "    \n",
    "    print(f\"ì²­í¬ í¬ê¸°: {chunk_size:,}\")\n",
    "    \n",
    "    # ì „ì²´ ë°ì´í„° ì²˜ë¦¬\n",
    "    total_rows = df.count()\n",
    "    n_chunks = max(1, (total_rows + chunk_size - 1) // chunk_size)\n",
    "    print(f\"ì´ {n_chunks}ê°œ ì²­í¬ë¡œ ë¶„í• \")\n",
    "    \n",
    "    correlation_sum = None\n",
    "    total_weight = 0\n",
    "    \n",
    "    for chunk_idx in range(n_chunks):\n",
    "        chunk_start_time = time.time()\n",
    "        print(f\"\\n  ì²­í¬ {chunk_idx + 1}/{n_chunks} ì²˜ë¦¬ ì¤‘...\")\n",
    "        \n",
    "        # ì²­í¬ ë°ì´í„° ì¶”ì¶œ\n",
    "        if n_chunks == 1:\n",
    "            chunk_df = df\n",
    "        else:\n",
    "            chunk_fraction = 1.0 / n_chunks\n",
    "            chunk_df = df.sample(fraction=chunk_fraction, seed=42 + chunk_idx)\n",
    "        \n",
    "        # Pandasë¡œ ë³€í™˜\n",
    "        chunk_pdf = chunk_df.select(*numeric_period_cols).fillna(0).toPandas()\n",
    "        actual_chunk_size = len(chunk_pdf)\n",
    "        \n",
    "        if actual_chunk_size == 0:\n",
    "            continue\n",
    "        \n",
    "        print(f\"    ì‹¤ì œ ì²­í¬ í¬ê¸°: {actual_chunk_size:,} x {len(chunk_pdf.columns)}\")\n",
    "        \n",
    "        try:\n",
    "            # GPU í…ì„œë¡œ ë³€í™˜\n",
    "            chunk_tensor = torch.tensor(chunk_pdf.values, dtype=torch.float32).to(device)\n",
    "            \n",
    "            if torch.cuda.is_available():\n",
    "                memory_used = torch.cuda.memory_allocated() / 1024**3\n",
    "                print(f\"    GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {memory_used:.2f} GB\")\n",
    "            \n",
    "            # ìƒê´€ê´€ê³„ ê³„ì‚°\n",
    "            chunk_correlation = torch.corrcoef(chunk_tensor.T)\n",
    "            chunk_corr_cpu = chunk_correlation.cpu().numpy()\n",
    "            \n",
    "            # ê°€ì¤‘ í‰ê· ìœ¼ë¡œ ëˆ„ì \n",
    "            weight = actual_chunk_size\n",
    "            if correlation_sum is None:\n",
    "                correlation_sum = chunk_corr_cpu * weight\n",
    "            else:\n",
    "                correlation_sum += chunk_corr_cpu * weight\n",
    "            total_weight += weight\n",
    "            \n",
    "            # ë©”ëª¨ë¦¬ ì •ë¦¬\n",
    "            del chunk_tensor, chunk_correlation, chunk_pdf\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "            \n",
    "            chunk_time = time.time() - chunk_start_time\n",
    "            print(f\"    ì²­í¬ ì²˜ë¦¬ ì‹œê°„: {chunk_time:.2f}ì´ˆ\")\n",
    "            \n",
    "        except RuntimeError as e:\n",
    "            if \"out of memory\" in str(e):\n",
    "                print(f\"    âš ï¸ GPU ë©”ëª¨ë¦¬ ë¶€ì¡±, ì²­í¬ í¬ê¸° ì¡°ì • í•„ìš”\")\n",
    "                return None, numeric_period_cols\n",
    "            else:\n",
    "                raise e\n",
    "    \n",
    "    # ìµœì¢… ìƒê´€ê´€ê³„ ë§¤íŠ¸ë¦­ìŠ¤ ê³„ì‚°\n",
    "    if total_weight > 0:\n",
    "        final_correlation = correlation_sum / total_weight\n",
    "        return final_correlation, numeric_period_cols\n",
    "    else:\n",
    "        return None, numeric_period_cols\n",
    "\n",
    "# 6. ìƒê´€ê´€ê³„ ë¶„ì„ í•¨ìˆ˜\n",
    "def analyze_correlations(corr_matrix, features, threshold=0.7):\n",
    "    \"\"\"ë†’ì€ ìƒê´€ê´€ê³„ ë¶„ì„\"\"\"\n",
    "    high_corr = []\n",
    "    n = len(features)\n",
    "    for i in range(n):\n",
    "        for j in range(i+1, n):\n",
    "            corr_val = corr_matrix[i, j]\n",
    "            if abs(corr_val) > threshold:\n",
    "                high_corr.append({\n",
    "                    'feature1': features[i],\n",
    "                    'feature2': features[j],\n",
    "                    'correlation': float(corr_val)\n",
    "                })\n",
    "    return high_corr\n",
    "\n",
    "# 7. ê° ê¸°ê°„ë³„ë¡œ ìƒê´€ê´€ê³„ ë¶„ì„ ì‹¤í–‰\n",
    "period_results = {}\n",
    "analysis_start_time = time.time()\n",
    "\n",
    "for period_name, period_cols in column_groups.items():\n",
    "    if period_name == 'non_period':\n",
    "        continue  # ê¸°ê°„ì´ ì—†ëŠ” ì»¬ëŸ¼ì€ ë³„ë„ ì²˜ë¦¬\n",
    "    \n",
    "    if len(period_cols) == 0:\n",
    "        continue\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"ğŸ” {period_name} ê¸°ê°„ ë¶„ì„ ì‹œì‘\")\n",
    "    \n",
    "    period_start_time = time.time()\n",
    "    \n",
    "    # GPU ìƒê´€ê´€ê³„ ë¶„ì„\n",
    "    correlation_matrix, feature_names = compute_period_correlation_gpu(\n",
    "        ps_df, period_cols, period_name, device\n",
    "    )\n",
    "    \n",
    "    if correlation_matrix is not None:\n",
    "        period_time = time.time() - period_start_time\n",
    "        \n",
    "        print(f\"\\n=== {period_name} ë¶„ì„ ê²°ê³¼ ===\")\n",
    "        print(f\"ì²˜ë¦¬ ì‹œê°„: {period_time:.2f}ì´ˆ\")\n",
    "        print(f\"ë¶„ì„ëœ í”¼ì²˜: {len(feature_names)}ê°œ\")\n",
    "        print(f\"ìƒê´€ê´€ê³„ ë§¤íŠ¸ë¦­ìŠ¤ í¬ê¸°: {correlation_matrix.shape}\")\n",
    "        \n",
    "        # ë†’ì€ ìƒê´€ê´€ê³„ ë¶„ì„\n",
    "        high_correlations = analyze_correlations(correlation_matrix, feature_names, 0.7)\n",
    "        print(f\"ë†’ì€ ìƒê´€ê´€ê³„ (|r| > 0.7): {len(high_correlations)}ê°œ\")\n",
    "        \n",
    "        # ìƒìœ„ 10ê°œ ì¶œë ¥\n",
    "        for pair in sorted(high_correlations, key=lambda x: abs(x['correlation']), reverse=True)[:100]:\n",
    "            print(f\"  {pair['feature1']} â†” {pair['feature2']}: {pair['correlation']:.3f}\")\n",
    "        \n",
    "        # ë‹¤ì¤‘ê³µì„ ì„± ìœ„í—˜\n",
    "        multicollinear = analyze_correlations(correlation_matrix, feature_names, 0.9)\n",
    "        print(f\"ë‹¤ì¤‘ê³µì„ ì„± ìœ„í—˜ (|r| > 0.9): {len(multicollinear)}ê°œ\")\n",
    "        \n",
    "        for pair in sorted(multicollinear, key=lambda x: abs(x['correlation']), reverse=True)[:50]:\n",
    "            print(f\"  âš ï¸ {pair['feature1']} â†” {pair['feature2']}: {pair['correlation']:.3f}\")\n",
    "        \n",
    "        # ê²°ê³¼ ì €ì¥\n",
    "        period_results[period_name] = {\n",
    "            'correlation_matrix': correlation_matrix,\n",
    "            'feature_names': feature_names,\n",
    "            'high_correlations': high_correlations,\n",
    "            'multicollinear': multicollinear,\n",
    "            'processing_time': period_time\n",
    "        }\n",
    "    \n",
    "    else:\n",
    "        print(f\"âŒ {period_name} ë¶„ì„ ì‹¤íŒ¨\")\n",
    "\n",
    "# 8. ì „ì²´ ê²°ê³¼ ìš”ì•½\n",
    "total_analysis_time = time.time() - analysis_start_time\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"ğŸ¯ ì „ì²´ ê¸°ê°„ë³„ ìƒê´€ê´€ê³„ ë¶„ì„ ì™„ë£Œ!\")\n",
    "print(f\"ì´ ì²˜ë¦¬ ì‹œê°„: {total_analysis_time:.2f}ì´ˆ ({total_analysis_time/60:.1f}ë¶„)\")\n",
    "\n",
    "print(f\"\\n=== ê¸°ê°„ë³„ ë¶„ì„ ìš”ì•½ ===\")\n",
    "for period_name, results in period_results.items():\n",
    "    print(f\"{period_name}:\")\n",
    "    print(f\"  - í”¼ì²˜ ìˆ˜: {len(results['feature_names'])}\")\n",
    "    print(f\"  - ë†’ì€ ìƒê´€ê´€ê³„: {len(results['high_correlations'])}ê°œ\")\n",
    "    print(f\"  - ë‹¤ì¤‘ê³µì„ ì„± ìœ„í—˜: {len(results['multicollinear'])}ê°œ\")\n",
    "    print(f\"  - ì²˜ë¦¬ ì‹œê°„: {results['processing_time']:.2f}ì´ˆ\")\n",
    "\n",
    "# 9. ê¸°ê°„ ê°„ ë¹„êµ ë¶„ì„ (ì˜µì…˜)\n",
    "print(f\"\\n=== ê¸°ê°„ ê°„ ìƒê´€ê´€ê³„ íŒ¨í„´ ë¹„êµ ===\")\n",
    "for period_name, results in period_results.items():\n",
    "    if len(results['high_correlations']) > 0:\n",
    "        print(f\"\\n{period_name} ì£¼ìš” ìƒê´€ê´€ê³„:\")\n",
    "        # ê°€ì¥ ë†’ì€ ìƒê´€ê´€ê³„ 3ê°œ\n",
    "        top_corrs = sorted(results['high_correlations'], \n",
    "                          key=lambda x: abs(x['correlation']), reverse=True)[:3]\n",
    "        for i, pair in enumerate(top_corrs, 1):\n",
    "            print(f\"  {i}. {pair['feature1']} â†” {pair['feature2']}: {pair['correlation']:.3f}\")\n",
    "\n",
    "# 10. ë©”ëª¨ë¦¬ ì •ë¦¬\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    final_memory = torch.cuda.memory_allocated() / 1024**3\n",
    "    peak_memory = torch.cuda.max_memory_allocated() / 1024**3\n",
    "    print(f\"\\nğŸ§¹ ìµœì¢… GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {final_memory:.2f} GB\")\n",
    "    print(f\"ğŸ“ˆ ìµœëŒ€ GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {peak_memory:.2f} GB\")\n",
    "\n",
    "print(\"\\nâœ… ê¸°ê°„ë³„ GPU ìƒê´€ê´€ê³„ ë¶„ì„ ì™„ë£Œ!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bddfe3b3-e857-45a4-8140-3ff065947ce9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pyspark.sql.functions import col\n",
    "import time\n",
    "\n",
    "print(\"=== GPUë¡œ ìƒê´€ê´€ê³„ ë¶„ì„ ===\")\n",
    "\n",
    "# 1. GPU ë©”ëª¨ë¦¬ ìµœì í™” ì„¤ì •\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"ì‚¬ìš© ë””ë°”ì´ìŠ¤: {device}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    gpu_props = torch.cuda.get_device_properties(0)\n",
    "    total_memory = gpu_props.total_memory / 1024**3\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU ì´ ë©”ëª¨ë¦¬: {total_memory:.1f} GB\")\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "# 2. ì „ì²´ ë°ì´í„° ë¡œë“œ\n",
    "ps_df = ps_df_month_final\n",
    "numeric_cols = [col_name for col_name, data_type in ps_df.dtypes\n",
    "                if data_type in ['int', 'bigint', 'float', 'double']]\n",
    "\n",
    "total_rows = ps_df.count()\n",
    "n_features = len(numeric_cols)\n",
    "print(f\"ì „ì²´ ë°ì´í„° ìˆ˜: {total_rows:,}\")\n",
    "print(f\"ë¶„ì„í•  í”¼ì²˜ ìˆ˜: {n_features}\")\n",
    "\n",
    "def compute_large_chunk_correlation(df, numeric_cols, device):\n",
    "    \"\"\"\n",
    "    ëŒ€ìš©ëŸ‰ ì²­í¬ë¡œ GPU ìƒê´€ê´€ê³„ ê³„ì‚° (ë©”ëª¨ë¦¬ í™œìš©ë„ ê·¹ëŒ€í™”)\n",
    "    \"\"\"\n",
    "    print(\"\\n=== ëŒ€ìš©ëŸ‰ ì²­í¬ GPU ì²˜ë¦¬ ì‹œì‘ ===\")\n",
    "    \n",
    "    # í›¨ì”¬ í° ì²­í¬ í¬ê¸° ì„¤ì • (Tesla T4 16GB ê¸°ì¤€)\n",
    "    if torch.cuda.is_available():\n",
    "        # 16GB GPUì—ì„œ ì•ˆì „í•˜ê²Œ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” í¬ê¸°\n",
    "        # ìƒê´€ê´€ê³„ ë§¤íŠ¸ë¦­ìŠ¤ ê³„ì‚° ì‹œ ì¤‘ê°„ ê²°ê³¼ë¬¼ ê³ ë ¤í•˜ì—¬ ë³´ìˆ˜ì ìœ¼ë¡œ ì„¤ì •\n",
    "        large_chunk_size = 2000000  # 200ë§Œ í–‰ë¶€í„° ì‹œì‘\n",
    "        \n",
    "        # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¶”ì •\n",
    "        estimated_memory_gb = (large_chunk_size * n_features * 4) / 1024**3  # float32 ê¸°ì¤€\n",
    "        print(f\"ì²­í¬ë‹¹ ì˜ˆìƒ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {estimated_memory_gb:.2f} GB\")\n",
    "        \n",
    "        # GPU ë©”ëª¨ë¦¬ì˜ 70% ì´ìƒ ì‚¬ìš©í•˜ë„ë¡ ì¡°ì •\n",
    "        target_memory_usage = total_memory * 0.7  # 70% ì‚¬ìš© ëª©í‘œ\n",
    "        optimal_chunk_size = int((target_memory_usage * 1024**3) / (n_features * 4 * 3))  # ì•ˆì „ ë§ˆì§„\n",
    "        \n",
    "        # ìµœì¢… ì²­í¬ í¬ê¸° ê²°ì • (ìµœì†Œ 100ë§Œ, ìµœëŒ€ 500ë§Œ)\n",
    "        final_chunk_size = max(1000000, min(optimal_chunk_size, 5000000))\n",
    "        \n",
    "    else:\n",
    "        final_chunk_size = 1000000  # CPUì˜ ê²½ìš°\n",
    "    \n",
    "    print(f\"ìµœì¢… ì²­í¬ í¬ê¸°: {final_chunk_size:,} í–‰\")\n",
    "    \n",
    "    # ì²­í¬ ê°œìˆ˜ ê³„ì‚°\n",
    "    n_rows = df.count()\n",
    "    n_chunks = max(1, (n_rows + final_chunk_size - 1) // final_chunk_size)\n",
    "    \n",
    "    print(f\"ì´ {n_chunks}ê°œ ì²­í¬ë¡œ ë¶„í• \")\n",
    "    \n",
    "    if n_chunks > 10:\n",
    "        print(\"âš ï¸ ì²­í¬ ê°œìˆ˜ê°€ ë§ìŠµë‹ˆë‹¤. ì²­í¬ í¬ê¸°ë¥¼ ë” ëŠ˜ë ¤ë³´ê² ìŠµë‹ˆë‹¤.\")\n",
    "        final_chunk_size = max(final_chunk_size, n_rows // 5)  # ìµœëŒ€ 5ê°œ ì²­í¬ë¡œ ì œí•œ\n",
    "        n_chunks = max(1, (n_rows + final_chunk_size - 1) // final_chunk_size)\n",
    "        print(f\"ì¡°ì •ëœ ì²­í¬ í¬ê¸°: {final_chunk_size:,} í–‰\")\n",
    "        print(f\"ì¡°ì •ëœ ì²­í¬ ê°œìˆ˜: {n_chunks}ê°œ\")\n",
    "    \n",
    "    # ìƒê´€ê´€ê³„ ë§¤íŠ¸ë¦­ìŠ¤ ëˆ„ì ì„ ìœ„í•œ ë³€ìˆ˜ë“¤\n",
    "    correlation_sum = None\n",
    "    total_weight = 0\n",
    "    \n",
    "    for chunk_idx in range(n_chunks):\n",
    "        chunk_start_time = time.time()\n",
    "        \n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"ì²­í¬ {chunk_idx + 1}/{n_chunks} ì²˜ë¦¬ ì¤‘...\")\n",
    "        \n",
    "        # ì²­í¬ ë°ì´í„° ì¶”ì¶œ (ë” íš¨ìœ¨ì ì¸ ë°©ë²•)\n",
    "        if n_chunks == 1:\n",
    "            # ì „ì²´ ë°ì´í„°ë¥¼ í•œ ë²ˆì— ì²˜ë¦¬\n",
    "            chunk_df = df\n",
    "        else:\n",
    "            # ë¶„í•  ì²˜ë¦¬\n",
    "            chunk_fraction = 1.0 / n_chunks\n",
    "            chunk_df = df.sample(fraction=chunk_fraction, seed=42 + chunk_idx)\n",
    "        \n",
    "        # Pandasë¡œ ë³€í™˜\n",
    "        print(\"  PySpark â†’ Pandas ë³€í™˜ ì¤‘...\")\n",
    "        conversion_start = time.time()\n",
    "        chunk_pdf = chunk_df.select(*numeric_cols).fillna(0).toPandas()\n",
    "        conversion_time = time.time() - conversion_start\n",
    "        \n",
    "        actual_chunk_size = len(chunk_pdf)\n",
    "        print(f\"  ì‹¤ì œ ì²­í¬ í¬ê¸°: {actual_chunk_size:,} x {len(chunk_pdf.columns)}\")\n",
    "        print(f\"  ë³€í™˜ ì‹œê°„: {conversion_time:.2f}ì´ˆ\")\n",
    "        \n",
    "        if actual_chunk_size == 0:\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            # GPU ë©”ëª¨ë¦¬ ìƒíƒœ í™•ì¸\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "                initial_memory = torch.cuda.memory_allocated() / 1024**3\n",
    "                available_memory = (torch.cuda.get_device_properties(0).total_memory - torch.cuda.memory_allocated()) / 1024**3\n",
    "                print(f\"  ì‚¬ìš© ê°€ëŠ¥ GPU ë©”ëª¨ë¦¬: {available_memory:.2f} GB\")\n",
    "            \n",
    "            # GPU í…ì„œë¡œ ë³€í™˜\n",
    "            print(\"  GPU í…ì„œ ë³€í™˜ ì¤‘...\")\n",
    "            tensor_start = time.time()\n",
    "            chunk_tensor = torch.tensor(chunk_pdf.values, dtype=torch.float32).to(device)\n",
    "            tensor_time = time.time() - tensor_start\n",
    "            \n",
    "            if torch.cuda.is_available():\n",
    "                after_tensor_memory = torch.cuda.memory_allocated() / 1024**3\n",
    "                memory_used = after_tensor_memory - initial_memory\n",
    "                print(f\"  GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {memory_used:.2f} GB\")\n",
    "                print(f\"  í…ì„œ ë³€í™˜ ì‹œê°„: {tensor_time:.2f}ì´ˆ\")\n",
    "            \n",
    "            # ìƒê´€ê´€ê³„ ê³„ì‚°\n",
    "            print(\"  ìƒê´€ê´€ê³„ ê³„ì‚° ì¤‘...\")\n",
    "            corr_start = time.time()\n",
    "            chunk_correlation = torch.corrcoef(chunk_tensor.T)\n",
    "            corr_time = time.time() - corr_start\n",
    "            print(f\"  ìƒê´€ê´€ê³„ ê³„ì‚° ì‹œê°„: {corr_time:.2f}ì´ˆ\")\n",
    "            \n",
    "            # CPUë¡œ ì´ë™í•˜ì—¬ ëˆ„ì \n",
    "            chunk_corr_cpu = chunk_correlation.cpu().numpy()\n",
    "            \n",
    "            # ê°€ì¤‘ í‰ê· ìœ¼ë¡œ ëˆ„ì \n",
    "            weight = actual_chunk_size\n",
    "            if correlation_sum is None:\n",
    "                correlation_sum = chunk_corr_cpu * weight\n",
    "            else:\n",
    "                correlation_sum += chunk_corr_cpu * weight\n",
    "            total_weight += weight\n",
    "            \n",
    "            # ë©”ëª¨ë¦¬ ì •ë¦¬\n",
    "            del chunk_tensor, chunk_correlation, chunk_pdf\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "            \n",
    "            chunk_total_time = time.time() - chunk_start_time\n",
    "            print(f\"  ì²­í¬ ì´ ì²˜ë¦¬ ì‹œê°„: {chunk_total_time:.2f}ì´ˆ\")\n",
    "            print(f\"  ì§„í–‰ë¥ : {(chunk_idx + 1) / n_chunks * 100:.1f}%\")\n",
    "            \n",
    "            # ë‚¨ì€ ì‹œê°„ ì¶”ì •\n",
    "            if chunk_idx > 0:\n",
    "                avg_time_per_chunk = (time.time() - start_time) / (chunk_idx + 1)\n",
    "                remaining_time = avg_time_per_chunk * (n_chunks - chunk_idx - 1)\n",
    "                print(f\"  ì˜ˆìƒ ë‚¨ì€ ì‹œê°„: {remaining_time:.1f}ì´ˆ ({remaining_time/60:.1f}ë¶„)\")\n",
    "            \n",
    "        except RuntimeError as e:\n",
    "            if \"out of memory\" in str(e):\n",
    "                print(f\"  âŒ GPU ë©”ëª¨ë¦¬ ë¶€ì¡±! í˜„ì¬ ì²­í¬ í¬ê¸°: {actual_chunk_size:,}\")\n",
    "                print(\"  ë” ì‘ì€ ì²­í¬ë¡œ ì¬ì‹œë„í•˜ê±°ë‚˜ CPUë¡œ fallbackì´ í•„ìš”í•©ë‹ˆë‹¤.\")\n",
    "                raise e\n",
    "            else:\n",
    "                raise e\n",
    "    \n",
    "    # ìµœì¢… ìƒê´€ê´€ê³„ ë§¤íŠ¸ë¦­ìŠ¤ ê³„ì‚°\n",
    "    if total_weight > 0:\n",
    "        final_correlation = correlation_sum / total_weight\n",
    "        return final_correlation, numeric_cols\n",
    "    else:\n",
    "        return None, numeric_cols\n",
    "\n",
    "# 3. ëŒ€ìš©ëŸ‰ ì²­í¬ ìƒê´€ê´€ê³„ ë¶„ì„ ì‹¤í–‰\n",
    "print(f\"\\nğŸš€ ëŒ€ìš©ëŸ‰ ì²­í¬ë¡œ ì „ì²´ ë°ì´í„° {total_rows:,}í–‰ ë¶„ì„ ì‹œì‘\")\n",
    "start_time = time.time()\n",
    "\n",
    "try:\n",
    "    correlation_matrix, feature_names = compute_large_chunk_correlation(\n",
    "        ps_df, \n",
    "        numeric_cols, \n",
    "        device\n",
    "    )\n",
    "    \n",
    "    end_time = time.time()\n",
    "    total_time = end_time - start_time\n",
    "    print(f\"\\nâ±ï¸ ì „ì²´ ì²˜ë¦¬ ì‹œê°„: {total_time:.2f}ì´ˆ ({total_time/60:.1f}ë¶„)\")\n",
    "    print(f\"ğŸ“Š ì²˜ë¦¬ ì†ë„: {total_rows/total_time:,.0f} í–‰/ì´ˆ\")\n",
    "    \n",
    "    # 4. ê²°ê³¼ ë¶„ì„\n",
    "    if correlation_matrix is not None:\n",
    "        print(f\"\\n=== âœ… ëŒ€ìš©ëŸ‰ ì²­í¬ ìƒê´€ê´€ê³„ ë¶„ì„ ê²°ê³¼ âœ… ===\")\n",
    "        print(f\"ë¶„ì„ëœ ë°ì´í„°: {total_rows:,}í–‰\")\n",
    "        print(f\"ë¶„ì„ëœ í”¼ì²˜: {len(feature_names)}ê°œ\")\n",
    "        print(f\"ìƒê´€ê´€ê³„ ë§¤íŠ¸ë¦­ìŠ¤ í¬ê¸°: {correlation_matrix.shape}\")\n",
    "        \n",
    "        # ë†’ì€ ìƒê´€ê´€ê³„ ë¶„ì„\n",
    "        def analyze_correlations(corr_matrix, features, threshold=0.7):\n",
    "            high_corr = []\n",
    "            n = len(features)\n",
    "            for i in range(n):\n",
    "                for j in range(i+1, n):\n",
    "                    corr_val = corr_matrix[i, j]\n",
    "                    if abs(corr_val) > threshold:\n",
    "                        high_corr.append({\n",
    "                            'feature1': features[i],\n",
    "                            'feature2': features[j],\n",
    "                            'correlation': float(corr_val)\n",
    "                        })\n",
    "            return high_corr\n",
    "        \n",
    "        # ë†’ì€ ìƒê´€ê´€ê³„ ì¶œë ¥\n",
    "        high_correlations = analyze_correlations(correlation_matrix, feature_names, 0.7)\n",
    "        print(f\"\\në†’ì€ ìƒê´€ê´€ê³„ (|r| > 0.7): {len(high_correlations)}ê°œ\")\n",
    "        \n",
    "        for pair in sorted(high_correlations, key=lambda x: abs(x['correlation']), reverse=True)[:20]:\n",
    "            print(f\"  {pair['feature1']} â†” {pair['feature2']}: {pair['correlation']:.3f}\")\n",
    "        \n",
    "        # ë‹¤ì¤‘ê³µì„ ì„± ìœ„í—˜\n",
    "        multicollinear = analyze_correlations(correlation_matrix, feature_names, 0.9)\n",
    "        print(f\"\\në‹¤ì¤‘ê³µì„ ì„± ìœ„í—˜ (|r| > 0.9): {len(multicollinear)}ê°œ\")\n",
    "        \n",
    "        for pair in sorted(multicollinear, key=lambda x: abs(x['correlation']), reverse=True):\n",
    "            print(f\"  âš ï¸ {pair['feature1']} â†” {pair['feature2']}: {pair['correlation']:.3f}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"âŒ ì˜¤ë¥˜ ë°œìƒ: {str(e)}\")\n",
    "    print(\"CPUë¡œ fallbackì„ ì‹œë„í•˜ê±°ë‚˜ ì²­í¬ í¬ê¸°ë¥¼ ë” ì¤„ì—¬ë³´ì„¸ìš”.\")\n",
    "\n",
    "# 5. ë©”ëª¨ë¦¬ ì •ë¦¬\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    final_memory = torch.cuda.memory_allocated() / 1024**3\n",
    "    peak_memory = torch.cuda.max_memory_allocated() / 1024**3\n",
    "    print(f\"\\nğŸ§¹ ìµœì¢… GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {final_memory:.2f} GB\")\n",
    "    print(f\"ğŸ“ˆ ìµœëŒ€ GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {peak_memory:.2f} GB\")\n",
    "\n",
    "print(\"\\nâœ… GPU ìƒê´€ê´€ê³„ ë¶„ì„ ì™„ë£Œ!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "03d92c3a-1504-49a7-af59-377c5c108933",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def solve_use_month_multicollinearity(ps_df):\n",
    "    \"\"\"\n",
    "    USE_MONTH í…Œì´ë¸” ê¸°ê°„ë³„ ë‹¤ì¤‘ê³µì„ ì„± ì™„ì „ í•´ê²°\n",
    "    \"\"\"\n",
    "    print(\"=== USE_MONTH ê¸°ê°„ë³„ ë‹¤ì¤‘ê³µì„ ì„± ì™„ì „ í•´ê²° ===\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # ê¸°ê°„ë³„ ë‹¤ì¤‘ê³µì„ ì„± í•´ê²° ì „ëµ\n",
    "    # ì›ì¹™: R12M(ì¥ê¸°) > R6M(ì¤‘ê¸°) > R3M(ë‹¨ê¸°) ìˆœìœ¼ë¡œ ìš°ì„ ìˆœìœ„\n",
    "    multicollinearity_drops = {\n",
    "        # 1. R3M ì‚­ì œ (ë‹¨ê¸° - ê°€ì¥ ë³€ë™ì„± í¼)\n",
    "        \"ë‹¨ê¸°_R3M_ì‚­ì œ\": [\n",
    "            \"ì´ìš©ê°œì›”ìˆ˜_ì²´í¬_R3M\",      # R6Mê³¼ 0.982\n",
    "            \"ì´ìš©ê°œì›”ìˆ˜_CA_R3M\",        # R6Mê³¼ 0.969  \n",
    "            \"ì´ìš©ê°œì›”ìˆ˜_ì‹ ìš©_R3M\",      # R6Mê³¼ 0.968\n",
    "            \"ì´ìš©ê°œì›”ìˆ˜_ì „ì²´_R3M\",      # R6Mê³¼ 0.966\n",
    "            \"ì´ìš©ê°œì›”ìˆ˜_í• ë¶€_R3M\",      # R6Mê³¼ 0.946\n",
    "            \"ì´ìš©ê°œì›”ìˆ˜_í• ë¶€_ìœ ì´ì_R3M\" # R6Mê³¼ 0.891\n",
    "        ],\n",
    "        \n",
    "        # 2. R6M ì¼ë¶€ ì‚­ì œ (ì¤‘ê¸° - R12Mê³¼ ì¤‘ë³µë˜ëŠ” ê²ƒë“¤)\n",
    "        \"ì¤‘ê¸°_R6M_ì‚­ì œ\": [\n",
    "            \"ì´ìš©ê°œì›”ìˆ˜_ì²´í¬_R6M\",      # R12Mê³¼ 0.953\n",
    "            \"ì´ìš©ê°œì›”ìˆ˜_CA_R6M\",        # R12Mê³¼ 0.938\n",
    "            \"ì´ìš©ê°œì›”ìˆ˜_í• ë¶€_R6M\",      # R12Mê³¼ 0.937\n",
    "            \"ì´ìš©ê°œì›”ìˆ˜_ì‹ ìš©_R6M\"       # R12Mê³¼ 0.927\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    # ì „ì²´ ì‚­ì œ ì»¬ëŸ¼ ë¦¬ìŠ¤íŠ¸\n",
    "    all_drops = []\n",
    "    for category, cols in multicollinearity_drops.items():\n",
    "        all_drops.extend(cols)\n",
    "    \n",
    "    print(\"ğŸ“‹ ê¸°ê°„ë³„ ë‹¤ì¤‘ê³µì„ ì„± í•´ê²° ì „ëµ:\")\n",
    "    print(\"-\" * 60)\n",
    "    print(\"ğŸ¯ ê¸°ë³¸ ì›ì¹™: ì¥ê¸°(R12M) > ì¤‘ê¸°(R6M) > ë‹¨ê¸°(R3M)\")\n",
    "    print(\"   â†’ ì•ˆì •ì ì¸ ì¥ê¸° íŒ¨í„´ì„ ìš°ì„  ë³´ì¡´\")\n",
    "    \n",
    "    total_drops = 0\n",
    "    for category, cols in multicollinearity_drops.items():\n",
    "        print(f\"\\nğŸ”¸ {category} ({len(cols)}ê°œ):\")\n",
    "        for i, col in enumerate(cols, 1):\n",
    "            print(f\"   {i}. {col}\")\n",
    "        total_drops += len(cols)\n",
    "    \n",
    "    print(f\"\\nğŸ“Š ì‚­ì œ ìš”ì•½:\")\n",
    "    print(f\"   ì´ ì‚­ì œ ëŒ€ìƒ: {total_drops}ê°œ ì»¬ëŸ¼\")\n",
    "    \n",
    "    # ì‹¤ì œ ì¡´ì¬í•˜ëŠ” ì»¬ëŸ¼ë§Œ í•„í„°ë§\n",
    "    existing_drops = [col for col in all_drops if col in ps_df.columns]\n",
    "    missing_cols = [col for col in all_drops if col not in ps_df.columns]\n",
    "    \n",
    "    print(f\"   ì‹¤ì œ ì¡´ì¬: {len(existing_drops)}ê°œ\")\n",
    "    print(f\"   ì¡´ì¬í•˜ì§€ ì•ŠìŒ: {len(missing_cols)}ê°œ\")\n",
    "    \n",
    "    if missing_cols:\n",
    "        print(f\"\\nâš ï¸ ì¡´ì¬í•˜ì§€ ì•ŠëŠ” ì»¬ëŸ¼ë“¤:\")\n",
    "        for col in missing_cols:\n",
    "            print(f\"   - {col}\")\n",
    "    \n",
    "    print(f\"\\nğŸ”§ ì‹¤ì œ ì‚­ì œí•  ì»¬ëŸ¼ë“¤ ({len(existing_drops)}ê°œ):\")\n",
    "    for i, col in enumerate(existing_drops, 1):\n",
    "        print(f\"   {i:2d}. {col}\")\n",
    "    \n",
    "    # ì‚­ì œ ì‹¤í–‰\n",
    "    if existing_drops:\n",
    "        ps_df_cleaned = ps_df.drop(*existing_drops)\n",
    "        \n",
    "        print(f\"\\nâœ… ê¸°ê°„ë³„ ë‹¤ì¤‘ê³µì„ ì„± ì™„ì „ í•´ê²°!\")\n",
    "        print(f\"   ì‚­ì œ ì „: {len(ps_df.columns)} ì»¬ëŸ¼\")\n",
    "        print(f\"   ì‚­ì œ í›„: {len(ps_df_cleaned.columns)} ì»¬ëŸ¼\")\n",
    "        print(f\"   ì‹¤ì œ ì‚­ì œ: {len(existing_drops)} ì»¬ëŸ¼\")\n",
    "        print(f\"   ì‚­ì œ ë¹„ìœ¨: {len(existing_drops)/len(ps_df.columns)*100:.1f}%\")\n",
    "        \n",
    "        # í•´ê²°ëœ ë‹¤ì¤‘ê³µì„ ì„± ìŒë“¤\n",
    "        print(f\"\\nğŸ“‹ í•´ê²°ëœ ëª¨ë“  ë‹¤ì¤‘ê³µì„ ì„± ìŒë“¤ (11ê°œ):\")\n",
    "        resolved_pairs = [\n",
    "            \"ì´ìš©ê°œì›”ìˆ˜_ì²´í¬_R6M â†” ì´ìš©ê°œì›”ìˆ˜_ì²´í¬_R3M (0.982)\",\n",
    "            \"ì´ìš©ê°œì›”ìˆ˜_CA_R6M â†” ì´ìš©ê°œì›”ìˆ˜_CA_R3M (0.969)\",\n",
    "            \"ì´ìš©ê°œì›”ìˆ˜_ì‹ ìš©_R6M â†” ì´ìš©ê°œì›”ìˆ˜_ì‹ ìš©_R3M (0.968)\",\n",
    "            \"ì´ìš©ê°œì›”ìˆ˜_ì „ì²´_R6M â†” ì´ìš©ê°œì›”ìˆ˜_ì „ì²´_R3M (0.966)\",\n",
    "            \"ì´ìš©ê°œì›”ìˆ˜_ì²´í¬_R12M â†” ì´ìš©ê°œì›”ìˆ˜_ì²´í¬_R6M (0.953)\",\n",
    "            \"ì´ìš©ê°œì›”ìˆ˜_í• ë¶€_R6M â†” ì´ìš©ê°œì›”ìˆ˜_í• ë¶€_R3M (0.946)\",\n",
    "            \"ì´ìš©ê°œì›”ìˆ˜_CA_R12M â†” ì´ìš©ê°œì›”ìˆ˜_CA_R6M (0.938)\",\n",
    "            \"ì´ìš©ê°œì›”ìˆ˜_í• ë¶€_R12M â†” ì´ìš©ê°œì›”ìˆ˜_í• ë¶€_R6M (0.937)\",\n",
    "            \"ì´ìš©ê°œì›”ìˆ˜_ì²´í¬_R12M â†” ì´ìš©ê°œì›”ìˆ˜_ì²´í¬_R3M (0.934)\",\n",
    "            \"ì´ìš©ê°œì›”ìˆ˜_ì‹ ìš©_R12M â†” ì´ìš©ê°œì›”ìˆ˜_ì‹ ìš©_R6M (0.927)\",\n",
    "            \"ì´ìš©ê°œì›”ìˆ˜_CA_R12M â†” ì´ìš©ê°œì›”ìˆ˜_CA_R3M (0.908)\"\n",
    "        ]\n",
    "        \n",
    "        for i, pair in enumerate(resolved_pairs, 1):\n",
    "            print(f\"   {i:2d}. âœ… {pair}\")\n",
    "        \n",
    "        # ìµœì¢… ë‚¨ì€ í•µì‹¬ ë³€ìˆ˜ë“¤\n",
    "        print(f\"\\nğŸ”‘ ìµœì¢… ë‚¨ì€ í•µì‹¬ ì´ìš©ê°œì›”ìˆ˜ ë³€ìˆ˜ë“¤:\")\n",
    "        remaining_vars = [\n",
    "            \"ì´ìš©ê°œì›”ìˆ˜_ì „ì²´_R6M\",        # ì „ì²´ ì¤‘ê¸° íŒ¨í„´\n",
    "            \"ì´ìš©ê°œì›”ìˆ˜_ì „ì²´_R12M\",       # ì „ì²´ ì¥ê¸° íŒ¨í„´ (ìˆë‹¤ë©´)\n",
    "            \"ì´ìš©ê°œì›”ìˆ˜_ì‹ ìš©_R12M\",       # ì‹ ìš©ì¹´ë“œ ì¥ê¸° íŒ¨í„´\n",
    "            \"ì´ìš©ê°œì›”ìˆ˜_ì²´í¬_R12M\",       # ì²´í¬ì¹´ë“œ ì¥ê¸° íŒ¨í„´\n",
    "            \"ì´ìš©ê°œì›”ìˆ˜_CA_R12M\",         # CA ì¥ê¸° íŒ¨í„´\n",
    "            \"ì´ìš©ê°œì›”ìˆ˜_í• ë¶€_R12M\",       # í• ë¶€ ì¥ê¸° íŒ¨í„´\n",
    "            \"ì´ìš©ê°œì›”ìˆ˜_í• ë¶€_ìœ ì´ì_R6M\", # í• ë¶€ìœ ì´ì ì¤‘ê¸° íŒ¨í„´\n",
    "            \"ì´ìš©ê°œì›”ìˆ˜_í• ë¶€_ìœ ì´ì_R12M\" # í• ë¶€ìœ ì´ì ì¥ê¸° íŒ¨í„´ (ìˆë‹¤ë©´)\n",
    "        ]\n",
    "        \n",
    "        actual_remaining = [var for var in remaining_vars if var in ps_df_cleaned.columns]\n",
    "        \n",
    "        print(f\"   ì‹¤ì œ ë‚¨ì€ ë³€ìˆ˜ ({len(actual_remaining)}ê°œ):\")\n",
    "        for i, var in enumerate(actual_remaining, 1):\n",
    "            print(f\"   {i:2d}. {var}\")\n",
    "        \n",
    "        print(f\"\\nğŸ¯ í•´ê²° ë…¼ë¦¬:\")\n",
    "        print(f\"   â€¢ R3M ì „ì²´ ì‚­ì œ: ë‹¨ê¸° ë³€ë™ì„±ì´ ì»¤ì„œ ë…¸ì´ì¦ˆ ë§ìŒ\")\n",
    "        print(f\"   â€¢ R6M ì¼ë¶€ ì‚­ì œ: R12Mê³¼ ì¤‘ë³µë˜ëŠ” ê²ƒë“¤ë§Œ\")\n",
    "        print(f\"   â€¢ R12M ìš°ì„  ë³´ì¡´: ì•ˆì •ì ì¸ ì¥ê¸° ì‚¬ìš© íŒ¨í„´\")\n",
    "        print(f\"   â€¢ ì „ì²´_R6M ë³´ì¡´: ì¤‘ê¸° ì „ì²´ íŒ¨í„´ íŒŒì•…ìš©\")\n",
    "        \n",
    "        print(f\"\\nğŸ‰ ê²°ê³¼:\")\n",
    "        print(f\"   â€¢ ë‹¤ì¤‘ê³µì„ ì„± ìœ„í—˜: 11ê°œ â†’ 0ê°œ âœ…\")\n",
    "        print(f\"   â€¢ ê¸°ê°„ë³„ ì¤‘ë³µ ì™„ì „ í•´ê²°\")\n",
    "        print(f\"   â€¢ í•µì‹¬ ì¥ê¸° íŒ¨í„´ ì •ë³´ ë³´ì¡´\")\n",
    "        \n",
    "        return ps_df_cleaned, existing_drops\n",
    "    else:\n",
    "        print(\"\\nâŒ ì‚­ì œí•  ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "        return ps_df, []\n",
    "\n",
    "def validate_use_month_solution(ps_df_cleaned, deleted_cols):\n",
    "    \"\"\"\n",
    "    USE_MONTH ë‹¤ì¤‘ê³µì„ ì„± í•´ê²° íš¨ê³¼ ê²€ì¦\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"ğŸ” USE_MONTH ë‹¤ì¤‘ê³µì„ ì„± í•´ê²° ê²€ì¦\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    print(f\"ğŸ“‹ ì‚­ì œëœ ì»¬ëŸ¼ë“¤ ({len(deleted_cols)}ê°œ):\")\n",
    "    for i, col in enumerate(deleted_cols, 1):\n",
    "        if \"_R3M\" in col:\n",
    "            print(f\"   {i:2d}. {col} [ë‹¨ê¸°]\")\n",
    "        elif \"_R6M\" in col:\n",
    "            print(f\"   {i:2d}. {col} [ì¤‘ê¸°]\")\n",
    "        else:\n",
    "            print(f\"   {i:2d}. {col}\")\n",
    "    \n",
    "    return ps_df_cleaned\n",
    "\n",
    "# ì‹¤í–‰\n",
    "print(\"ğŸš€ USE_MONTH ê¸°ê°„ë³„ ë‹¤ì¤‘ê³µì„ ì„± ì™„ì „ í•´ê²° ì‹œì‘\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "ps_df_cleaned, deleted_cols = solve_use_month_multicollinearity(ps_df)\n",
    "ps_df_final = validate_use_month_solution(ps_df_cleaned, deleted_cols)\n",
    "\n",
    "print(f\"\\nğŸ¯ USE_MONTH ë‹¤ì¤‘ê³µì„ ì„± 100% í•´ê²° ì™„ë£Œ!\")\n",
    "print(f\"   ì‚­ì œëœ ì»¬ëŸ¼: {len(deleted_cols)}ê°œ\")\n",
    "print(f\"   í•´ê²°ëœ ë‹¤ì¤‘ê³µì„ ì„±: 11ê°œ ìŒ ëª¨ë‘\")\n",
    "print(f\"   ìµœì¢… ì»¬ëŸ¼ ìˆ˜: {len(ps_df_final.columns)}ê°œ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e66633c0-3d3b-423f-8ed9-39c832cb388f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pyspark.sql.functions import col\n",
    "import time\n",
    "\n",
    "print(\"=== GPUë¡œ ìƒê´€ê´€ê³„ ë¶„ì„ ===\")\n",
    "\n",
    "# 1. GPU ë©”ëª¨ë¦¬ ìµœì í™” ì„¤ì •\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"ì‚¬ìš© ë””ë°”ì´ìŠ¤: {device}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    gpu_props = torch.cuda.get_device_properties(0)\n",
    "    total_memory = gpu_props.total_memory / 1024**3\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU ì´ ë©”ëª¨ë¦¬: {total_memory:.1f} GB\")\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "# 2. ì „ì²´ ë°ì´í„° ë¡œë“œ\n",
    "ps_df = ps_df_final\n",
    "numeric_cols = [col_name for col_name, data_type in ps_df.dtypes\n",
    "                if data_type in ['int', 'bigint', 'float', 'double']]\n",
    "\n",
    "total_rows = ps_df.count()\n",
    "n_features = len(numeric_cols)\n",
    "print(f\"ì „ì²´ ë°ì´í„° ìˆ˜: {total_rows:,}\")\n",
    "print(f\"ë¶„ì„í•  í”¼ì²˜ ìˆ˜: {n_features}\")\n",
    "\n",
    "def compute_large_chunk_correlation(df, numeric_cols, device):\n",
    "    \"\"\"\n",
    "    ëŒ€ìš©ëŸ‰ ì²­í¬ë¡œ GPU ìƒê´€ê´€ê³„ ê³„ì‚° (ë©”ëª¨ë¦¬ í™œìš©ë„ ê·¹ëŒ€í™”)\n",
    "    \"\"\"\n",
    "    print(\"\\n=== ëŒ€ìš©ëŸ‰ ì²­í¬ GPU ì²˜ë¦¬ ì‹œì‘ ===\")\n",
    "    \n",
    "    # í›¨ì”¬ í° ì²­í¬ í¬ê¸° ì„¤ì • (Tesla T4 16GB ê¸°ì¤€)\n",
    "    if torch.cuda.is_available():\n",
    "        # 16GB GPUì—ì„œ ì•ˆì „í•˜ê²Œ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” í¬ê¸°\n",
    "        # ìƒê´€ê´€ê³„ ë§¤íŠ¸ë¦­ìŠ¤ ê³„ì‚° ì‹œ ì¤‘ê°„ ê²°ê³¼ë¬¼ ê³ ë ¤í•˜ì—¬ ë³´ìˆ˜ì ìœ¼ë¡œ ì„¤ì •\n",
    "        large_chunk_size = 2000000  # 200ë§Œ í–‰ë¶€í„° ì‹œì‘\n",
    "        \n",
    "        # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¶”ì •\n",
    "        estimated_memory_gb = (large_chunk_size * n_features * 4) / 1024**3  # float32 ê¸°ì¤€\n",
    "        print(f\"ì²­í¬ë‹¹ ì˜ˆìƒ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {estimated_memory_gb:.2f} GB\")\n",
    "        \n",
    "        # GPU ë©”ëª¨ë¦¬ì˜ 70% ì´ìƒ ì‚¬ìš©í•˜ë„ë¡ ì¡°ì •\n",
    "        target_memory_usage = total_memory * 0.7  # 70% ì‚¬ìš© ëª©í‘œ\n",
    "        optimal_chunk_size = int((target_memory_usage * 1024**3) / (n_features * 4 * 3))  # ì•ˆì „ ë§ˆì§„\n",
    "        \n",
    "        # ìµœì¢… ì²­í¬ í¬ê¸° ê²°ì • (ìµœì†Œ 100ë§Œ, ìµœëŒ€ 500ë§Œ)\n",
    "        final_chunk_size = max(1000000, min(optimal_chunk_size, 5000000))\n",
    "        \n",
    "    else:\n",
    "        final_chunk_size = 1000000  # CPUì˜ ê²½ìš°\n",
    "    \n",
    "    print(f\"ìµœì¢… ì²­í¬ í¬ê¸°: {final_chunk_size:,} í–‰\")\n",
    "    \n",
    "    # ì²­í¬ ê°œìˆ˜ ê³„ì‚°\n",
    "    n_rows = df.count()\n",
    "    n_chunks = max(1, (n_rows + final_chunk_size - 1) // final_chunk_size)\n",
    "    \n",
    "    print(f\"ì´ {n_chunks}ê°œ ì²­í¬ë¡œ ë¶„í• \")\n",
    "    \n",
    "    if n_chunks > 10:\n",
    "        print(\"âš ï¸ ì²­í¬ ê°œìˆ˜ê°€ ë§ìŠµë‹ˆë‹¤. ì²­í¬ í¬ê¸°ë¥¼ ë” ëŠ˜ë ¤ë³´ê² ìŠµë‹ˆë‹¤.\")\n",
    "        final_chunk_size = max(final_chunk_size, n_rows // 5)  # ìµœëŒ€ 5ê°œ ì²­í¬ë¡œ ì œí•œ\n",
    "        n_chunks = max(1, (n_rows + final_chunk_size - 1) // final_chunk_size)\n",
    "        print(f\"ì¡°ì •ëœ ì²­í¬ í¬ê¸°: {final_chunk_size:,} í–‰\")\n",
    "        print(f\"ì¡°ì •ëœ ì²­í¬ ê°œìˆ˜: {n_chunks}ê°œ\")\n",
    "    \n",
    "    # ìƒê´€ê´€ê³„ ë§¤íŠ¸ë¦­ìŠ¤ ëˆ„ì ì„ ìœ„í•œ ë³€ìˆ˜ë“¤\n",
    "    correlation_sum = None\n",
    "    total_weight = 0\n",
    "    \n",
    "    for chunk_idx in range(n_chunks):\n",
    "        chunk_start_time = time.time()\n",
    "        \n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"ì²­í¬ {chunk_idx + 1}/{n_chunks} ì²˜ë¦¬ ì¤‘...\")\n",
    "        \n",
    "        # ì²­í¬ ë°ì´í„° ì¶”ì¶œ (ë” íš¨ìœ¨ì ì¸ ë°©ë²•)\n",
    "        if n_chunks == 1:\n",
    "            # ì „ì²´ ë°ì´í„°ë¥¼ í•œ ë²ˆì— ì²˜ë¦¬\n",
    "            chunk_df = df\n",
    "        else:\n",
    "            # ë¶„í•  ì²˜ë¦¬\n",
    "            chunk_fraction = 1.0 / n_chunks\n",
    "            chunk_df = df.sample(fraction=chunk_fraction, seed=42 + chunk_idx)\n",
    "        \n",
    "        # Pandasë¡œ ë³€í™˜\n",
    "        print(\"  PySpark â†’ Pandas ë³€í™˜ ì¤‘...\")\n",
    "        conversion_start = time.time()\n",
    "        chunk_pdf = chunk_df.select(*numeric_cols).fillna(0).toPandas()\n",
    "        conversion_time = time.time() - conversion_start\n",
    "        \n",
    "        actual_chunk_size = len(chunk_pdf)\n",
    "        print(f\"  ì‹¤ì œ ì²­í¬ í¬ê¸°: {actual_chunk_size:,} x {len(chunk_pdf.columns)}\")\n",
    "        print(f\"  ë³€í™˜ ì‹œê°„: {conversion_time:.2f}ì´ˆ\")\n",
    "        \n",
    "        if actual_chunk_size == 0:\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            # GPU ë©”ëª¨ë¦¬ ìƒíƒœ í™•ì¸\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "                initial_memory = torch.cuda.memory_allocated() / 1024**3\n",
    "                available_memory = (torch.cuda.get_device_properties(0).total_memory - torch.cuda.memory_allocated()) / 1024**3\n",
    "                print(f\"  ì‚¬ìš© ê°€ëŠ¥ GPU ë©”ëª¨ë¦¬: {available_memory:.2f} GB\")\n",
    "            \n",
    "            # GPU í…ì„œë¡œ ë³€í™˜\n",
    "            print(\"  GPU í…ì„œ ë³€í™˜ ì¤‘...\")\n",
    "            tensor_start = time.time()\n",
    "            chunk_tensor = torch.tensor(chunk_pdf.values, dtype=torch.float32).to(device)\n",
    "            tensor_time = time.time() - tensor_start\n",
    "            \n",
    "            if torch.cuda.is_available():\n",
    "                after_tensor_memory = torch.cuda.memory_allocated() / 1024**3\n",
    "                memory_used = after_tensor_memory - initial_memory\n",
    "                print(f\"  GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {memory_used:.2f} GB\")\n",
    "                print(f\"  í…ì„œ ë³€í™˜ ì‹œê°„: {tensor_time:.2f}ì´ˆ\")\n",
    "            \n",
    "            # ìƒê´€ê´€ê³„ ê³„ì‚°\n",
    "            print(\"  ìƒê´€ê´€ê³„ ê³„ì‚° ì¤‘...\")\n",
    "            corr_start = time.time()\n",
    "            chunk_correlation = torch.corrcoef(chunk_tensor.T)\n",
    "            corr_time = time.time() - corr_start\n",
    "            print(f\"  ìƒê´€ê´€ê³„ ê³„ì‚° ì‹œê°„: {corr_time:.2f}ì´ˆ\")\n",
    "            \n",
    "            # CPUë¡œ ì´ë™í•˜ì—¬ ëˆ„ì \n",
    "            chunk_corr_cpu = chunk_correlation.cpu().numpy()\n",
    "            \n",
    "            # ê°€ì¤‘ í‰ê· ìœ¼ë¡œ ëˆ„ì \n",
    "            weight = actual_chunk_size\n",
    "            if correlation_sum is None:\n",
    "                correlation_sum = chunk_corr_cpu * weight\n",
    "            else:\n",
    "                correlation_sum += chunk_corr_cpu * weight\n",
    "            total_weight += weight\n",
    "            \n",
    "            # ë©”ëª¨ë¦¬ ì •ë¦¬\n",
    "            del chunk_tensor, chunk_correlation, chunk_pdf\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "            \n",
    "            chunk_total_time = time.time() - chunk_start_time\n",
    "            print(f\"  ì²­í¬ ì´ ì²˜ë¦¬ ì‹œê°„: {chunk_total_time:.2f}ì´ˆ\")\n",
    "            print(f\"  ì§„í–‰ë¥ : {(chunk_idx + 1) / n_chunks * 100:.1f}%\")\n",
    "            \n",
    "            # ë‚¨ì€ ì‹œê°„ ì¶”ì •\n",
    "            if chunk_idx > 0:\n",
    "                avg_time_per_chunk = (time.time() - start_time) / (chunk_idx + 1)\n",
    "                remaining_time = avg_time_per_chunk * (n_chunks - chunk_idx - 1)\n",
    "                print(f\"  ì˜ˆìƒ ë‚¨ì€ ì‹œê°„: {remaining_time:.1f}ì´ˆ ({remaining_time/60:.1f}ë¶„)\")\n",
    "            \n",
    "        except RuntimeError as e:\n",
    "            if \"out of memory\" in str(e):\n",
    "                print(f\"  âŒ GPU ë©”ëª¨ë¦¬ ë¶€ì¡±! í˜„ì¬ ì²­í¬ í¬ê¸°: {actual_chunk_size:,}\")\n",
    "                print(\"  ë” ì‘ì€ ì²­í¬ë¡œ ì¬ì‹œë„í•˜ê±°ë‚˜ CPUë¡œ fallbackì´ í•„ìš”í•©ë‹ˆë‹¤.\")\n",
    "                raise e\n",
    "            else:\n",
    "                raise e\n",
    "    \n",
    "    # ìµœì¢… ìƒê´€ê´€ê³„ ë§¤íŠ¸ë¦­ìŠ¤ ê³„ì‚°\n",
    "    if total_weight > 0:\n",
    "        final_correlation = correlation_sum / total_weight\n",
    "        return final_correlation, numeric_cols\n",
    "    else:\n",
    "        return None, numeric_cols\n",
    "\n",
    "# 3. ëŒ€ìš©ëŸ‰ ì²­í¬ ìƒê´€ê´€ê³„ ë¶„ì„ ì‹¤í–‰\n",
    "print(f\"\\nğŸš€ ëŒ€ìš©ëŸ‰ ì²­í¬ë¡œ ì „ì²´ ë°ì´í„° {total_rows:,}í–‰ ë¶„ì„ ì‹œì‘\")\n",
    "start_time = time.time()\n",
    "\n",
    "try:\n",
    "    correlation_matrix, feature_names = compute_large_chunk_correlation(\n",
    "        ps_df, \n",
    "        numeric_cols, \n",
    "        device\n",
    "    )\n",
    "    \n",
    "    end_time = time.time()\n",
    "    total_time = end_time - start_time\n",
    "    print(f\"\\nâ±ï¸ ì „ì²´ ì²˜ë¦¬ ì‹œê°„: {total_time:.2f}ì´ˆ ({total_time/60:.1f}ë¶„)\")\n",
    "    print(f\"ğŸ“Š ì²˜ë¦¬ ì†ë„: {total_rows/total_time:,.0f} í–‰/ì´ˆ\")\n",
    "    \n",
    "    # 4. ê²°ê³¼ ë¶„ì„\n",
    "    if correlation_matrix is not None:\n",
    "        print(f\"\\n=== âœ… ëŒ€ìš©ëŸ‰ ì²­í¬ ìƒê´€ê´€ê³„ ë¶„ì„ ê²°ê³¼ âœ… ===\")\n",
    "        print(f\"ë¶„ì„ëœ ë°ì´í„°: {total_rows:,}í–‰\")\n",
    "        print(f\"ë¶„ì„ëœ í”¼ì²˜: {len(feature_names)}ê°œ\")\n",
    "        print(f\"ìƒê´€ê´€ê³„ ë§¤íŠ¸ë¦­ìŠ¤ í¬ê¸°: {correlation_matrix.shape}\")\n",
    "        \n",
    "        # ë†’ì€ ìƒê´€ê´€ê³„ ë¶„ì„\n",
    "        def analyze_correlations(corr_matrix, features, threshold=0.7):\n",
    "            high_corr = []\n",
    "            n = len(features)\n",
    "            for i in range(n):\n",
    "                for j in range(i+1, n):\n",
    "                    corr_val = corr_matrix[i, j]\n",
    "                    if abs(corr_val) > threshold:\n",
    "                        high_corr.append({\n",
    "                            'feature1': features[i],\n",
    "                            'feature2': features[j],\n",
    "                            'correlation': float(corr_val)\n",
    "                        })\n",
    "            return high_corr\n",
    "        \n",
    "        # ë†’ì€ ìƒê´€ê´€ê³„ ì¶œë ¥\n",
    "        high_correlations = analyze_correlations(correlation_matrix, feature_names, 0.7)\n",
    "        print(f\"\\në†’ì€ ìƒê´€ê´€ê³„ (|r| > 0.7): {len(high_correlations)}ê°œ\")\n",
    "        \n",
    "        for pair in sorted(high_correlations, key=lambda x: abs(x['correlation']), reverse=True)[:20]:\n",
    "            print(f\"  {pair['feature1']} â†” {pair['feature2']}: {pair['correlation']:.3f}\")\n",
    "        \n",
    "        # ë‹¤ì¤‘ê³µì„ ì„± ìœ„í—˜\n",
    "        multicollinear = analyze_correlations(correlation_matrix, feature_names, 0.9)\n",
    "        print(f\"\\në‹¤ì¤‘ê³µì„ ì„± ìœ„í—˜ (|r| > 0.9): {len(multicollinear)}ê°œ\")\n",
    "        \n",
    "        for pair in sorted(multicollinear, key=lambda x: abs(x['correlation']), reverse=True):\n",
    "            print(f\"  âš ï¸ {pair['feature1']} â†” {pair['feature2']}: {pair['correlation']:.3f}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"âŒ ì˜¤ë¥˜ ë°œìƒ: {str(e)}\")\n",
    "    print(\"CPUë¡œ fallbackì„ ì‹œë„í•˜ê±°ë‚˜ ì²­í¬ í¬ê¸°ë¥¼ ë” ì¤„ì—¬ë³´ì„¸ìš”.\")\n",
    "\n",
    "# 5. ë©”ëª¨ë¦¬ ì •ë¦¬\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    final_memory = torch.cuda.memory_allocated() / 1024**3\n",
    "    peak_memory = torch.cuda.max_memory_allocated() / 1024**3\n",
    "    print(f\"\\nğŸ§¹ ìµœì¢… GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {final_memory:.2f} GB\")\n",
    "    print(f\"ğŸ“ˆ ìµœëŒ€ GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {peak_memory:.2f} GB\")\n",
    "\n",
    "print(\"\\nâœ… GPU ìƒê´€ê´€ê³„ ë¶„ì„ ì™„ë£Œ!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d1140fe0-1f3e-4144-8cf3-d00c70a5891b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "### ë°ì´í„° ë² ì´ìŠ¤ ì‚¬ìš© ì„¤ì •\n",
    "spark.sql(\"USE database_pjt\")\n",
    "print(\"í˜„ì¬ ë°ì´í„°ë² ì´ìŠ¤ë¥¼ 'database_pjt'ë¡œ ì„¤ì •\")\n",
    "\n",
    "ps_df_final.write.mode(\"overwrite\").saveAsTable(\"3_use_month_df_fin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f6e5e7af-80b7-4262-8a98-0b73ab9290d8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "ALTER TABLE database_pjt.3_use_month_df_fin\n",
    "RENAME TO database_pjt.3_use_month;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "60eccc33-60fc-468a-8d44-7ab61c1d3579",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c42f9617-05cd-4d52-9d6f-b7b617b893d2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### 2.2 ê¸°ê°„ ë¶ˆí¬í•¨ ì»¬ëŸ¼ (ì—†ìœ¼ë¯€ë¡œ ìƒëµ)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [
    {
     "elements": [
      {
       "dashboardResultIndex": null,
       "elementNUID": "5b40b8c9-1434-41fd-93a5-ddbffd22221d",
       "elementType": "command",
       "guid": "68696a03-7229-4f52-8287-9387fc7a4fad",
       "options": null,
       "position": {
        "height": 6,
        "width": 12,
        "x": 0,
        "y": 9,
        "z": null
       },
       "resultIndex": null
      }
     ],
     "globalVars": {},
     "guid": "",
     "layoutOption": {
      "grid": true,
      "stack": false
     },
     "nuid": "46ef278e-af8b-4845-a8f4-990babf3c9f6",
     "origId": 952016186377612,
     "title": "ì»¬ëŸ¼ë³„ ë¶„í¬",
     "version": "DashboardViewV1",
     "width": 2560
    }
   ],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 7161869183782037,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "03_5. use_month",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
