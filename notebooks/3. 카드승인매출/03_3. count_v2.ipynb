{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "18bc46ab-10d7-4880-b2cf-66721a926cd7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when, col, to_date\n",
    "from pyspark.sql import functions as F\n",
    "from matplotlib import font_manager\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import warnings\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8321e8bf-4642-43c0-9d88-46a54de55de5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "!apt-get update -qq\n",
    "!apt-get install -y fonts-nanum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "80ce7b84-61bb-4fb3-8f0e-219a1b804a35",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "font_dirs = [\"/usr/share/fonts/truetype/nanum/\"]\n",
    "font_files = font_manager.findSystemFonts(fontpaths=font_dirs)\n",
    " \n",
    "for font_file in font_files:\n",
    "    font_manager.fontManager.addfont(font_file)\n",
    " \n",
    "plt.rc('font', family='NanumGothic')\n",
    "plt.rc('axes', unicode_minus=False)\n",
    " \n",
    "pd.Series([-1,2,3]).plot(title='테스트', figsize=(3,2))\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8e63dcd3-09b5-4793-923b-e9ee657386ed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 처리할 # 타겟 테이블 가져오기\n",
    "ps_df = spark.read.table(\"database_03_cache.count_df\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5d63f60a-f091-4c15-9c30-d90cfee768a0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 1. 결측치 처리 & 데이터형 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "81917fcc-7e12-4a0a-805f-642af385ee2c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when, col, to_date\n",
    "\n",
    "#for c in ps_df.columns[2:]:\n",
    "#    ps_df = ps_df.withColumn(c, col(c).cast(\"float\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7a77cf2c-3357-4cea-931d-96b13397df9d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Databricks data profile. Run in Databricks to view."
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1.subcommand+json": {
       "baseErrorDetails": null,
       "bindings": {},
       "collapsed": false,
       "command": "%python\nif hasattr(dbutils, \"data\") and hasattr(dbutils.data, \"summarize\"):\n  # setup\n  __data_summary_display_orig = display\n  __data_summary_dfs = []\n  def __data_summary_display_new(df):\n    # add only when result is going to be table type\n    __data_summary_df_modules = [\"pandas.core.frame\", \"databricks.koalas.frame\", \"pyspark.sql.dataframe\", \"pyspark.pandas.frame\", \"pyspark.sql.connect.dataframe\", \"pyspark.sql.classic.dataframe\"]\n    if (type(df).__module__ in __data_summary_df_modules and type(df).__name__ == 'DataFrame') or isinstance(df, list):\n      __data_summary_dfs.append(df)\n  display = __data_summary_display_new\n\n  def __data_summary_user_code_fn():\n    import base64\n    exec(base64.standard_b64decode(\"ZGlzcGxheShkYXRlX2RmKQ==\").decode())\n\n  try:\n    # run user code\n    __data_summary_user_code_fn()\n\n    # run on valid tableResultIndex\n    if len(__data_summary_dfs) > 0:\n      # run summarize\n      if type(__data_summary_dfs[0]).__module__ == \"databricks.koalas.frame\":\n        # koalas dataframe\n        dbutils.data.summarize(__data_summary_dfs[0].to_spark())\n      elif type(__data_summary_dfs[0]).__module__ == \"pandas.core.frame\":\n        # pandas dataframe\n        dbutils.data.summarize(spark.createDataFrame(__data_summary_dfs[0]))\n      else:\n        dbutils.data.summarize(__data_summary_dfs[0])\n    else:\n        displayHTML(\"dataframe no longer exists. If you're using dataframe.display(), use display(dataframe) instead.\")\n\n  finally:\n    display = __data_summary_display_orig\n    del __data_summary_display_new\n    del __data_summary_display_orig\n    del __data_summary_dfs\n    del __data_summary_user_code_fn\nelse:\n  print(\"This DBR version does not support data profiles.\")",
       "commandTitle": "데이터 프로필 1",
       "commandType": "auto",
       "commandVersion": 0,
       "commentThread": [],
       "commentsVisible": false,
       "contentSha256Hex": null,
       "customPlotOptions": {},
       "datasetPreviewNameToCmdIdMap": {},
       "diffDeletes": [],
       "diffInserts": [],
       "displayType": "table",
       "error": null,
       "errorDetails": null,
       "errorSummary": null,
       "errorTraceType": null,
       "finishTime": 0,
       "globalVars": {},
       "guid": "",
       "height": "auto",
       "hideCommandCode": false,
       "hideCommandResult": false,
       "iPythonMetadata": null,
       "inputWidgets": {},
       "isLockedInExamMode": false,
       "latestUser": "a user",
       "latestUserId": null,
       "listResultMetadata": null,
       "metadata": {
        "byteLimit": 2048000,
        "rowLimit": 10000
       },
       "nuid": "8462e18c-fc6d-4d82-84b7-be56f5c2e54b",
       "origId": 0,
       "parentHierarchy": [],
       "pivotAggregation": null,
       "pivotColumns": null,
       "position": 4.0,
       "resultDbfsErrorMessage": null,
       "resultDbfsStatus": "INLINED_IN_TREE",
       "results": null,
       "showCommandTitle": false,
       "startTime": 0,
       "state": "input",
       "streamStates": {},
       "subcommandOptions": null,
       "submitTime": 0,
       "subtype": "tableResultSubCmd.dataSummary",
       "tableResultIndex": 0,
       "tableResultSettingsMap": {},
       "useConsistentColors": false,
       "version": "CommandV1",
       "width": "auto",
       "workflows": null,
       "xColumns": null,
       "yColumns": null
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(ps_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c40667df-d7fa-4246-b2b5-4312f87567d6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 2. 컬럼 세분화"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dd355f98-26c8-4fae-bb3b-7f414d6adce0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### 2.0 기간 포함/불포함 구분\n",
    "\n",
    "- 전체 : 137개\n",
    "- 분류 : 기간 포함된 컬럼 (119개) / 불포함된 컬럼 (18개)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6f1722d2-eb67-4ec4-b42f-3903a11a5ca4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "ps_columns = ps_df.columns\n",
    "len(ps_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d835288e-1fca-4afc-abeb-ffb0c7420d7c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "### Step 1: 기간이 포함된 컬럼 정규표현식\n",
    "period_pattern = re.compile(r'^(.*)_(B\\d+M|R\\d+M)$') # B나 R기간\n",
    "\n",
    "### Step 2: prefix-period 딕셔너리 만들기\n",
    "prefix_period_map = {}  # 기간별로 prefix를 저장할 딕셔너리\n",
    "non_matching_cols = []  # 기간이 포함되지 않은 컬럼\n",
    "\n",
    "for col in ps_columns:\n",
    "    match = period_pattern.match(col)\n",
    "    # prefix(ex.'이용금액_신용')와 기간(ex.'R12M')을 따로 저장\n",
    "    if match:\n",
    "        prefix = match.group(1) # ex. '이용금액_신용'\n",
    "        period = match.group(2) # ex. 'R12M'\n",
    "        if prefix not in prefix_period_map:\n",
    "            prefix_period_map[prefix] = {}\n",
    "        prefix_period_map[prefix][period] = col\n",
    "    else:\n",
    "        non_matching_cols.append(col)\n",
    "\n",
    "print(137 - len(non_matching_cols), prefix_period_map)\n",
    "print()\n",
    "print(len(non_matching_cols), non_matching_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "79c80f77-9299-4ba1-aeb3-92eef1792413",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### 2.1 기간 포함 컬럼\n",
    "(prefix_period_map 에 담겨져 있음)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4a3a6776-6c7b-462c-a0e1-0c3dc5fd9038",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "> **기간 unique 정리**\n",
    "| 기간코드   | 의미                                               |\n",
    "| ------ | ------------------------------------------------ |\n",
    "| `B0M`  | **현재 기준 시점** (예: 분석 기준이 2025년 5월이면 → 2025년 5월)   |\n",
    "| `B1M`  | **1개월 전** 기준 (예: 2025년 4월)                       |\n",
    "| `B2M`  | **2개월 전** 기준 (예: 2025년 3월)                       |\n",
    "| `R3M`  | **최근 3개월 평균치** (예: 2025년 3\\~5월의 평균 이용금액)         |\n",
    "| `R6M`  | **최근 6개월 평균치** (예: 2024년 12월\\~2025년 5월의 평균 이용금액) |\n",
    "| `R12M` | **최근 12개월 평균치** (예: 2024년 6월\\~2025년 5월의 평균 이용금액) |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ae910147-46dd-4ea2-a219-82a740997137",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 모든 prefix에 대해 period key만 모아서 set으로 중복 제거\n",
    "all_periods = set()\n",
    "\n",
    "for periods in prefix_period_map.values():\n",
    "    all_periods.update(periods.keys())\n",
    "\n",
    "# 보기 좋게 숫자 정렬 (예: B0M, B1M, ..., R3M, R6M, R12M)\n",
    "sorted_periods = sorted(\n",
    "    list(all_periods),\n",
    "    key=lambda x: (x[0], int(re.search(r'\\d+', x).group()))\n",
    ")\n",
    "\n",
    "print(\"✔ 사용된 기간들:\", sorted_periods)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e40b1718-b767-41f0-a871-df3c47d9ad34",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "ps_columns = [col for col in ps_columns if col not in non_matching_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b17d721f-26f0-40bc-b416-6cf7d11b101c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "key_columns = [\"기준년월\", \"발급회원번호\"]\n",
    "final_period_df_columns = key_columns + ps_columns\n",
    "period_df = ps_df.select(*final_period_df_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f61f9b6c-c790-493a-b457-2578b41dd7d4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Databricks data profile. Run in Databricks to view."
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1.subcommand+json": {
       "baseErrorDetails": null,
       "bindings": {},
       "collapsed": false,
       "command": "%python\nif hasattr(dbutils, \"data\") and hasattr(dbutils.data, \"summarize\"):\n  # setup\n  __data_summary_display_orig = display\n  __data_summary_dfs = []\n  def __data_summary_display_new(df):\n    # add only when result is going to be table type\n    __data_summary_df_modules = [\"pandas.core.frame\", \"databricks.koalas.frame\", \"pyspark.sql.dataframe\", \"pyspark.pandas.frame\", \"pyspark.sql.connect.dataframe\", \"pyspark.sql.classic.dataframe\"]\n    if (type(df).__module__ in __data_summary_df_modules and type(df).__name__ == 'DataFrame') or isinstance(df, list):\n      __data_summary_dfs.append(df)\n  display = __data_summary_display_new\n\n  def __data_summary_user_code_fn():\n    import base64\n    exec(base64.standard_b64decode(\"ZGlzcGxheShwZXJpb2RfZGYp\").decode())\n\n  try:\n    # run user code\n    __data_summary_user_code_fn()\n\n    # run on valid tableResultIndex\n    if len(__data_summary_dfs) > 0:\n      # run summarize\n      if type(__data_summary_dfs[0]).__module__ == \"databricks.koalas.frame\":\n        # koalas dataframe\n        dbutils.data.summarize(__data_summary_dfs[0].to_spark())\n      elif type(__data_summary_dfs[0]).__module__ == \"pandas.core.frame\":\n        # pandas dataframe\n        dbutils.data.summarize(spark.createDataFrame(__data_summary_dfs[0]))\n      else:\n        dbutils.data.summarize(__data_summary_dfs[0])\n    else:\n        displayHTML(\"dataframe no longer exists. If you're using dataframe.display(), use display(dataframe) instead.\")\n\n  finally:\n    display = __data_summary_display_orig\n    del __data_summary_display_new\n    del __data_summary_display_orig\n    del __data_summary_dfs\n    del __data_summary_user_code_fn\nelse:\n  print(\"This DBR version does not support data profiles.\")",
       "commandTitle": "데이터 프로필 1",
       "commandType": "auto",
       "commandVersion": 0,
       "commentThread": [],
       "commentsVisible": false,
       "contentSha256Hex": null,
       "customPlotOptions": {},
       "datasetPreviewNameToCmdIdMap": {},
       "diffDeletes": [],
       "diffInserts": [],
       "displayType": "table",
       "error": null,
       "errorDetails": null,
       "errorSummary": null,
       "errorTraceType": null,
       "finishTime": 0,
       "globalVars": {},
       "guid": "",
       "height": "auto",
       "hideCommandCode": false,
       "hideCommandResult": false,
       "iPythonMetadata": null,
       "inputWidgets": {},
       "isLockedInExamMode": false,
       "latestUser": "a user",
       "latestUserId": null,
       "listResultMetadata": null,
       "metadata": {
        "byteLimit": 2048000,
        "rowLimit": 10000
       },
       "nuid": "b216152d-0887-45b7-80dd-6d18874ac211",
       "origId": 0,
       "parentHierarchy": [],
       "pivotAggregation": null,
       "pivotColumns": null,
       "position": 6.970588235294114,
       "resultDbfsErrorMessage": null,
       "resultDbfsStatus": "INLINED_IN_TREE",
       "results": null,
       "showCommandTitle": false,
       "startTime": 0,
       "state": "input",
       "streamStates": {},
       "subcommandOptions": null,
       "submitTime": 0,
       "subtype": "tableResultSubCmd.dataSummary",
       "tableResultIndex": 0,
       "tableResultSettingsMap": {},
       "useConsistentColors": false,
       "version": "CommandV1",
       "width": "auto",
       "workflows": null,
       "xColumns": null,
       "yColumns": null
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(period_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3c329c81-0723-4fb6-91b4-81b257785114",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### 💡 데이터 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7217c7da-422b-4a7f-845a-2fb1caa80f7a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "### 데이터 베이스 사용 설정\n",
    "period_df = period_df.cache()\n",
    "spark.sql(\"USE database_03_cache\")\n",
    "print(\"현재 데이터베이스를 'database_03_cache'로 설정\")\n",
    "\n",
    "### 저장할 테이블 값 입력\n",
    "period_df.write.mode(\"overwrite\").saveAsTable(\"count_period_df\")\n",
    "print(\"이용금액(기간 포함) 관련 테이블 생성 완료\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1d992609-6a9f-4d67-85c7-ca85d1aa0f78",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### 💡다시 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "28e94960-c195-420a-ab5e-bfc09c1b2226",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "### 저장한 테이블 값 입력\n",
    "ps_period_df = spark.read.table(\"database_03_cache.count_period_df\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4ec28720-219c-452d-a486-a444e44ea449",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Databricks data profile. Run in Databricks to view."
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1.subcommand+json": {
       "baseErrorDetails": null,
       "bindings": {},
       "collapsed": false,
       "command": "%python\nif hasattr(dbutils, \"data\") and hasattr(dbutils.data, \"summarize\"):\n  # setup\n  __data_summary_display_orig = display\n  __data_summary_dfs = []\n  def __data_summary_display_new(df):\n    # add only when result is going to be table type\n    __data_summary_df_modules = [\"pandas.core.frame\", \"databricks.koalas.frame\", \"pyspark.sql.dataframe\", \"pyspark.pandas.frame\", \"pyspark.sql.connect.dataframe\", \"pyspark.sql.classic.dataframe\"]\n    if (type(df).__module__ in __data_summary_df_modules and type(df).__name__ == 'DataFrame') or isinstance(df, list):\n      __data_summary_dfs.append(df)\n  display = __data_summary_display_new\n\n  def __data_summary_user_code_fn():\n    import base64\n    exec(base64.standard_b64decode(\"ZGlzcGxheShhbW91bnRfcGVyaW9kX2RmKQ==\").decode())\n\n  try:\n    # run user code\n    __data_summary_user_code_fn()\n\n    # run on valid tableResultIndex\n    if len(__data_summary_dfs) > 0:\n      # run summarize\n      if type(__data_summary_dfs[0]).__module__ == \"databricks.koalas.frame\":\n        # koalas dataframe\n        dbutils.data.summarize(__data_summary_dfs[0].to_spark())\n      elif type(__data_summary_dfs[0]).__module__ == \"pandas.core.frame\":\n        # pandas dataframe\n        dbutils.data.summarize(spark.createDataFrame(__data_summary_dfs[0]))\n      else:\n        dbutils.data.summarize(__data_summary_dfs[0])\n    else:\n        displayHTML(\"dataframe no longer exists. If you're using dataframe.display(), use display(dataframe) instead.\")\n\n  finally:\n    display = __data_summary_display_orig\n    del __data_summary_display_new\n    del __data_summary_display_orig\n    del __data_summary_dfs\n    del __data_summary_user_code_fn\nelse:\n  print(\"This DBR version does not support data profiles.\")",
       "commandTitle": "데이터 프로필 1",
       "commandType": "auto",
       "commandVersion": 0,
       "commentThread": [],
       "commentsVisible": false,
       "contentSha256Hex": null,
       "customPlotOptions": {},
       "datasetPreviewNameToCmdIdMap": {},
       "diffDeletes": [],
       "diffInserts": [],
       "displayType": "table",
       "error": null,
       "errorDetails": null,
       "errorSummary": null,
       "errorTraceType": null,
       "finishTime": 0,
       "globalVars": {},
       "guid": "",
       "height": "auto",
       "hideCommandCode": false,
       "hideCommandResult": false,
       "iPythonMetadata": null,
       "inputWidgets": {},
       "isLockedInExamMode": false,
       "latestUser": "a user",
       "latestUserId": null,
       "listResultMetadata": null,
       "metadata": {
        "byteLimit": 2048000,
        "rowLimit": 10000
       },
       "nuid": "5b40b8c9-1434-41fd-93a5-ddbffd22221d",
       "origId": 0,
       "parentHierarchy": [],
       "pivotAggregation": null,
       "pivotColumns": null,
       "position": 1.0,
       "resultDbfsErrorMessage": null,
       "resultDbfsStatus": "INLINED_IN_TREE",
       "results": null,
       "showCommandTitle": false,
       "startTime": 0,
       "state": "input",
       "streamStates": {},
       "subcommandOptions": null,
       "submitTime": 0,
       "subtype": "tableResultSubCmd.dataSummary",
       "tableResultIndex": 0,
       "tableResultSettingsMap": {},
       "useConsistentColors": false,
       "version": "CommandV1",
       "width": "auto",
       "workflows": null,
       "xColumns": null,
       "yColumns": null
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(ps_period_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f981fa1d-7116-42c7-9e40-5ea66e46d384",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "period_numeric_cols = [c for c in ps_period_df.columns if c not in ['기준년월', '발급회원번호']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7895ff8c-0f09-4b94-b9ed-bee4afe11b30",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### 이상치 처리/스케일링 - 로그 변환\n",
    "테이블 분석에서 box plot 확인 결과 대부분 positive skew로 이상치 많은 분포임. 따라서 로그 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "41162498-a297-492f-8608-e38a5ad8b04d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import log1p, col\n",
    "\n",
    "# 로그 변환\n",
    "for col_name in period_numeric_cols:\n",
    "    ps_period_df = ps_period_df.withColumn(col_name, log1p(col(col_name)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "75b16c18-e0f4-4332-a23c-0e08dc7abfbb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Databricks data profile. Run in Databricks to view."
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1.subcommand+json": {
       "baseErrorDetails": null,
       "bindings": {},
       "collapsed": false,
       "command": "%python\nif hasattr(dbutils, \"data\") and hasattr(dbutils.data, \"summarize\"):\n  # setup\n  __data_summary_display_orig = display\n  __data_summary_dfs = []\n  def __data_summary_display_new(df):\n    # add only when result is going to be table type\n    __data_summary_df_modules = [\"pandas.core.frame\", \"databricks.koalas.frame\", \"pyspark.sql.dataframe\", \"pyspark.pandas.frame\", \"pyspark.sql.connect.dataframe\", \"pyspark.sql.classic.dataframe\"]\n    if (type(df).__module__ in __data_summary_df_modules and type(df).__name__ == 'DataFrame') or isinstance(df, list):\n      __data_summary_dfs.append(df)\n  display = __data_summary_display_new\n\n  def __data_summary_user_code_fn():\n    import base64\n    exec(base64.standard_b64decode(\"ZGlzcGxheShhbW91bnRfcGVyaW9kX2RmKQ==\").decode())\n\n  try:\n    # run user code\n    __data_summary_user_code_fn()\n\n    # run on valid tableResultIndex\n    if len(__data_summary_dfs) > 0:\n      # run summarize\n      if type(__data_summary_dfs[0]).__module__ == \"databricks.koalas.frame\":\n        # koalas dataframe\n        dbutils.data.summarize(__data_summary_dfs[0].to_spark())\n      elif type(__data_summary_dfs[0]).__module__ == \"pandas.core.frame\":\n        # pandas dataframe\n        dbutils.data.summarize(spark.createDataFrame(__data_summary_dfs[0]))\n      else:\n        dbutils.data.summarize(__data_summary_dfs[0])\n    else:\n        displayHTML(\"dataframe no longer exists. If you're using dataframe.display(), use display(dataframe) instead.\")\n\n  finally:\n    display = __data_summary_display_orig\n    del __data_summary_display_new\n    del __data_summary_display_orig\n    del __data_summary_dfs\n    del __data_summary_user_code_fn\nelse:\n  print(\"This DBR version does not support data profiles.\")",
       "commandTitle": "데이터 프로필 1",
       "commandType": "auto",
       "commandVersion": 0,
       "commentThread": [],
       "commentsVisible": false,
       "contentSha256Hex": null,
       "customPlotOptions": {},
       "datasetPreviewNameToCmdIdMap": {},
       "diffDeletes": [],
       "diffInserts": [],
       "displayType": "table",
       "error": null,
       "errorDetails": null,
       "errorSummary": null,
       "errorTraceType": null,
       "finishTime": 0,
       "globalVars": {},
       "guid": "",
       "height": "auto",
       "hideCommandCode": false,
       "hideCommandResult": false,
       "iPythonMetadata": null,
       "inputWidgets": {},
       "isLockedInExamMode": false,
       "latestUser": "a user",
       "latestUserId": null,
       "listResultMetadata": null,
       "metadata": {
        "byteLimit": 2048000,
        "rowLimit": 10000
       },
       "nuid": "19390342-e7d4-4f71-b87e-8e9b3d5c09a3",
       "origId": 0,
       "parentHierarchy": [],
       "pivotAggregation": null,
       "pivotColumns": null,
       "position": 1.0,
       "resultDbfsErrorMessage": null,
       "resultDbfsStatus": "INLINED_IN_TREE",
       "results": null,
       "showCommandTitle": false,
       "startTime": 0,
       "state": "input",
       "streamStates": {},
       "subcommandOptions": null,
       "submitTime": 0,
       "subtype": "tableResultSubCmd.dataSummary",
       "tableResultIndex": 0,
       "tableResultSettingsMap": {},
       "useConsistentColors": false,
       "version": "CommandV1",
       "width": "auto",
       "workflows": null,
       "xColumns": null,
       "yColumns": null
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(ps_period_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fb6b9dd7-a4eb-4e07-8ce7-402b1d2f94bd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### 상관관계 분석 (fin)\n",
    "pyspark.ml.stat.Correlation은 **벡터 열**(아래 코드에서 features변수)에에서만 작동하므로<br>\n",
    "→ 반드시 VectorAssembler 사용해야 함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "053f85f5-b058-474e-af5b-c6f71a48b910",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**상관관계 유형 설멍**\n",
    "\n",
    "| 구분    | 피어슨 (Pearson)          | 스피어만 (Spearman)                        |\n",
    "| ----- | ---------------------- | -------------------------------------- |\n",
    "| 정의    | 변수 간의 **선형 관계** 측정     | 변수 간의 **순위 기반(모노톤) 관계** 측정             |\n",
    "| 전제 조건 | 연속형 변수 + 정규분포 근처       | 순위로 바꿔도 의미 있는 데이터                      |\n",
    "| 민감도   | 이상치에 민감                | 이상치에 강건                                |\n",
    "| 사용 예  | 소비금액처럼 **정량적인 값 간 관계** | **비선형적이지만 단조적인 관계** (ex. 만족도 등급 vs 소비 등급) |\n",
    "\n",
    "✅ 우리 분석 목적엔?\n",
    "- \"소비 금액의 절대 크기\"를 분석하고 싶다면 → 피어슨 (소비 크기에 따른 상품 추천)\n",
    "-  \"어디에 더 많이 쓰는지 성향\"을 보고 싶다면 → 스피어만(소비 성향 기반 클러스터링)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "65deb436-180c-4672-92b4-effde8ed69c1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "from pyspark.ml.stat import Correlation\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# 통계적으로 유의한 샘플 크기 계산\n",
    "def calculate_sample_size(population_size, confidence_level=0.95, margin_error=0.05):\n",
    "    \"\"\"\n",
    "    통계적으로 유의한 샘플 크기 계산\n",
    "    \"\"\"\n",
    "    z_score = 2.576  # 99% 신뢰도\n",
    "    p = 0.5  # 최대 분산\n",
    "    \n",
    "    n = (z_score**2 * p * (1-p)) / (margin_error**2)\n",
    "    n_adjusted = n / (1 + (n-1)/population_size)\n",
    "    \n",
    "    return int(n_adjusted)\n",
    "\n",
    "# 데이터 로드\n",
    "ps_df = ps_period_df\n",
    "total_count = ps_df.count()\n",
    "\n",
    "# 통계적 샘플 크기 계산\n",
    "sample_size = calculate_sample_size(total_count)\n",
    "sample_fraction = sample_size / total_count\n",
    "\n",
    "print(f\"전체 데이터: {total_count:,}\")\n",
    "print(f\"필요 샘플 크기: {sample_size:,}\")\n",
    "print(f\"샘플링 비율: {sample_fraction:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5376120c-7588-4329-81a9-c8672a67c619",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 2. 층화 샘플링 (기준년월별로 균등하게)\n",
    "def stratified_sampling(df, strata_col=\"기준년월\", sample_fraction=0.01):\n",
    "    \"\"\"\n",
    "    층화 샘플링으로 대표성 있는 샘플 생성\n",
    "    \"\"\"\n",
    "    # 각 층(기준년월)별 샘플링\n",
    "    strata_samples = []\n",
    "    \n",
    "    for month in df.select(strata_col).distinct().collect():\n",
    "        month_value = month[strata_col]\n",
    "        month_df = df.filter(col(strata_col) == month_value)\n",
    "        month_sample = month_df.sample(fraction=sample_fraction, seed=42)\n",
    "        strata_samples.append(month_sample)\n",
    "    \n",
    "    # 모든 층 합치기\n",
    "    final_sample = strata_samples[0]\n",
    "    for sample in strata_samples[1:]:\n",
    "        final_sample = final_sample.union(sample)\n",
    "    \n",
    "    return final_sample\n",
    "\n",
    "# 층화 샘플링 실행\n",
    "print(\"=== 층화 샘플링 실행 ===\")\n",
    "sampled_df = stratified_sampling(ps_df, sample_fraction=0.005)  # 0.5%\n",
    "sampled_count = sampled_df.count()\n",
    "print(f\"샘플 데이터: {sampled_count:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0414c982-972e-4d1f-a320-35d94add94e5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 3. 빠른 상관관계 분석 (피쳐 수 제한 없음)\n",
    "def fast_correlation_analysis(df):\n",
    "\n",
    "    # 수치형 컬럼 선택\n",
    "    numeric_cols = [col_name for col_name, data_type in df.dtypes\n",
    "                    if data_type in ['int', 'bigint', 'float', 'double']]\n",
    "    \n",
    "    # 키 컬럼 제외\n",
    "    exclude_cols = ['기준년월', '발급회원번호']\n",
    "    analysis_cols = [col for col in numeric_cols if col not in exclude_cols]\n",
    "    \n",
    "    print(f\"분석할 피처 수: {len(analysis_cols)}\")\n",
    "    \n",
    "    # null 처리\n",
    "    df_filled = df.fillna(0, subset=analysis_cols)\n",
    "    \n",
    "    # 벡터화\n",
    "    assembler = VectorAssembler(inputCols=analysis_cols, outputCol=\"features\")\n",
    "    vector_df = assembler.transform(df_filled).select(\"features\")\n",
    "    \n",
    "    # 캐싱\n",
    "    vector_df.cache()\n",
    "    vector_df.count()\n",
    "    \n",
    "    # 상관관계 계산\n",
    "    print(\"상관관계 계산 중...\")\n",
    "    correlation_matrix = Correlation.corr(vector_df, \"features\", method=\"pearson\").head()[0]\n",
    "    \n",
    "    return correlation_matrix, analysis_cols\n",
    "\n",
    "# 빠른 분석 실행 (모든 피처 사용)\n",
    "correlation_matrix, feature_names = fast_correlation_analysis(sampled_df)\n",
    "print(\"상관관계 계산 완료!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "49e63cf4-1e62-498e-bc9f-cc45e1dbf95a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 4. 결과 분석 및 시각화\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# 상관관계 매트릭스를 numpy 배열로 변환\n",
    "corr_array = correlation_matrix.toArray()\n",
    "\n",
    "# 높은 상관관계 찾기\n",
    "high_correlations = []\n",
    "for i in range(len(feature_names)):\n",
    "    for j in range(i+1, len(feature_names)):\n",
    "        corr_value = corr_array[i][j]\n",
    "        if abs(corr_value) > 0.7:  # 0.7 이상\n",
    "            high_correlations.append({\n",
    "                'feature1': feature_names[i],\n",
    "                'feature2': feature_names[j],\n",
    "                'correlation': corr_value\n",
    "            })\n",
    "\n",
    "# 결과 출력\n",
    "print(f\"\\n=== 높은 상관관계 ({len(high_correlations)}개) ===\")\n",
    "high_correlations_sorted = sorted(high_correlations, \n",
    "                                 key=lambda x: abs(x['correlation']), \n",
    "                                 reverse=True)\n",
    "\n",
    "for corr in high_correlations_sorted[:10]:\n",
    "    print(f\"{corr['feature1']} ↔ {corr['feature2']}: {corr['correlation']:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f659962a-efe2-452e-8116-7ba800ed929b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 5.히트맵 생성\n",
    "try:\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    \n",
    "    # 상관관계 매트릭스를 DataFrame으로 변환\n",
    "    corr_df = pd.DataFrame(corr_array, \n",
    "                          index=feature_names, \n",
    "                          columns=feature_names)\n",
    "    \n",
    "    print(f\"DataFrame 크기: {corr_df.shape}\")\n",
    "    print(f\"DataFrame 인덱스 수: {len(corr_df.index)}\")\n",
    "    print(f\"DataFrame 컬럼 수: {len(corr_df.columns)}\")\n",
    "    \n",
    "    # 큰 히트맵을 위한 설정\n",
    "    plt.figure(figsize=(20, 18))  # 크기 증가\n",
    "    \n",
    "    # 히트맵 생성 (라벨 크기 조정)\n",
    "    sns.heatmap(corr_df, \n",
    "                annot=False,  # 숫자 표시 끄기 (너무 많아서)\n",
    "                cmap='coolwarm', \n",
    "                center=0,\n",
    "                square=True, \n",
    "                fmt='.2f',\n",
    "                xticklabels=True,  # x축 라벨 표시\n",
    "                yticklabels=True,  # y축 라벨 표시\n",
    "                cbar_kws={'shrink': 0.8})\n",
    "    \n",
    "    # 라벨 크기 조정\n",
    "    plt.xticks(rotation=45, ha='right', fontsize=8)\n",
    "    plt.yticks(rotation=0, fontsize=8)\n",
    "    plt.title('Feature Correlation Heatmap (All Features)', fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # 상관관계가 높은 피처들만 별도 히트맵\n",
    "    print(\"\\n=== 높은 상관관계 피처들만 히트맵 ===\")\n",
    "    \n",
    "    # 높은 상관관계를 가진 피처들 찾기\n",
    "    high_corr_features = set()\n",
    "    threshold = 0.7\n",
    "    \n",
    "    for i in range(len(feature_names)):\n",
    "        for j in range(i+1, len(feature_names)):\n",
    "            if abs(corr_array[i][j]) > threshold:\n",
    "                high_corr_features.add(feature_names[i])\n",
    "                high_corr_features.add(feature_names[j])\n",
    "    \n",
    "    if high_corr_features:\n",
    "        high_corr_features = list(high_corr_features)\n",
    "        print(f\"높은 상관관계 피처 수: {len(high_corr_features)}\")\n",
    "        \n",
    "        # 서브셋 히트맵\n",
    "        corr_subset = corr_df.loc[high_corr_features, high_corr_features]\n",
    "        \n",
    "        plt.figure(figsize=(12, 10))\n",
    "        sns.heatmap(corr_subset, \n",
    "                    annot=False, \n",
    "                    cmap='coolwarm', \n",
    "                    center=0,\n",
    "                    square=True, \n",
    "                    fmt='.2f',\n",
    "                    xticklabels=True,\n",
    "                    yticklabels=True)\n",
    "        plt.xticks(rotation=45, ha='right')\n",
    "        plt.yticks(rotation=0)\n",
    "        plt.title(f'High Correlation Features Heatmap (>{threshold})')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"높은 상관관계를 가진 피처가 없습니다.\")\n",
    "        \n",
    "except ImportError:\n",
    "    print(\"matplotlib/seaborn이 없어 히트맵을 생성할 수 없습니다.\")\n",
    "except Exception as e:\n",
    "    print(f\"히트맵 생성 중 오류: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4dd575b6-f645-4259-9ac6-f60ea8eb4592",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 6. 다중공선성 검사\n",
    "def check_multicollinearity(corr_matrix, feature_names, threshold=0.9):\n",
    "    \"\"\"\n",
    "    다중공선성 검사\n",
    "    \"\"\"\n",
    "    corr_array = corr_matrix.toArray()\n",
    "    multicollinear_pairs = []\n",
    "    \n",
    "    for i in range(len(feature_names)):\n",
    "        for j in range(i+1, len(feature_names)):\n",
    "            corr_value = abs(corr_array[i][j])\n",
    "            if corr_value > threshold:\n",
    "                multicollinear_pairs.append({\n",
    "                    'feature1': feature_names[i],\n",
    "                    'feature2': feature_names[j],\n",
    "                    'correlation': corr_array[i][j]\n",
    "                })\n",
    "    \n",
    "    return multicollinear_pairs\n",
    "\n",
    "# 다중공선성 검사\n",
    "multicollinear = check_multicollinearity(correlation_matrix, feature_names, 0.9)\n",
    "\n",
    "print(f\"\\n=== 다중공선성 위험 ({len(multicollinear)}개) ===\")\n",
    "for pair in multicollinear:\n",
    "    print(f\"⚠️ {pair['feature1']} ↔ {pair['feature2']}: {pair['correlation']:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ec0a8a17-c810-4507-998a-cba542d76b9e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### 기간별 상관관계 분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dee092b3-4407-4658-ab42-14ae07d714d8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 기간별 컬럼 분류 함수\n",
    "def classify_columns_by_period(df_columns):\n",
    "    \"\"\"\n",
    "    컬럼명을 기간별로 분류하는 함수\n",
    "    \"\"\"\n",
    "    period_groups = {\n",
    "        'B0M': [],\n",
    "        'R3M': [],\n",
    "        'R6M': [],\n",
    "        'R12M': []\n",
    "    }\n",
    "    \n",
    "    # 기간 패턴 정의\n",
    "    period_patterns = {\n",
    "        'B0M': re.compile(r'.*_B0M$'),\n",
    "        'R3M': re.compile(r'.*_R3M$'),\n",
    "        'R6M': re.compile(r'.*_R6M$'), \n",
    "        'R12M': re.compile(r'.*_R12M$')\n",
    "    }\n",
    "    \n",
    "    for col_name in df_columns:\n",
    "        # 제외할 컬럼들\n",
    "        if col_name in ['기준년월', '발급회원번호']:\n",
    "            continue\n",
    "            \n",
    "        # 각 기간 패턴에 매칭되는지 확인\n",
    "        for period, pattern in period_patterns.items():\n",
    "            if pattern.match(col_name):\n",
    "                period_groups[period].append(col_name)\n",
    "                break\n",
    "    \n",
    "    return period_groups\n",
    "\n",
    "# ps_period_df의 컬럼 분류\n",
    "column_groups = classify_columns_by_period(ps_period_df.columns)\n",
    "\n",
    "# 결과 확인\n",
    "print(\"=== 기간별 컬럼 분류 결과 ===\")\n",
    "for period, cols in column_groups.items():\n",
    "    print(f\"{period}: {len(cols)}개 컬럼\")\n",
    "    if len(cols) > 0:\n",
    "        print(f\"  예시: {cols[:3]}...\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0e75f96e-1d6b-4138-91c0-0bc9347b305e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 기간별 상관관계 분석 함수 (기존 코드 구조 유지)\n",
    "def analyze_correlation_by_period(period_columns, period_name):\n",
    "    \"\"\"\n",
    "    특정 기간의 컬럼들에 대해 상관관계 분석을 수행하는 함수\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"=== {period_name} 기간 상관관계 분석 ===\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    if len(period_columns) == 0:\n",
    "        print(f\"{period_name} 기간에 해당하는 컬럼이 없습니다.\")\n",
    "        return None, None\n",
    "    \n",
    "    print(f\"분석 대상 컬럼 수: {len(period_columns)}\")\n",
    "    \n",
    "    # 1. 층화 샘플링 (기존 코드와 동일한 방식)\n",
    "    def stratified_sampling(df, strata_col=\"기준년월\", sample_fraction=0.005):\n",
    "        strata_samples = []\n",
    "        \n",
    "        for month in df.select(strata_col).distinct().collect():\n",
    "            month_value = month[strata_col]\n",
    "            month_df = df.filter(col(strata_col) == month_value)\n",
    "            month_sample = month_df.sample(fraction=sample_fraction, seed=42)\n",
    "            strata_samples.append(month_sample)\n",
    "        \n",
    "        # 모든 층 합치기\n",
    "        final_sample = strata_samples[0]\n",
    "        for sample in strata_samples[1:]:\n",
    "            final_sample = final_sample.union(sample)\n",
    "        \n",
    "        return final_sample\n",
    "    \n",
    "    # 샘플링 실행\n",
    "    sampled_df = stratified_sampling(ps_period_df, sample_fraction=0.005)\n",
    "    sampled_count = sampled_df.count()\n",
    "    print(f\"샘플 데이터: {sampled_count:,}\")\n",
    "    \n",
    "    # 2. 해당 기간 컬럼만 선택 + 기준년월 (층화샘플링을 위해 필요했던 컬럼)\n",
    "    period_df = sampled_df.select(period_columns)\n",
    "    \n",
    "    # 3. 빠른 상관관계 분석 (기존 함수와 동일한 로직)\n",
    "    def fast_correlation_analysis(df, analysis_cols):\n",
    "        print(f\"분석할 피처 수: {len(analysis_cols)}\")\n",
    "        \n",
    "        # null 처리\n",
    "        df_filled = df.fillna(0, subset=analysis_cols)\n",
    "        \n",
    "        # 벡터화\n",
    "        assembler = VectorAssembler(inputCols=analysis_cols, outputCol=\"features\")\n",
    "        vector_df = assembler.transform(df_filled).select(\"features\")\n",
    "        \n",
    "        # 캐싱\n",
    "        vector_df.cache()\n",
    "        vector_df.count()\n",
    "        \n",
    "        # 상관관계 계산\n",
    "        print(\"상관관계 계산 중...\")\n",
    "        correlation_matrix = Correlation.corr(vector_df, \"features\", method=\"pearson\").head()[0]\n",
    "        \n",
    "        return correlation_matrix, analysis_cols\n",
    "    \n",
    "    # 상관관계 분석 실행\n",
    "    correlation_matrix, feature_names = fast_correlation_analysis(period_df, period_columns)\n",
    "    print(\"상관관계 계산 완료!\")\n",
    "    \n",
    "    # 4. 결과 분석 (기존 코드와 동일)\n",
    "    corr_array = correlation_matrix.toArray()\n",
    "    \n",
    "    # 높은 상관관계 찾기\n",
    "    high_correlations = []\n",
    "    for i in range(len(feature_names)):\n",
    "        for j in range(i+1, len(feature_names)):\n",
    "            corr_value = corr_array[i][j]\n",
    "            if abs(corr_value) > 0.7:\n",
    "                high_correlations.append({\n",
    "                    'feature1': feature_names[i],\n",
    "                    'feature2': feature_names[j],\n",
    "                    'correlation': corr_value\n",
    "                })\n",
    "    \n",
    "    # 결과 출력\n",
    "    print(f\"\\n=== 높은 상관관계 ({len(high_correlations)}개) ===\")\n",
    "    high_correlations_sorted = sorted(high_correlations, \n",
    "                                     key=lambda x: abs(x['correlation']), \n",
    "                                     reverse=True)\n",
    "    \n",
    "    for corr in high_correlations_sorted[:10]:\n",
    "        print(f\"{corr['feature1']} ↔ {corr['feature2']}: {corr['correlation']:.3f}\")\n",
    "    \n",
    "    # 5. 다중공선성 검사 (기존 코드와 동일)\n",
    "    def check_multicollinearity(corr_matrix, feature_names, threshold=0.9):\n",
    "        corr_array = corr_matrix.toArray()\n",
    "        multicollinear_pairs = []\n",
    "        \n",
    "        for i in range(len(feature_names)):\n",
    "            for j in range(i+1, len(feature_names)):\n",
    "                corr_value = abs(corr_array[i][j])\n",
    "                if corr_value > threshold:\n",
    "                    multicollinear_pairs.append({\n",
    "                        'feature1': feature_names[i],\n",
    "                        'feature2': feature_names[j],\n",
    "                        'correlation': corr_array[i][j]\n",
    "                    })\n",
    "        \n",
    "        return multicollinear_pairs\n",
    "    \n",
    "    multicollinear = check_multicollinearity(correlation_matrix, feature_names, 0.9)\n",
    "    \n",
    "    print(f\"\\n=== 다중공선성 위험 ({len(multicollinear)}개) ===\")\n",
    "    for pair in multicollinear:\n",
    "        print(f\"⚠️ {pair['feature1']} ↔ {pair['feature2']}: {pair['correlation']:.3f}\")\n",
    "    \n",
    "    return correlation_matrix, feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "78f8728f-a6fb-4073-b1ed-3974b3205af1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 각 기간별로 상관관계 분석 실행\n",
    "correlation_results = {}\n",
    "\n",
    "for period in ['B0M', 'R3M', 'R6M', 'R12M']:\n",
    "    if len(column_groups[period]) > 0:\n",
    "        corr_matrix, feature_names = analyze_correlation_by_period(\n",
    "            column_groups[period], \n",
    "            period\n",
    "        )\n",
    "        \n",
    "        if corr_matrix is not None:\n",
    "            correlation_results[period] = {\n",
    "                'matrix': corr_matrix,\n",
    "                'features': feature_names\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9ee03287-bd3e-4246-949e-ad92afc98527",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 5. 히트맵 생성 (기존 코드와 동일한 방식)\n",
    "def create_period_heatmaps():\n",
    "    for period, data in correlation_results.items():\n",
    "        try:\n",
    "            print(f\"\\n=== {period} 히트맵 생성 ===\")\n",
    "            \n",
    "            # 상관관계 매트릭스를 DataFrame으로 변환\n",
    "            corr_array = data['matrix'].toArray()\n",
    "            feature_names = data['features']\n",
    "            \n",
    "            corr_df = pd.DataFrame(corr_array, \n",
    "                                  index=feature_names, \n",
    "                                  columns=feature_names)\n",
    "            \n",
    "            print(f\"DataFrame 크기: {corr_df.shape}\")\n",
    "            \n",
    "            # 큰 히트맵을 위한 설정\n",
    "            plt.figure(figsize=(20, 18))\n",
    "            \n",
    "            # 히트맵 생성 (라벨 크기 조정)\n",
    "            sns.heatmap(corr_df, \n",
    "                        annot=False,  # 숫자 표시 끄기\n",
    "                        cmap='coolwarm', \n",
    "                        center=0,\n",
    "                        square=True, \n",
    "                        fmt='.2f',\n",
    "                        xticklabels=True,\n",
    "                        yticklabels=True,\n",
    "                        cbar_kws={'shrink': 0.8})\n",
    "            \n",
    "            # 라벨 크기 조정\n",
    "            plt.xticks(rotation=45, ha='right', fontsize=8)\n",
    "            plt.yticks(rotation=0, fontsize=8)\n",
    "            plt.title(f'{period} 기간 Feature Correlation Heatmap', fontsize=16)\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            \n",
    "            # 상관관계가 높은 피처들만 별도 히트맵\n",
    "            print(f\"\\n=== {period} 높은 상관관계 피처들만 히트맵 ===\")\n",
    "            \n",
    "            high_corr_features = set()\n",
    "            threshold = 0.7\n",
    "            \n",
    "            for i in range(len(feature_names)):\n",
    "                for j in range(i+1, len(feature_names)):\n",
    "                    if abs(corr_array[i][j]) > threshold:\n",
    "                        high_corr_features.add(feature_names[i])\n",
    "                        high_corr_features.add(feature_names[j])\n",
    "            \n",
    "            if high_corr_features:\n",
    "                high_corr_features = list(high_corr_features)\n",
    "                print(f\"높은 상관관계 피처 수: {len(high_corr_features)}\")\n",
    "                \n",
    "                # 서브셋 히트맵\n",
    "                corr_subset = corr_df.loc[high_corr_features, high_corr_features]\n",
    "                \n",
    "                plt.figure(figsize=(12, 10))\n",
    "                sns.heatmap(corr_subset, \n",
    "                            annot=False, \n",
    "                            cmap='coolwarm', \n",
    "                            center=0,\n",
    "                            square=True, \n",
    "                            fmt='.2f',\n",
    "                            xticklabels=True,\n",
    "                            yticklabels=True)\n",
    "                plt.xticks(rotation=45, ha='right')\n",
    "                plt.yticks(rotation=0)\n",
    "                plt.title(f'{period} High Correlation Features Heatmap (>{threshold})')\n",
    "                plt.tight_layout()\n",
    "                plt.show()\n",
    "            else:\n",
    "                print(\"높은 상관관계를 가진 피처가 없습니다.\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"{period} 히트맵 생성 중 오류: {str(e)}\")\n",
    "\n",
    "# 히트맵 생성 실행\n",
    "create_period_heatmaps()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2469056c-eb78-4e0b-b1e1-4b1ade176ff5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 기간별 분석 결과 요약\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"=== 기간별 상관관계 분석 결과 요약 ===\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for period in ['B0M', 'R3M', 'R6M', 'R12M']:\n",
    "    print(f\"\\n[{period}]\")\n",
    "    if period in correlation_results:\n",
    "        n_features = len(correlation_results[period]['features'])\n",
    "        print(f\"  - 분석된 피처 수: {n_features}\")\n",
    "        print(f\"  - 상관관계 매트릭스 크기: {n_features}x{n_features}\")\n",
    "        print(f\"  - 상태: ✅ 분석 완료\")\n",
    "    else:\n",
    "        n_features = len(column_groups[period])\n",
    "        if n_features == 0:\n",
    "            print(f\"  - 해당 기간 컬럼 없음\")\n",
    "        else:\n",
    "            print(f\"  - 피처 수: {n_features}\")\n",
    "            print(f\"  - 상태: ❌ 분석 실패\")\n",
    "\n",
    "print(f\"\\n전체 분석 완료된 기간: {len(correlation_results)}개\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "81fdc4c4-3194-4975-bcdc-3d15fe638083",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### 상관관계 분석 (GPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1389fde4-7d8a-4ace-96ce-82a6e704f7cc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pyspark.sql.functions import col\n",
    "import time\n",
    "\n",
    "print(\"=== GPU로 상관관계 분석 ===\")\n",
    "\n",
    "# 1. GPU 메모리 최적화 설정\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"사용 디바이스: {device}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    gpu_props = torch.cuda.get_device_properties(0)\n",
    "    total_memory = gpu_props.total_memory / 1024**3\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU 총 메모리: {total_memory:.1f} GB\")\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "# 2. 전체 데이터 로드\n",
    "ps_df = ps_period_df\n",
    "numeric_cols = [col_name for col_name, data_type in ps_df.dtypes\n",
    "                if data_type in ['int', 'bigint', 'float', 'double']]\n",
    "\n",
    "total_rows = ps_df.count()\n",
    "n_features = len(numeric_cols)\n",
    "print(f\"전체 데이터 수: {total_rows:,}\")\n",
    "print(f\"분석할 피처 수: {n_features}\")\n",
    "\n",
    "def compute_large_chunk_correlation(df, numeric_cols, device):\n",
    "    \"\"\"\n",
    "    대용량 청크로 GPU 상관관계 계산 (메모리 활용도 극대화)\n",
    "    \"\"\"\n",
    "    print(\"\\n=== 대용량 청크 GPU 처리 시작 ===\")\n",
    "    \n",
    "    # 훨씬 큰 청크 크기 설정 (Tesla T4 16GB 기준)\n",
    "    if torch.cuda.is_available():\n",
    "        # 16GB GPU에서 안전하게 사용할 수 있는 크기\n",
    "        # 상관관계 매트릭스 계산 시 중간 결과물 고려하여 보수적으로 설정\n",
    "        large_chunk_size = 2000000  # 200만 행부터 시작\n",
    "        \n",
    "        # 메모리 사용량 추정\n",
    "        estimated_memory_gb = (large_chunk_size * n_features * 4) / 1024**3  # float32 기준\n",
    "        print(f\"청크당 예상 메모리 사용량: {estimated_memory_gb:.2f} GB\")\n",
    "        \n",
    "        # GPU 메모리의 70% 이상 사용하도록 조정\n",
    "        target_memory_usage = total_memory * 0.7  # 70% 사용 목표\n",
    "        optimal_chunk_size = int((target_memory_usage * 1024**3) / (n_features * 4 * 3))  # 안전 마진\n",
    "        \n",
    "        # 최종 청크 크기 결정 (최소 100만, 최대 500만)\n",
    "        final_chunk_size = max(1000000, min(optimal_chunk_size, 5000000))\n",
    "        \n",
    "    else:\n",
    "        final_chunk_size = 1000000  # CPU의 경우\n",
    "    \n",
    "    print(f\"최종 청크 크기: {final_chunk_size:,} 행\")\n",
    "    \n",
    "    # 청크 개수 계산\n",
    "    n_rows = df.count()\n",
    "    n_chunks = max(1, (n_rows + final_chunk_size - 1) // final_chunk_size)\n",
    "    \n",
    "    print(f\"총 {n_chunks}개 청크로 분할\")\n",
    "    \n",
    "    if n_chunks > 10:\n",
    "        print(\"⚠️ 청크 개수가 많습니다. 청크 크기를 더 늘려보겠습니다.\")\n",
    "        final_chunk_size = max(final_chunk_size, n_rows // 5)  # 최대 5개 청크로 제한\n",
    "        n_chunks = max(1, (n_rows + final_chunk_size - 1) // final_chunk_size)\n",
    "        print(f\"조정된 청크 크기: {final_chunk_size:,} 행\")\n",
    "        print(f\"조정된 청크 개수: {n_chunks}개\")\n",
    "    \n",
    "    # 상관관계 매트릭스 누적을 위한 변수들\n",
    "    correlation_sum = None\n",
    "    total_weight = 0\n",
    "    \n",
    "    for chunk_idx in range(n_chunks):\n",
    "        chunk_start_time = time.time()\n",
    "        \n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"청크 {chunk_idx + 1}/{n_chunks} 처리 중...\")\n",
    "        \n",
    "        # 청크 데이터 추출 (더 효율적인 방법)\n",
    "        if n_chunks == 1:\n",
    "            # 전체 데이터를 한 번에 처리\n",
    "            chunk_df = df\n",
    "        else:\n",
    "            # 분할 처리\n",
    "            chunk_fraction = 1.0 / n_chunks\n",
    "            chunk_df = df.sample(fraction=chunk_fraction, seed=42 + chunk_idx)\n",
    "        \n",
    "        # Pandas로 변환\n",
    "        print(\"  PySpark → Pandas 변환 중...\")\n",
    "        conversion_start = time.time()\n",
    "        chunk_pdf = chunk_df.select(*numeric_cols).fillna(0).toPandas()\n",
    "        conversion_time = time.time() - conversion_start\n",
    "        \n",
    "        actual_chunk_size = len(chunk_pdf)\n",
    "        print(f\"  실제 청크 크기: {actual_chunk_size:,} x {len(chunk_pdf.columns)}\")\n",
    "        print(f\"  변환 시간: {conversion_time:.2f}초\")\n",
    "        \n",
    "        if actual_chunk_size == 0:\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            # GPU 메모리 상태 확인\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "                initial_memory = torch.cuda.memory_allocated() / 1024**3\n",
    "                available_memory = (torch.cuda.get_device_properties(0).total_memory - torch.cuda.memory_allocated()) / 1024**3\n",
    "                print(f\"  사용 가능 GPU 메모리: {available_memory:.2f} GB\")\n",
    "            \n",
    "            # GPU 텐서로 변환\n",
    "            print(\"  GPU 텐서 변환 중...\")\n",
    "            tensor_start = time.time()\n",
    "            chunk_tensor = torch.tensor(chunk_pdf.values, dtype=torch.float32).to(device)\n",
    "            tensor_time = time.time() - tensor_start\n",
    "            \n",
    "            if torch.cuda.is_available():\n",
    "                after_tensor_memory = torch.cuda.memory_allocated() / 1024**3\n",
    "                memory_used = after_tensor_memory - initial_memory\n",
    "                print(f\"  GPU 메모리 사용량: {memory_used:.2f} GB\")\n",
    "                print(f\"  텐서 변환 시간: {tensor_time:.2f}초\")\n",
    "            \n",
    "            # 상관관계 계산\n",
    "            print(\"  상관관계 계산 중...\")\n",
    "            corr_start = time.time()\n",
    "            chunk_correlation = torch.corrcoef(chunk_tensor.T)\n",
    "            corr_time = time.time() - corr_start\n",
    "            print(f\"  상관관계 계산 시간: {corr_time:.2f}초\")\n",
    "            \n",
    "            # CPU로 이동하여 누적\n",
    "            chunk_corr_cpu = chunk_correlation.cpu().numpy()\n",
    "            \n",
    "            # 가중 평균으로 누적\n",
    "            weight = actual_chunk_size\n",
    "            if correlation_sum is None:\n",
    "                correlation_sum = chunk_corr_cpu * weight\n",
    "            else:\n",
    "                correlation_sum += chunk_corr_cpu * weight\n",
    "            total_weight += weight\n",
    "            \n",
    "            # 메모리 정리\n",
    "            del chunk_tensor, chunk_correlation, chunk_pdf\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "            \n",
    "            chunk_total_time = time.time() - chunk_start_time\n",
    "            print(f\"  청크 총 처리 시간: {chunk_total_time:.2f}초\")\n",
    "            print(f\"  진행률: {(chunk_idx + 1) / n_chunks * 100:.1f}%\")\n",
    "            \n",
    "            # 남은 시간 추정\n",
    "            if chunk_idx > 0:\n",
    "                avg_time_per_chunk = (time.time() - start_time) / (chunk_idx + 1)\n",
    "                remaining_time = avg_time_per_chunk * (n_chunks - chunk_idx - 1)\n",
    "                print(f\"  예상 남은 시간: {remaining_time:.1f}초 ({remaining_time/60:.1f}분)\")\n",
    "            \n",
    "        except RuntimeError as e:\n",
    "            if \"out of memory\" in str(e):\n",
    "                print(f\"  ❌ GPU 메모리 부족! 현재 청크 크기: {actual_chunk_size:,}\")\n",
    "                print(\"  더 작은 청크로 재시도하거나 CPU로 fallback이 필요합니다.\")\n",
    "                raise e\n",
    "            else:\n",
    "                raise e\n",
    "    \n",
    "    # 최종 상관관계 매트릭스 계산\n",
    "    if total_weight > 0:\n",
    "        final_correlation = correlation_sum / total_weight\n",
    "        return final_correlation, numeric_cols\n",
    "    else:\n",
    "        return None, numeric_cols\n",
    "\n",
    "# 3. 대용량 청크 상관관계 분석 실행\n",
    "print(f\"\\n🚀 대용량 청크로 전체 데이터 {total_rows:,}행 분석 시작\")\n",
    "start_time = time.time()\n",
    "\n",
    "try:\n",
    "    correlation_matrix, feature_names = compute_large_chunk_correlation(\n",
    "        ps_df, \n",
    "        numeric_cols, \n",
    "        device\n",
    "    )\n",
    "    \n",
    "    end_time = time.time()\n",
    "    total_time = end_time - start_time\n",
    "    print(f\"\\n⏱️ 전체 처리 시간: {total_time:.2f}초 ({total_time/60:.1f}분)\")\n",
    "    print(f\"📊 처리 속도: {total_rows/total_time:,.0f} 행/초\")\n",
    "    \n",
    "    # 4. 결과 분석\n",
    "    if correlation_matrix is not None:\n",
    "        print(f\"\\n=== ✅ 대용량 청크 상관관계 분석 결과 ✅ ===\")\n",
    "        print(f\"분석된 데이터: {total_rows:,}행\")\n",
    "        print(f\"분석된 피처: {len(feature_names)}개\")\n",
    "        print(f\"상관관계 매트릭스 크기: {correlation_matrix.shape}\")\n",
    "        \n",
    "        # 높은 상관관계 분석\n",
    "        def analyze_correlations(corr_matrix, features, threshold=0.7):\n",
    "            high_corr = []\n",
    "            n = len(features)\n",
    "            for i in range(n):\n",
    "                for j in range(i+1, n):\n",
    "                    corr_val = corr_matrix[i, j]\n",
    "                    if abs(corr_val) > threshold:\n",
    "                        high_corr.append({\n",
    "                            'feature1': features[i],\n",
    "                            'feature2': features[j],\n",
    "                            'correlation': float(corr_val)\n",
    "                        })\n",
    "            return high_corr\n",
    "        \n",
    "        # 높은 상관관계 출력\n",
    "        high_correlations = analyze_correlations(correlation_matrix, feature_names, 0.7)\n",
    "        print(f\"\\n높은 상관관계 (|r| > 0.7): {len(high_correlations)}개\")\n",
    "        \n",
    "        for pair in sorted(high_correlations, key=lambda x: abs(x['correlation']), reverse=True)[:20]:\n",
    "            print(f\"  {pair['feature1']} ↔ {pair['feature2']}: {pair['correlation']:.3f}\")\n",
    "        \n",
    "        # 다중공선성 위험\n",
    "        multicollinear = analyze_correlations(correlation_matrix, feature_names, 0.9)\n",
    "        print(f\"\\n다중공선성 위험 (|r| > 0.9): {len(multicollinear)}개\")\n",
    "        \n",
    "        for pair in sorted(multicollinear, key=lambda x: abs(x['correlation']), reverse=True):\n",
    "            print(f\"  ⚠️ {pair['feature1']} ↔ {pair['feature2']}: {pair['correlation']:.3f}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"❌ 오류 발생: {str(e)}\")\n",
    "    print(\"CPU로 fallback을 시도하거나 청크 크기를 더 줄여보세요.\")\n",
    "\n",
    "# 5. 메모리 정리\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    final_memory = torch.cuda.memory_allocated() / 1024**3\n",
    "    peak_memory = torch.cuda.max_memory_allocated() / 1024**3\n",
    "    print(f\"\\n🧹 최종 GPU 메모리 사용량: {final_memory:.2f} GB\")\n",
    "    print(f\"📈 최대 GPU 메모리 사용량: {peak_memory:.2f} GB\")\n",
    "\n",
    "print(\"\\n✅ GPU 상관관계 분석 완료!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "759341fa-7045-49a7-905b-482244f5a775",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### 기간별 상관관계 분석 (GPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c94e55e7-bec6-45b6-bda5-a2bdbaca4de4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pyspark.sql.functions import col\n",
    "import time\n",
    "import re\n",
    "\n",
    "print(\"=== 기간별 컬럼 분류 후 GPU 상관관계 분석 ===\")\n",
    "\n",
    "# 1. GPU 메모리 최적화 설정\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"사용 디바이스: {device}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    gpu_props = torch.cuda.get_device_properties(0)\n",
    "    total_memory = gpu_props.total_memory / 1024**3\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU 총 메모리: {total_memory:.1f} GB\")\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "# 2. 데이터 로드\n",
    "ps_df = ps_period_df\n",
    "print(f\"전체 데이터 수: {ps_df.count():,}\")\n",
    "\n",
    "# 3. 기간별 컬럼 분류 함수 (개선된 버전)\n",
    "def classify_columns_by_period(df_columns):\n",
    "    \"\"\"\n",
    "    컬럼명을 기간별로 분류하는 함수\n",
    "    \"\"\"\n",
    "    period_groups = {\n",
    "        'B0M': [],\n",
    "        'R3M': [],\n",
    "        'R6M': [],\n",
    "        'R12M': []\n",
    "    }\n",
    "    \n",
    "    # 기간 패턴 정의 (더 포괄적으로)\n",
    "    period_patterns = {\n",
    "        'B0M': re.compile(r'.*_B0M$'),\n",
    "        'R3M': re.compile(r'.*_R3M$'),\n",
    "        'R6M': re.compile(r'.*_R6M$'),\n",
    "        'R12M': re.compile(r'.*_R12M$')\n",
    "    }\n",
    "    \n",
    "    non_period_cols = []  # 기간이 없는 컬럼들\n",
    "    \n",
    "    for col_name in df_columns:\n",
    "        # 제외할 컬럼들\n",
    "        if col_name in ['기준년월', '발급회원번호']:\n",
    "            continue\n",
    "            \n",
    "        # 각 기간 패턴에 매칭되는지 확인\n",
    "        matched = False\n",
    "        for period, pattern in period_patterns.items():\n",
    "            if pattern.match(col_name):\n",
    "                period_groups[period].append(col_name)\n",
    "                matched = True\n",
    "                break\n",
    "        \n",
    "        if not matched:\n",
    "            non_period_cols.append(col_name)\n",
    "    \n",
    "    period_groups['non_period'] = non_period_cols\n",
    "    return period_groups\n",
    "\n",
    "# 4. 컬럼 분류 실행\n",
    "column_groups = classify_columns_by_period(ps_df.columns)\n",
    "\n",
    "print(\"=== 기간별 컬럼 분류 결과 ===\")\n",
    "for period, cols in column_groups.items():\n",
    "    print(f\"{period}: {len(cols)}개 컬럼\")\n",
    "    if len(cols) > 0:\n",
    "        print(f\"  예시: {cols[:3]}...\")\n",
    "print()\n",
    "\n",
    "# 5. GPU 상관관계 분석 함수 (기간별 적용)\n",
    "def compute_period_correlation_gpu(df, period_columns, period_name, device):\n",
    "    \"\"\"\n",
    "    특정 기간의 컬럼들에 대해 GPU 상관관계 계산\n",
    "    \"\"\"\n",
    "    if len(period_columns) == 0:\n",
    "        print(f\"⚠️ {period_name}: 분석할 컬럼이 없습니다.\")\n",
    "        return None, []\n",
    "    \n",
    "    print(f\"\\n=== {period_name} 기간 GPU 상관관계 분석 ===\")\n",
    "    print(f\"분석 컬럼 수: {len(period_columns)}\")\n",
    "    \n",
    "    # 수치형 컬럼만 필터링\n",
    "    numeric_period_cols = []\n",
    "    for col_name in period_columns:\n",
    "        col_type = dict(df.dtypes)[col_name]\n",
    "        if col_type in ['int', 'bigint', 'float', 'double']:\n",
    "            numeric_period_cols.append(col_name)\n",
    "    \n",
    "    if len(numeric_period_cols) == 0:\n",
    "        print(f\"⚠️ {period_name}: 수치형 컬럼이 없습니다.\")\n",
    "        return None, []\n",
    "    \n",
    "    print(f\"수치형 컬럼 수: {len(numeric_period_cols)}\")\n",
    "    \n",
    "    # 메모리 효율적인 청크 크기 계산\n",
    "    n_features = len(numeric_period_cols)\n",
    "    if torch.cuda.is_available():\n",
    "        # 피처 수에 따른 동적 청크 크기 조정\n",
    "        if n_features <= 10:\n",
    "            chunk_size = 5000000  # 피처가 적으면 더 큰 청크\n",
    "        elif n_features <= 20:\n",
    "            chunk_size = 3000000\n",
    "        elif n_features <= 50:\n",
    "            chunk_size = 2000000\n",
    "        else:\n",
    "            chunk_size = 1000000  # 피처가 많으면 작은 청크\n",
    "    else:\n",
    "        chunk_size = 500000\n",
    "    \n",
    "    print(f\"청크 크기: {chunk_size:,}\")\n",
    "    \n",
    "    # 전체 데이터 처리\n",
    "    total_rows = df.count()\n",
    "    n_chunks = max(1, (total_rows + chunk_size - 1) // chunk_size)\n",
    "    print(f\"총 {n_chunks}개 청크로 분할\")\n",
    "    \n",
    "    correlation_sum = None\n",
    "    total_weight = 0\n",
    "    \n",
    "    for chunk_idx in range(n_chunks):\n",
    "        chunk_start_time = time.time()\n",
    "        print(f\"\\n  청크 {chunk_idx + 1}/{n_chunks} 처리 중...\")\n",
    "        \n",
    "        # 청크 데이터 추출\n",
    "        if n_chunks == 1:\n",
    "            chunk_df = df\n",
    "        else:\n",
    "            chunk_fraction = 1.0 / n_chunks\n",
    "            chunk_df = df.sample(fraction=chunk_fraction, seed=42 + chunk_idx)\n",
    "        \n",
    "        # Pandas로 변환\n",
    "        chunk_pdf = chunk_df.select(*numeric_period_cols).fillna(0).toPandas()\n",
    "        actual_chunk_size = len(chunk_pdf)\n",
    "        \n",
    "        if actual_chunk_size == 0:\n",
    "            continue\n",
    "        \n",
    "        print(f\"    실제 청크 크기: {actual_chunk_size:,} x {len(chunk_pdf.columns)}\")\n",
    "        \n",
    "        try:\n",
    "            # GPU 텐서로 변환\n",
    "            chunk_tensor = torch.tensor(chunk_pdf.values, dtype=torch.float32).to(device)\n",
    "            \n",
    "            if torch.cuda.is_available():\n",
    "                memory_used = torch.cuda.memory_allocated() / 1024**3\n",
    "                print(f\"    GPU 메모리 사용량: {memory_used:.2f} GB\")\n",
    "            \n",
    "            # 상관관계 계산\n",
    "            chunk_correlation = torch.corrcoef(chunk_tensor.T)\n",
    "            chunk_corr_cpu = chunk_correlation.cpu().numpy()\n",
    "            \n",
    "            # 가중 평균으로 누적\n",
    "            weight = actual_chunk_size\n",
    "            if correlation_sum is None:\n",
    "                correlation_sum = chunk_corr_cpu * weight\n",
    "            else:\n",
    "                correlation_sum += chunk_corr_cpu * weight\n",
    "            total_weight += weight\n",
    "            \n",
    "            # 메모리 정리\n",
    "            del chunk_tensor, chunk_correlation, chunk_pdf\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "            \n",
    "            chunk_time = time.time() - chunk_start_time\n",
    "            print(f\"    청크 처리 시간: {chunk_time:.2f}초\")\n",
    "            \n",
    "        except RuntimeError as e:\n",
    "            if \"out of memory\" in str(e):\n",
    "                print(f\"    ⚠️ GPU 메모리 부족, 청크 크기 조정 필요\")\n",
    "                return None, numeric_period_cols\n",
    "            else:\n",
    "                raise e\n",
    "    \n",
    "    # 최종 상관관계 매트릭스 계산\n",
    "    if total_weight > 0:\n",
    "        final_correlation = correlation_sum / total_weight\n",
    "        return final_correlation, numeric_period_cols\n",
    "    else:\n",
    "        return None, numeric_period_cols\n",
    "\n",
    "# 6. 상관관계 분석 함수\n",
    "def analyze_correlations(corr_matrix, features, threshold=0.7):\n",
    "    \"\"\"높은 상관관계 분석\"\"\"\n",
    "    high_corr = []\n",
    "    n = len(features)\n",
    "    for i in range(n):\n",
    "        for j in range(i+1, n):\n",
    "            corr_val = corr_matrix[i, j]\n",
    "            if abs(corr_val) > threshold:\n",
    "                high_corr.append({\n",
    "                    'feature1': features[i],\n",
    "                    'feature2': features[j],\n",
    "                    'correlation': float(corr_val)\n",
    "                })\n",
    "    return high_corr\n",
    "\n",
    "# 7. 각 기간별로 상관관계 분석 실행\n",
    "period_results = {}\n",
    "analysis_start_time = time.time()\n",
    "\n",
    "for period_name, period_cols in column_groups.items():\n",
    "    if period_name == 'non_period':\n",
    "        continue  # 기간이 없는 컬럼은 별도 처리\n",
    "    \n",
    "    if len(period_cols) == 0:\n",
    "        continue\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"🔍 {period_name} 기간 분석 시작\")\n",
    "    \n",
    "    period_start_time = time.time()\n",
    "    \n",
    "    # GPU 상관관계 분석\n",
    "    correlation_matrix, feature_names = compute_period_correlation_gpu(\n",
    "        ps_df, period_cols, period_name, device\n",
    "    )\n",
    "    \n",
    "    if correlation_matrix is not None:\n",
    "        period_time = time.time() - period_start_time\n",
    "        \n",
    "        print(f\"\\n=== {period_name} 분석 결과 ===\")\n",
    "        print(f\"처리 시간: {period_time:.2f}초\")\n",
    "        print(f\"분석된 피처: {len(feature_names)}개\")\n",
    "        print(f\"상관관계 매트릭스 크기: {correlation_matrix.shape}\")\n",
    "        \n",
    "        # 높은 상관관계 분석\n",
    "        high_correlations = analyze_correlations(correlation_matrix, feature_names, 0.7)\n",
    "        print(f\"높은 상관관계 (|r| > 0.7): {len(high_correlations)}개\")\n",
    "        \n",
    "        # 상위 10개 출력\n",
    "        for pair in sorted(high_correlations, key=lambda x: abs(x['correlation']), reverse=True)[:100]:\n",
    "            print(f\"  {pair['feature1']} ↔ {pair['feature2']}: {pair['correlation']:.3f}\")\n",
    "        \n",
    "        # 다중공선성 위험\n",
    "        multicollinear = analyze_correlations(correlation_matrix, feature_names, 0.9)\n",
    "        print(f\"다중공선성 위험 (|r| > 0.9): {len(multicollinear)}개\")\n",
    "        \n",
    "        for pair in sorted(multicollinear, key=lambda x: abs(x['correlation']), reverse=True)[:50]:\n",
    "            print(f\"  ⚠️ {pair['feature1']} ↔ {pair['feature2']}: {pair['correlation']:.3f}\")\n",
    "        \n",
    "        # 결과 저장\n",
    "        period_results[period_name] = {\n",
    "            'correlation_matrix': correlation_matrix,\n",
    "            'feature_names': feature_names,\n",
    "            'high_correlations': high_correlations,\n",
    "            'multicollinear': multicollinear,\n",
    "            'processing_time': period_time\n",
    "        }\n",
    "    \n",
    "    else:\n",
    "        print(f\"❌ {period_name} 분석 실패\")\n",
    "\n",
    "# 8. 전체 결과 요약\n",
    "total_analysis_time = time.time() - analysis_start_time\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"🎯 전체 기간별 상관관계 분석 완료!\")\n",
    "print(f\"총 처리 시간: {total_analysis_time:.2f}초 ({total_analysis_time/60:.1f}분)\")\n",
    "\n",
    "print(f\"\\n=== 기간별 분석 요약 ===\")\n",
    "for period_name, results in period_results.items():\n",
    "    print(f\"{period_name}:\")\n",
    "    print(f\"  - 피처 수: {len(results['feature_names'])}\")\n",
    "    print(f\"  - 높은 상관관계: {len(results['high_correlations'])}개\")\n",
    "    print(f\"  - 다중공선성 위험: {len(results['multicollinear'])}개\")\n",
    "    print(f\"  - 처리 시간: {results['processing_time']:.2f}초\")\n",
    "\n",
    "# 9. 기간 간 비교 분석 (옵션)\n",
    "print(f\"\\n=== 기간 간 상관관계 패턴 비교 ===\")\n",
    "for period_name, results in period_results.items():\n",
    "    if len(results['high_correlations']) > 0:\n",
    "        print(f\"\\n{period_name} 주요 상관관계:\")\n",
    "        # 가장 높은 상관관계 3개\n",
    "        top_corrs = sorted(results['high_correlations'], \n",
    "                          key=lambda x: abs(x['correlation']), reverse=True)[:3]\n",
    "        for i, pair in enumerate(top_corrs, 1):\n",
    "            print(f\"  {i}. {pair['feature1']} ↔ {pair['feature2']}: {pair['correlation']:.3f}\")\n",
    "\n",
    "# 10. 메모리 정리\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    final_memory = torch.cuda.memory_allocated() / 1024**3\n",
    "    peak_memory = torch.cuda.max_memory_allocated() / 1024**3\n",
    "    print(f\"\\n🧹 최종 GPU 메모리 사용량: {final_memory:.2f} GB\")\n",
    "    print(f\"📈 최대 GPU 메모리 사용량: {peak_memory:.2f} GB\")\n",
    "\n",
    "print(\"\\n✅ 기간별 GPU 상관관계 분석 완료!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "77cb700d-290f-41f1-a6e0-6a4f23a6415f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### 다중공신성 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f595fc5c-b024-4515-99e3-8cc886c1fb6c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def solve_count_df_complete_multicollinearity_final(ps_df):\n",
    "    \"\"\"\n",
    "    COUNT_DF 모든 다중공선성 완전 해결 (추가 누락분 포함)\n",
    "    \"\"\"\n",
    "    print(\"=== COUNT_DF 모든 다중공선성 완전 해결 (최종) ===\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # 전체 다중공선성 패턴 분석 후 삭제 전략 (누락분 포함)\n",
    "    multicollinearity_drops = {\n",
    "        # 1. 완전 중복 (상관계수 1.0)\n",
    "        \"완전_중복\": [\n",
    "            \"할부건수_부분_12M_R12M\"  # 이용건수_부분무이자_R12M과 1.000\n",
    "        ],\n",
    "        \n",
    "        # 2. 신용카드 계층 구조 (신용 > 신판 > 일시불)\n",
    "        \"신용카드_계층\": [\n",
    "            # B0M 기간\n",
    "            \"이용건수_신판_B0M\",      # 신용과 0.999, 일시불과 0.999\n",
    "            \"이용건수_일시불_B0M\",     # 신용과 0.998\n",
    "            \n",
    "            # R3M 기간  \n",
    "            \"이용건수_신판_R3M\",      # 신용과 0.998, 일시불과 0.999\n",
    "            \"이용건수_일시불_R3M\",     # 신용과 0.997\n",
    "            \n",
    "            # R6M 기간\n",
    "            \"이용건수_신판_R6M\",      # 신용과 0.998, 일시불과 0.997  \n",
    "            \"이용건수_일시불_R6M\",     # 신용과 0.995\n",
    "            \n",
    "            # R12M 기간\n",
    "            \"이용건수_신판_R12M\",     # 신용과 0.997, 일시불과 0.993\n",
    "            \"이용건수_일시불_R12M\"     # 신용과 0.988\n",
    "        ],\n",
    "        \n",
    "        # 3. 오프라인 중복 (신용카드 사용이 대부분 오프라인)\n",
    "        \"오프라인_중복\": [\n",
    "            \"이용건수_오프라인_B0M\",   # 이용건수_신용_B0M과 0.948\n",
    "            \"이용건수_오프라인_R3M\",   # 이용건수_신용_R3M과 0.932  \n",
    "            \"이용건수_오프라인_R6M\"    # 이용건수_신용_R6M과 0.916\n",
    "        ],\n",
    "        \n",
    "        # 4. 승인거절 관련 (전체 > 세부사항)\n",
    "        \"승인거절_중복\": [\n",
    "            \"승인거절건수_한도초과_B0M\"  # 승인거절건수_B0M과 0.991\n",
    "        ],\n",
    "        \n",
    "        # 5. RP 관련 중복 (RP건수가 RP유형건수 포함)\n",
    "        \"RP_중복\": [\n",
    "            \"RP유형건수_B0M\"          # RP건수_B0M과 0.916\n",
    "        ],\n",
    "        \n",
    "        # 6. 페이 서비스 중복\n",
    "        \"페이_중복\": [\n",
    "            # A페이는 페이_오프라인에 포함\n",
    "            \"이용건수_A페이_B0M\",      # 이용건수_페이_오프라인_B0M과 0.931\n",
    "            \"이용건수_A페이_R6M\",      # 이용건수_페이_오프라인_R6M과 0.911\n",
    "            \n",
    "            # 간편결제는 페이_온라인에 포함\n",
    "            \"이용건수_간편결제_R3M\",   # 이용건수_페이_온라인_R3M과 0.918\n",
    "            \"이용건수_간편결제_R6M\"    # 이용건수_페이_온라인_R6M과 0.950\n",
    "        ],\n",
    "        \n",
    "        # 7. 할부 관련 중복 (전체 할부 > 무이자 할부)\n",
    "        \"할부_기본_중복\": [\n",
    "            \"이용건수_할부_무이자_B0M\",   # 이용건수_할부_B0M과 0.944\n",
    "            \"이용건수_할부_무이자_R3M\",   # 이용건수_할부_R3M과 0.951  \n",
    "            \"이용건수_할부_무이자_R6M\",   # 이용건수_할부_R6M과 0.962\n",
    "            \"이용건수_할부_무이자_R12M\"   # 이용건수_할부_R12M과 0.959\n",
    "        ],\n",
    "        \n",
    "        # 8. 할부 기간별 세분화 중복 (R12M 집중)\n",
    "        \"할부_기간_중복\": [\n",
    "            # 할부건수는 이용건수에 포함됨\n",
    "            \"할부건수_3M_R12M\",           # 이용건수_할부_R12M과 0.968\n",
    "            \"할부건수_무이자_3M_R12M\",    # 이용건수_할부_무이자_R12M과 0.966\n",
    "            \"할부건수_무이자_6M_R12M\",    # 할부건수_6M_R12M과 0.954\n",
    "            \"할부건수_유이자_3M_R12M\",    # 이용건수_할부_유이자_R12M과 0.944\n",
    "            \"할부건수_유이자_14M_R12M\"    # 할부건수_14M_R12M과 0.987\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    # 전체 삭제 컬럼 리스트\n",
    "    all_drops = []\n",
    "    for category, cols in multicollinearity_drops.items():\n",
    "        all_drops.extend(cols)\n",
    "    \n",
    "    print(\"📋 완전한 다중공선성 해결 전략:\")\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    total_drops = 0\n",
    "    for category, cols in multicollinearity_drops.items():\n",
    "        print(f\"\\n🔸 {category} ({len(cols)}개):\")\n",
    "        for i, col in enumerate(cols, 1):\n",
    "            print(f\"   {i}. {col}\")\n",
    "        total_drops += len(cols)\n",
    "    \n",
    "    print(f\"\\n📊 삭제 요약:\")\n",
    "    print(f\"   총 삭제 대상: {total_drops}개 컬럼\")\n",
    "    \n",
    "    # 실제 존재하는 컬럼만 필터링\n",
    "    existing_drops = [col for col in all_drops if col in ps_df.columns]\n",
    "    missing_cols = [col for col in all_drops if col not in ps_df.columns]\n",
    "    \n",
    "    print(f\"   실제 존재: {len(existing_drops)}개\")\n",
    "    print(f\"   존재하지 않음: {len(missing_cols)}개\")\n",
    "    \n",
    "    if missing_cols:\n",
    "        print(f\"\\n⚠️ 존재하지 않는 컬럼들:\")\n",
    "        for col in missing_cols:\n",
    "            print(f\"   - {col}\")\n",
    "    \n",
    "    print(f\"\\n🔧 실제 삭제 실행할 컬럼들 ({len(existing_drops)}개):\")\n",
    "    for i, col in enumerate(existing_drops, 1):\n",
    "        print(f\"   {i:2d}. {col}\")\n",
    "    \n",
    "    # 삭제 실행\n",
    "    if existing_drops:\n",
    "        ps_df_cleaned = ps_df.drop(*existing_drops)\n",
    "        \n",
    "        print(f\"\\n✅ 모든 다중공선성 해결 완료!\")\n",
    "        print(f\"   삭제 전: {len(ps_df.columns)} 컬럼\")\n",
    "        print(f\"   삭제 후: {len(ps_df_cleaned.columns)} 컬럼\")\n",
    "        print(f\"   실제 삭제: {len(existing_drops)} 컬럼\")\n",
    "        print(f\"   삭제 비율: {len(existing_drops)/len(ps_df.columns)*100:.1f}%\")\n",
    "        \n",
    "        # 해결된 다중공선성 쌍들 정리\n",
    "        print(f\"\\n📋 해결된 모든 다중공선성 쌍들 (34개):\")\n",
    "        resolved_pairs = [\n",
    "            # 기존 20개\n",
    "            \"이용건수_부분무이자_R12M ↔ 할부건수_부분_12M_R12M (1.000)\",\n",
    "            \"이용건수_신판_B0M ↔ 이용건수_일시불_B0M (0.999)\", \n",
    "            \"이용건수_신용_B0M ↔ 이용건수_신판_B0M (0.999)\",\n",
    "            \"이용건수_신용_B0M ↔ 이용건수_일시불_B0M (0.998)\",\n",
    "            \"승인거절건수_B0M ↔ 승인거절건수_한도초과_B0M (0.991)\",\n",
    "            \"이용건수_할부_B0M ↔ 이용건수_할부_무이자_B0M (0.944)\",\n",
    "            \"이용건수_신판_R3M ↔ 이용건수_일시불_R3M (0.999)\",\n",
    "            \"이용건수_신용_R3M ↔ 이용건수_신판_R3M (0.998)\",\n",
    "            \"이용건수_신용_R3M ↔ 이용건수_일시불_R3M (0.997)\",\n",
    "            \"이용건수_할부_R3M ↔ 이용건수_할부_무이자_R3M (0.951)\",\n",
    "            \"이용건수_페이_온라인_R3M ↔ 이용건수_간편결제_R3M (0.918)\",\n",
    "            \"이용건수_신용_R6M ↔ 이용건수_신판_R6M (0.998)\",\n",
    "            \"이용건수_신판_R6M ↔ 이용건수_일시불_R6M (0.997)\",\n",
    "            \"이용건수_신용_R6M ↔ 이용건수_일시불_R6M (0.995)\",\n",
    "            \"이용건수_할부_R6M ↔ 이용건수_할부_무이자_R6M (0.962)\",\n",
    "            \"이용건수_페이_온라인_R6M ↔ 이용건수_간편결제_R6M (0.950)\",\n",
    "            \"이용건수_신용_R12M ↔ 이용건수_신판_R12M (0.997)\",\n",
    "            \"이용건수_신판_R12M ↔ 이용건수_일시불_R12M (0.993)\",\n",
    "            \"이용건수_신용_R12M ↔ 이용건수_일시불_R12M (0.988)\",\n",
    "            \"할부건수_14M_R12M ↔ 할부건수_유이자_14M_R12M (0.987)\",\n",
    "            \n",
    "            # 추가 14개\n",
    "            \"이용건수_할부_R12M ↔ 할부건수_3M_R12M (0.968)\",\n",
    "            \"이용건수_할부_무이자_R12M ↔ 할부건수_무이자_3M_R12M (0.966)\",\n",
    "            \"이용건수_할부_R12M ↔ 이용건수_할부_무이자_R12M (0.959)\",\n",
    "            \"할부건수_3M_R12M ↔ 할부건수_무이자_3M_R12M (0.956)\",\n",
    "            \"할부건수_6M_R12M ↔ 할부건수_무이자_6M_R12M (0.954)\",\n",
    "            \"이용건수_신용_B0M ↔ 이용건수_오프라인_B0M (0.948)\",\n",
    "            \"이용건수_할부_유이자_R12M ↔ 할부건수_유이자_3M_R12M (0.944)\",\n",
    "            \"이용건수_신용_R3M ↔ 이용건수_오프라인_R3M (0.932)\",\n",
    "            \"이용건수_페이_오프라인_B0M ↔ 이용건수_A페이_B0M (0.931)\",\n",
    "            \"이용건수_할부_무이자_R12M ↔ 할부건수_3M_R12M (0.928)\",\n",
    "            \"이용건수_할부_R12M ↔ 할부건수_무이자_3M_R12M (0.927)\",\n",
    "            \"이용건수_신용_R6M ↔ 이용건수_오프라인_R6M (0.916)\",\n",
    "            \"RP건수_B0M ↔ RP유형건수_B0M (0.916)\",\n",
    "            \"이용건수_페이_오프라인_R6M ↔ 이용건수_A페이_R6M (0.911)\"\n",
    "        ]\n",
    "        \n",
    "        for i, pair in enumerate(resolved_pairs, 1):\n",
    "            print(f\"   {i:2d}. ✅ {pair}\")\n",
    "        \n",
    "        return ps_df_cleaned, existing_drops\n",
    "    else:\n",
    "        print(\"\\n❌ 삭제할 컬럼이 없습니다.\")\n",
    "        return ps_df, []\n",
    "\n",
    "# 실행\n",
    "print(\"🚀 COUNT_DF 모든 다중공선성 완전 해결 시작 (누락분 포함)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "ps_df_cleaned, deleted_cols = solve_count_df_complete_multicollinearity_final(ps_df)\n",
    "\n",
    "print(f\"\\n🎯 모든 다중공선성 완전 해결!\")\n",
    "print(f\"   총 {len(deleted_cols)}개 컬럼 삭제\")\n",
    "print(f\"   34개 다중공선성 쌍 모두 해결\")\n",
    "print(f\"   다중공선성 문제 100% 해결 완료!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "20696592-0978-4d84-8904-626e11e6b418",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pyspark.sql.functions import col\n",
    "import time\n",
    "import re\n",
    "\n",
    "print(\"=== 기간별 컬럼 분류 후 GPU 상관관계 분석 ===\")\n",
    "\n",
    "# 1. GPU 메모리 최적화 설정\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"사용 디바이스: {device}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    gpu_props = torch.cuda.get_device_properties(0)\n",
    "    total_memory = gpu_props.total_memory / 1024**3\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU 총 메모리: {total_memory:.1f} GB\")\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "# 2. 데이터 로드\n",
    "ps_df = ps_df_cleaned\n",
    "print(f\"전체 데이터 수: {ps_df.count():,}\")\n",
    "\n",
    "# 3. 기간별 컬럼 분류 함수 (개선된 버전)\n",
    "def classify_columns_by_period(df_columns):\n",
    "    \"\"\"\n",
    "    컬럼명을 기간별로 분류하는 함수\n",
    "    \"\"\"\n",
    "    period_groups = {\n",
    "        'B0M': [],\n",
    "        'R3M': [],\n",
    "        'R6M': [],\n",
    "        'R12M': []\n",
    "    }\n",
    "    \n",
    "    # 기간 패턴 정의 (더 포괄적으로)\n",
    "    period_patterns = {\n",
    "        'B0M': re.compile(r'.*_B0M$'),\n",
    "        'R3M': re.compile(r'.*_R3M$'),\n",
    "        'R6M': re.compile(r'.*_R6M$'),\n",
    "        'R12M': re.compile(r'.*_R12M$')\n",
    "    }\n",
    "    \n",
    "    non_period_cols = []  # 기간이 없는 컬럼들\n",
    "    \n",
    "    for col_name in df_columns:\n",
    "        # 제외할 컬럼들\n",
    "        if col_name in ['기준년월', '발급회원번호']:\n",
    "            continue\n",
    "            \n",
    "        # 각 기간 패턴에 매칭되는지 확인\n",
    "        matched = False\n",
    "        for period, pattern in period_patterns.items():\n",
    "            if pattern.match(col_name):\n",
    "                period_groups[period].append(col_name)\n",
    "                matched = True\n",
    "                break\n",
    "        \n",
    "        if not matched:\n",
    "            non_period_cols.append(col_name)\n",
    "    \n",
    "    period_groups['non_period'] = non_period_cols\n",
    "    return period_groups\n",
    "\n",
    "# 4. 컬럼 분류 실행\n",
    "column_groups = classify_columns_by_period(ps_df.columns)\n",
    "\n",
    "print(\"=== 기간별 컬럼 분류 결과 ===\")\n",
    "for period, cols in column_groups.items():\n",
    "    print(f\"{period}: {len(cols)}개 컬럼\")\n",
    "    if len(cols) > 0:\n",
    "        print(f\"  예시: {cols[:3]}...\")\n",
    "print()\n",
    "\n",
    "# 5. GPU 상관관계 분석 함수 (기간별 적용)\n",
    "def compute_period_correlation_gpu(df, period_columns, period_name, device):\n",
    "    \"\"\"\n",
    "    특정 기간의 컬럼들에 대해 GPU 상관관계 계산\n",
    "    \"\"\"\n",
    "    if len(period_columns) == 0:\n",
    "        print(f\"⚠️ {period_name}: 분석할 컬럼이 없습니다.\")\n",
    "        return None, []\n",
    "    \n",
    "    print(f\"\\n=== {period_name} 기간 GPU 상관관계 분석 ===\")\n",
    "    print(f\"분석 컬럼 수: {len(period_columns)}\")\n",
    "    \n",
    "    # 수치형 컬럼만 필터링\n",
    "    numeric_period_cols = []\n",
    "    for col_name in period_columns:\n",
    "        col_type = dict(df.dtypes)[col_name]\n",
    "        if col_type in ['int', 'bigint', 'float', 'double']:\n",
    "            numeric_period_cols.append(col_name)\n",
    "    \n",
    "    if len(numeric_period_cols) == 0:\n",
    "        print(f\"⚠️ {period_name}: 수치형 컬럼이 없습니다.\")\n",
    "        return None, []\n",
    "    \n",
    "    print(f\"수치형 컬럼 수: {len(numeric_period_cols)}\")\n",
    "    \n",
    "    # 메모리 효율적인 청크 크기 계산\n",
    "    n_features = len(numeric_period_cols)\n",
    "    if torch.cuda.is_available():\n",
    "        # 피처 수에 따른 동적 청크 크기 조정\n",
    "        if n_features <= 10:\n",
    "            chunk_size = 5000000  # 피처가 적으면 더 큰 청크\n",
    "        elif n_features <= 20:\n",
    "            chunk_size = 3000000\n",
    "        elif n_features <= 50:\n",
    "            chunk_size = 2000000\n",
    "        else:\n",
    "            chunk_size = 1000000  # 피처가 많으면 작은 청크\n",
    "    else:\n",
    "        chunk_size = 500000\n",
    "    \n",
    "    print(f\"청크 크기: {chunk_size:,}\")\n",
    "    \n",
    "    # 전체 데이터 처리\n",
    "    total_rows = df.count()\n",
    "    n_chunks = max(1, (total_rows + chunk_size - 1) // chunk_size)\n",
    "    print(f\"총 {n_chunks}개 청크로 분할\")\n",
    "    \n",
    "    correlation_sum = None\n",
    "    total_weight = 0\n",
    "    \n",
    "    for chunk_idx in range(n_chunks):\n",
    "        chunk_start_time = time.time()\n",
    "        print(f\"\\n  청크 {chunk_idx + 1}/{n_chunks} 처리 중...\")\n",
    "        \n",
    "        # 청크 데이터 추출\n",
    "        if n_chunks == 1:\n",
    "            chunk_df = df\n",
    "        else:\n",
    "            chunk_fraction = 1.0 / n_chunks\n",
    "            chunk_df = df.sample(fraction=chunk_fraction, seed=42 + chunk_idx)\n",
    "        \n",
    "        # Pandas로 변환\n",
    "        chunk_pdf = chunk_df.select(*numeric_period_cols).fillna(0).toPandas()\n",
    "        actual_chunk_size = len(chunk_pdf)\n",
    "        \n",
    "        if actual_chunk_size == 0:\n",
    "            continue\n",
    "        \n",
    "        print(f\"    실제 청크 크기: {actual_chunk_size:,} x {len(chunk_pdf.columns)}\")\n",
    "        \n",
    "        try:\n",
    "            # GPU 텐서로 변환\n",
    "            chunk_tensor = torch.tensor(chunk_pdf.values, dtype=torch.float32).to(device)\n",
    "            \n",
    "            if torch.cuda.is_available():\n",
    "                memory_used = torch.cuda.memory_allocated() / 1024**3\n",
    "                print(f\"    GPU 메모리 사용량: {memory_used:.2f} GB\")\n",
    "            \n",
    "            # 상관관계 계산\n",
    "            chunk_correlation = torch.corrcoef(chunk_tensor.T)\n",
    "            chunk_corr_cpu = chunk_correlation.cpu().numpy()\n",
    "            \n",
    "            # 가중 평균으로 누적\n",
    "            weight = actual_chunk_size\n",
    "            if correlation_sum is None:\n",
    "                correlation_sum = chunk_corr_cpu * weight\n",
    "            else:\n",
    "                correlation_sum += chunk_corr_cpu * weight\n",
    "            total_weight += weight\n",
    "            \n",
    "            # 메모리 정리\n",
    "            del chunk_tensor, chunk_correlation, chunk_pdf\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "            \n",
    "            chunk_time = time.time() - chunk_start_time\n",
    "            print(f\"    청크 처리 시간: {chunk_time:.2f}초\")\n",
    "            \n",
    "        except RuntimeError as e:\n",
    "            if \"out of memory\" in str(e):\n",
    "                print(f\"    ⚠️ GPU 메모리 부족, 청크 크기 조정 필요\")\n",
    "                return None, numeric_period_cols\n",
    "            else:\n",
    "                raise e\n",
    "    \n",
    "    # 최종 상관관계 매트릭스 계산\n",
    "    if total_weight > 0:\n",
    "        final_correlation = correlation_sum / total_weight\n",
    "        return final_correlation, numeric_period_cols\n",
    "    else:\n",
    "        return None, numeric_period_cols\n",
    "\n",
    "# 6. 상관관계 분석 함수\n",
    "def analyze_correlations(corr_matrix, features, threshold=0.7):\n",
    "    \"\"\"높은 상관관계 분석\"\"\"\n",
    "    high_corr = []\n",
    "    n = len(features)\n",
    "    for i in range(n):\n",
    "        for j in range(i+1, n):\n",
    "            corr_val = corr_matrix[i, j]\n",
    "            if abs(corr_val) > threshold:\n",
    "                high_corr.append({\n",
    "                    'feature1': features[i],\n",
    "                    'feature2': features[j],\n",
    "                    'correlation': float(corr_val)\n",
    "                })\n",
    "    return high_corr\n",
    "\n",
    "# 7. 각 기간별로 상관관계 분석 실행\n",
    "period_results = {}\n",
    "analysis_start_time = time.time()\n",
    "\n",
    "for period_name, period_cols in column_groups.items():\n",
    "    if period_name == 'non_period':\n",
    "        continue  # 기간이 없는 컬럼은 별도 처리\n",
    "    \n",
    "    if len(period_cols) == 0:\n",
    "        continue\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"🔍 {period_name} 기간 분석 시작\")\n",
    "    \n",
    "    period_start_time = time.time()\n",
    "    \n",
    "    # GPU 상관관계 분석\n",
    "    correlation_matrix, feature_names = compute_period_correlation_gpu(\n",
    "        ps_df, period_cols, period_name, device\n",
    "    )\n",
    "    \n",
    "    if correlation_matrix is not None:\n",
    "        period_time = time.time() - period_start_time\n",
    "        \n",
    "        print(f\"\\n=== {period_name} 분석 결과 ===\")\n",
    "        print(f\"처리 시간: {period_time:.2f}초\")\n",
    "        print(f\"분석된 피처: {len(feature_names)}개\")\n",
    "        print(f\"상관관계 매트릭스 크기: {correlation_matrix.shape}\")\n",
    "        \n",
    "        # 높은 상관관계 분석\n",
    "        high_correlations = analyze_correlations(correlation_matrix, feature_names, 0.7)\n",
    "        print(f\"높은 상관관계 (|r| > 0.7): {len(high_correlations)}개\")\n",
    "        \n",
    "        # 상위 10개 출력\n",
    "        for pair in sorted(high_correlations, key=lambda x: abs(x['correlation']), reverse=True)[:100]:\n",
    "            print(f\"  {pair['feature1']} ↔ {pair['feature2']}: {pair['correlation']:.3f}\")\n",
    "        \n",
    "        # 다중공선성 위험\n",
    "        multicollinear = analyze_correlations(correlation_matrix, feature_names, 0.9)\n",
    "        print(f\"다중공선성 위험 (|r| > 0.9): {len(multicollinear)}개\")\n",
    "        \n",
    "        for pair in sorted(multicollinear, key=lambda x: abs(x['correlation']), reverse=True)[:50]:\n",
    "            print(f\"  ⚠️ {pair['feature1']} ↔ {pair['feature2']}: {pair['correlation']:.3f}\")\n",
    "        \n",
    "        # 결과 저장\n",
    "        period_results[period_name] = {\n",
    "            'correlation_matrix': correlation_matrix,\n",
    "            'feature_names': feature_names,\n",
    "            'high_correlations': high_correlations,\n",
    "            'multicollinear': multicollinear,\n",
    "            'processing_time': period_time\n",
    "        }\n",
    "    \n",
    "    else:\n",
    "        print(f\"❌ {period_name} 분석 실패\")\n",
    "\n",
    "# 8. 전체 결과 요약\n",
    "total_analysis_time = time.time() - analysis_start_time\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"🎯 전체 기간별 상관관계 분석 완료!\")\n",
    "print(f\"총 처리 시간: {total_analysis_time:.2f}초 ({total_analysis_time/60:.1f}분)\")\n",
    "\n",
    "print(f\"\\n=== 기간별 분석 요약 ===\")\n",
    "for period_name, results in period_results.items():\n",
    "    print(f\"{period_name}:\")\n",
    "    print(f\"  - 피처 수: {len(results['feature_names'])}\")\n",
    "    print(f\"  - 높은 상관관계: {len(results['high_correlations'])}개\")\n",
    "    print(f\"  - 다중공선성 위험: {len(results['multicollinear'])}개\")\n",
    "    print(f\"  - 처리 시간: {results['processing_time']:.2f}초\")\n",
    "\n",
    "# 9. 기간 간 비교 분석 (옵션)\n",
    "print(f\"\\n=== 기간 간 상관관계 패턴 비교 ===\")\n",
    "for period_name, results in period_results.items():\n",
    "    if len(results['high_correlations']) > 0:\n",
    "        print(f\"\\n{period_name} 주요 상관관계:\")\n",
    "        # 가장 높은 상관관계 3개\n",
    "        top_corrs = sorted(results['high_correlations'], \n",
    "                          key=lambda x: abs(x['correlation']), reverse=True)[:3]\n",
    "        for i, pair in enumerate(top_corrs, 1):\n",
    "            print(f\"  {i}. {pair['feature1']} ↔ {pair['feature2']}: {pair['correlation']:.3f}\")\n",
    "\n",
    "# 10. 메모리 정리\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    final_memory = torch.cuda.memory_allocated() / 1024**3\n",
    "    peak_memory = torch.cuda.max_memory_allocated() / 1024**3\n",
    "    print(f\"\\n🧹 최종 GPU 메모리 사용량: {final_memory:.2f} GB\")\n",
    "    print(f\"📈 최대 GPU 메모리 사용량: {peak_memory:.2f} GB\")\n",
    "\n",
    "print(\"\\n✅ 기간별 GPU 상관관계 분석 완료!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b180891e-ed22-4c08-9267-9fc5c119b727",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "### 데이터 베이스 사용 설정\n",
    "spark.sql(\"USE database_03_cache\")\n",
    "print(\"현재 데이터베이스를 'database_03_cache'로 설정\")\n",
    "\n",
    "ps_df_cleaned.write.mode(\"overwrite\").saveAsTable(\"count_period_df_proc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1bb58a07-93fe-45cd-8060-7ffdef65025b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c42f9617-05cd-4d52-9d6f-b7b617b893d2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### 2.2 기간 불포함 컬럼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6de253ae-b660-45a4-a9cd-d15b6544399f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(len(non_matching_cols))\n",
    "notperiod_df = ps_df[non_matching_cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f242209e-da8d-4506-957e-552b793c8984",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### 💡 데이터 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bf6c4e11-91ae-4ed7-a787-9a3adc808f1a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "### 데이터 베이스 사용 설정\n",
    "ps_notperiod_df = notperiod_df.cache()\n",
    "spark.sql(\"USE database_03_cache\")\n",
    "print(\"현재 데이터베이스를 'database_03_cache'로 설정\")\n",
    "\n",
    "### 저장할 테이블 값 입력\n",
    "notperiod_df.write.mode(\"overwrite\").saveAsTable(\"count_notperiod_df\")\n",
    "print(\"이용금액(기간 불포함) 관련 테이블 생성 완료\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "be071056-9bff-412c-9e9b-9bc73d12e103",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### 💡다시 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "adb6ff99-9afe-4845-bd6c-b1fb8727b8a4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "### 저장한 테이블 값 입력\n",
    "ps_notperiod_df = spark.read.table(\"database_03_cache.count_notperiod_df\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1a0af7da-f595-43f1-8f63-635e4bf4e82c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "ps_notperiod_columns = ps_notperiod_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "25558888-105c-4501-8027-ad35491ea91b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Databricks data profile. Run in Databricks to view."
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1.subcommand+json": {
       "baseErrorDetails": null,
       "bindings": {},
       "collapsed": false,
       "command": "%python\nif hasattr(dbutils, \"data\") and hasattr(dbutils.data, \"summarize\"):\n  # setup\n  __data_summary_display_orig = display\n  __data_summary_dfs = []\n  def __data_summary_display_new(df):\n    # add only when result is going to be table type\n    __data_summary_df_modules = [\"pandas.core.frame\", \"databricks.koalas.frame\", \"pyspark.sql.dataframe\", \"pyspark.pandas.frame\", \"pyspark.sql.connect.dataframe\", \"pyspark.sql.classic.dataframe\"]\n    if (type(df).__module__ in __data_summary_df_modules and type(df).__name__ == 'DataFrame') or isinstance(df, list):\n      __data_summary_dfs.append(df)\n  display = __data_summary_display_new\n\n  def __data_summary_user_code_fn():\n    import base64\n    exec(base64.standard_b64decode(\"ZGlzcGxheShwc19ub3RwZXJpb2RfZGYp\").decode())\n\n  try:\n    # run user code\n    __data_summary_user_code_fn()\n\n    # run on valid tableResultIndex\n    if len(__data_summary_dfs) > 0:\n      # run summarize\n      if type(__data_summary_dfs[0]).__module__ == \"databricks.koalas.frame\":\n        # koalas dataframe\n        dbutils.data.summarize(__data_summary_dfs[0].to_spark())\n      elif type(__data_summary_dfs[0]).__module__ == \"pandas.core.frame\":\n        # pandas dataframe\n        dbutils.data.summarize(spark.createDataFrame(__data_summary_dfs[0]))\n      else:\n        dbutils.data.summarize(__data_summary_dfs[0])\n    else:\n        displayHTML(\"dataframe no longer exists. If you're using dataframe.display(), use display(dataframe) instead.\")\n\n  finally:\n    display = __data_summary_display_orig\n    del __data_summary_display_new\n    del __data_summary_display_orig\n    del __data_summary_dfs\n    del __data_summary_user_code_fn\nelse:\n  print(\"This DBR version does not support data profiles.\")",
       "commandTitle": "데이터 프로필 1",
       "commandType": "auto",
       "commandVersion": 0,
       "commentThread": [],
       "commentsVisible": false,
       "contentSha256Hex": null,
       "customPlotOptions": {},
       "datasetPreviewNameToCmdIdMap": {},
       "diffDeletes": [],
       "diffInserts": [],
       "displayType": "table",
       "error": null,
       "errorDetails": null,
       "errorSummary": null,
       "errorTraceType": null,
       "finishTime": 0,
       "globalVars": {},
       "guid": "",
       "height": "auto",
       "hideCommandCode": false,
       "hideCommandResult": false,
       "iPythonMetadata": null,
       "inputWidgets": {},
       "isLockedInExamMode": false,
       "latestUser": "a user",
       "latestUserId": null,
       "listResultMetadata": null,
       "metadata": {
        "byteLimit": 2048000,
        "rowLimit": 10000
       },
       "nuid": "b7b7cdab-0f16-47ed-a6b6-863c9b2310ec",
       "origId": 0,
       "parentHierarchy": [],
       "pivotAggregation": null,
       "pivotColumns": null,
       "position": 8.483455882352935,
       "resultDbfsErrorMessage": null,
       "resultDbfsStatus": "INLINED_IN_TREE",
       "results": null,
       "showCommandTitle": false,
       "startTime": 0,
       "state": "input",
       "streamStates": {},
       "subcommandOptions": null,
       "submitTime": 0,
       "subtype": "tableResultSubCmd.dataSummary",
       "tableResultIndex": 0,
       "tableResultSettingsMap": {},
       "useConsistentColors": false,
       "version": "CommandV1",
       "width": "auto",
       "workflows": null,
       "xColumns": null,
       "yColumns": null
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(ps_notperiod_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "611b5a69-04d0-4aea-9a9a-f1d1dda50f1e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "numeric_cols = [c for c in ps_notperiod_columns if c not in ['기준년월', '발급회원번호']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e044a1ae-6ad5-4bf6-9e50-e89cded05e4c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### 이상치 처리/스케일링 - 로그 변환\n",
    "테이블 분석에서 box plot 확인 결과 대부분 positive skew로 이상치 많은 분포임. 따라서 로그 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fef6687a-8a83-4d74-a4a2-d9276a40263f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import log1p, col\n",
    "\n",
    "# 로그 변환\n",
    "for col_name in numeric_cols:\n",
    "    ps_notperiod_df = ps_notperiod_df.withColumn(col_name, log1p(col(col_name)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "17f65037-a89f-45fa-b583-0d7a6f32b4b4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Databricks data profile. Run in Databricks to view."
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1.subcommand+json": {
       "baseErrorDetails": null,
       "bindings": {},
       "collapsed": false,
       "command": "%python\nif hasattr(dbutils, \"data\") and hasattr(dbutils.data, \"summarize\"):\n  # setup\n  __data_summary_display_orig = display\n  __data_summary_dfs = []\n  def __data_summary_display_new(df):\n    # add only when result is going to be table type\n    __data_summary_df_modules = [\"pandas.core.frame\", \"databricks.koalas.frame\", \"pyspark.sql.dataframe\", \"pyspark.pandas.frame\", \"pyspark.sql.connect.dataframe\", \"pyspark.sql.classic.dataframe\"]\n    if (type(df).__module__ in __data_summary_df_modules and type(df).__name__ == 'DataFrame') or isinstance(df, list):\n      __data_summary_dfs.append(df)\n  display = __data_summary_display_new\n\n  def __data_summary_user_code_fn():\n    import base64\n    exec(base64.standard_b64decode(\"ZGlzcGxheShwc19ub3RwZXJpb2RfZGYp\").decode())\n\n  try:\n    # run user code\n    __data_summary_user_code_fn()\n\n    # run on valid tableResultIndex\n    if len(__data_summary_dfs) > 0:\n      # run summarize\n      if type(__data_summary_dfs[0]).__module__ == \"databricks.koalas.frame\":\n        # koalas dataframe\n        dbutils.data.summarize(__data_summary_dfs[0].to_spark())\n      elif type(__data_summary_dfs[0]).__module__ == \"pandas.core.frame\":\n        # pandas dataframe\n        dbutils.data.summarize(spark.createDataFrame(__data_summary_dfs[0]))\n      else:\n        dbutils.data.summarize(__data_summary_dfs[0])\n    else:\n        displayHTML(\"dataframe no longer exists. If you're using dataframe.display(), use display(dataframe) instead.\")\n\n  finally:\n    display = __data_summary_display_orig\n    del __data_summary_display_new\n    del __data_summary_display_orig\n    del __data_summary_dfs\n    del __data_summary_user_code_fn\nelse:\n  print(\"This DBR version does not support data profiles.\")",
       "commandTitle": "데이터 프로필 1",
       "commandType": "auto",
       "commandVersion": 0,
       "commentThread": [],
       "commentsVisible": false,
       "contentSha256Hex": null,
       "customPlotOptions": {},
       "datasetPreviewNameToCmdIdMap": {},
       "diffDeletes": [],
       "diffInserts": [],
       "displayType": "table",
       "error": null,
       "errorDetails": null,
       "errorSummary": null,
       "errorTraceType": null,
       "finishTime": 0,
       "globalVars": {},
       "guid": "",
       "height": "auto",
       "hideCommandCode": false,
       "hideCommandResult": false,
       "iPythonMetadata": null,
       "inputWidgets": {},
       "isLockedInExamMode": false,
       "latestUser": "a user",
       "latestUserId": null,
       "listResultMetadata": null,
       "metadata": {
        "byteLimit": 2048000,
        "rowLimit": 10000
       },
       "nuid": "6abccc6f-e2ca-4cd7-a214-5b377c0fb3ba",
       "origId": 0,
       "parentHierarchy": [],
       "pivotAggregation": null,
       "pivotColumns": null,
       "position": 8.555721507352935,
       "resultDbfsErrorMessage": null,
       "resultDbfsStatus": "INLINED_IN_TREE",
       "results": null,
       "showCommandTitle": false,
       "startTime": 0,
       "state": "input",
       "streamStates": {},
       "subcommandOptions": null,
       "submitTime": 0,
       "subtype": "tableResultSubCmd.dataSummary",
       "tableResultIndex": 0,
       "tableResultSettingsMap": {},
       "useConsistentColors": false,
       "version": "CommandV1",
       "width": "auto",
       "workflows": null,
       "xColumns": null,
       "yColumns": null
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(ps_notperiod_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cad78702-a1c7-4852-ba88-11ca7e0f3f37",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### 상관관계 분석 (fin)\n",
    "pyspark.ml.stat.Correlation은 **벡터 열**(아래 코드에서 features변수)에에서만 작동하므로<br>\n",
    "→ 반드시 VectorAssembler 사용해야 함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "615dedce-ecb2-4925-b85c-baa6efc0161e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**상관관계 유형 설멍**\n",
    "\n",
    "| 구분    | 피어슨 (Pearson)          | 스피어만 (Spearman)                        |\n",
    "| ----- | ---------------------- | -------------------------------------- |\n",
    "| 정의    | 변수 간의 **선형 관계** 측정     | 변수 간의 **순위 기반(모노톤) 관계** 측정             |\n",
    "| 전제 조건 | 연속형 변수 + 정규분포 근처       | 순위로 바꿔도 의미 있는 데이터                      |\n",
    "| 민감도   | 이상치에 민감                | 이상치에 강건                                |\n",
    "| 사용 예  | 소비금액처럼 **정량적인 값 간 관계** | **비선형적이지만 단조적인 관계** (ex. 만족도 등급 vs 소비 등급) |\n",
    "\n",
    "✅ 우리 분석 목적엔?\n",
    "- \"소비 금액의 절대 크기\"를 분석하고 싶다면 → 피어슨 (소비 크기에 따른 상품 추천)\n",
    "-  \"어디에 더 많이 쓰는지 성향\"을 보고 싶다면 → 스피어만(소비 성향 기반 클러스터링)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "99fcc81e-627e-4509-9a65-e7c9d6fa1f88",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "from pyspark.ml.stat import Correlation\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# 통계적으로 유의한 샘플 크기 계산\n",
    "def calculate_sample_size(population_size, confidence_level=0.95, margin_error=0.05):\n",
    "    \"\"\"\n",
    "    통계적으로 유의한 샘플 크기 계산\n",
    "    \"\"\"\n",
    "    z_score = 2.576  # 99% 신뢰도\n",
    "    p = 0.5  # 최대 분산\n",
    "    \n",
    "    n = (z_score**2 * p * (1-p)) / (margin_error**2)\n",
    "    n_adjusted = n / (1 + (n-1)/population_size)\n",
    "    \n",
    "    return int(n_adjusted)\n",
    "\n",
    "# 데이터 로드\n",
    "ps_df = ps_notperiod_df\n",
    "total_count = ps_df.count()\n",
    "\n",
    "# 통계적 샘플 크기 계산\n",
    "sample_size = calculate_sample_size(total_count)\n",
    "sample_fraction = sample_size / total_count\n",
    "\n",
    "print(f\"전체 데이터: {total_count:,}\")\n",
    "print(f\"필요 샘플 크기: {sample_size:,}\")\n",
    "print(f\"샘플링 비율: {sample_fraction:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5fabccdf-0f63-4b0c-bfd6-b424b247ca75",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 2. 층화 샘플링 (기준년월별로 균등하게)\n",
    "def stratified_sampling(df, strata_col=\"기준년월\", sample_fraction=0.01):\n",
    "    \"\"\"\n",
    "    층화 샘플링으로 대표성 있는 샘플 생성\n",
    "    \"\"\"\n",
    "    # 각 층(기준년월)별 샘플링\n",
    "    strata_samples = []\n",
    "    \n",
    "    for month in df.select(strata_col).distinct().collect():\n",
    "        month_value = month[strata_col]\n",
    "        month_df = df.filter(col(strata_col) == month_value)\n",
    "        month_sample = month_df.sample(fraction=sample_fraction, seed=42)\n",
    "        strata_samples.append(month_sample)\n",
    "    \n",
    "    # 모든 층 합치기\n",
    "    final_sample = strata_samples[0]\n",
    "    for sample in strata_samples[1:]:\n",
    "        final_sample = final_sample.union(sample)\n",
    "    \n",
    "    return final_sample\n",
    "\n",
    "# 층화 샘플링 실행\n",
    "print(\"=== 층화 샘플링 실행 ===\")\n",
    "sampled_df = stratified_sampling(ps_df, sample_fraction=0.005)  # 0.5%\n",
    "sampled_count = sampled_df.count()\n",
    "print(f\"샘플 데이터: {sampled_count:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d8cc4a3a-a139-4893-876f-445da5e1f16e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 3. 빠른 상관관계 분석 (피쳐 수 제한 없음)\n",
    "def fast_correlation_analysis(df):\n",
    "\n",
    "    # 수치형 컬럼 선택\n",
    "    numeric_cols = [col_name for col_name, data_type in df.dtypes\n",
    "                    if data_type in ['int', 'bigint', 'float', 'double']]\n",
    "    \n",
    "    # 키 컬럼 제외\n",
    "    exclude_cols = ['기준년월', '발급회원번호']\n",
    "    analysis_cols = [col for col in numeric_cols if col not in exclude_cols]\n",
    "    \n",
    "    print(f\"분석할 피처 수: {len(analysis_cols)}\")\n",
    "    \n",
    "    # null 처리\n",
    "    df_filled = df.fillna(0, subset=analysis_cols)\n",
    "    \n",
    "    # 벡터화\n",
    "    assembler = VectorAssembler(inputCols=analysis_cols, outputCol=\"features\")\n",
    "    vector_df = assembler.transform(df_filled).select(\"features\")\n",
    "    \n",
    "    # 캐싱\n",
    "    vector_df.cache()\n",
    "    vector_df.count()\n",
    "    \n",
    "    # 상관관계 계산\n",
    "    print(\"상관관계 계산 중...\")\n",
    "    correlation_matrix = Correlation.corr(vector_df, \"features\", method=\"pearson\").head()[0]\n",
    "    \n",
    "    return correlation_matrix, analysis_cols\n",
    "\n",
    "# 빠른 분석 실행 (모든 피처 사용)\n",
    "correlation_matrix, feature_names = fast_correlation_analysis(sampled_df)\n",
    "print(\"상관관계 계산 완료!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "af8b18e1-11f0-442b-bdef-7372fe6d9210",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 4. 결과 분석 및 시각화\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# 상관관계 매트릭스를 numpy 배열로 변환\n",
    "corr_array = correlation_matrix.toArray()\n",
    "\n",
    "# 높은 상관관계 찾기\n",
    "high_correlations = []\n",
    "for i in range(len(feature_names)):\n",
    "    for j in range(i+1, len(feature_names)):\n",
    "        corr_value = corr_array[i][j]\n",
    "        if abs(corr_value) > 0.7:  # 0.7 이상\n",
    "            high_correlations.append({\n",
    "                'feature1': feature_names[i],\n",
    "                'feature2': feature_names[j],\n",
    "                'correlation': corr_value\n",
    "            })\n",
    "\n",
    "# 결과 출력\n",
    "print(f\"\\n=== 높은 상관관계 ({len(high_correlations)}개) ===\")\n",
    "high_correlations_sorted = sorted(high_correlations, \n",
    "                                 key=lambda x: abs(x['correlation']), \n",
    "                                 reverse=True)\n",
    "\n",
    "for corr in high_correlations_sorted[:10]:\n",
    "    print(f\"{corr['feature1']} ↔ {corr['feature2']}: {corr['correlation']:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f45c09e7-0366-46a0-b68a-ddf6439db394",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 5.히트맵 생성\n",
    "try:\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    \n",
    "    # 상관관계 매트릭스를 DataFrame으로 변환\n",
    "    corr_df = pd.DataFrame(corr_array, \n",
    "                          index=feature_names, \n",
    "                          columns=feature_names)\n",
    "    \n",
    "    print(f\"DataFrame 크기: {corr_df.shape}\")\n",
    "    print(f\"DataFrame 인덱스 수: {len(corr_df.index)}\")\n",
    "    print(f\"DataFrame 컬럼 수: {len(corr_df.columns)}\")\n",
    "    \n",
    "    # 큰 히트맵을 위한 설정\n",
    "    plt.figure(figsize=(20, 18))  # 크기 증가\n",
    "    \n",
    "    # 히트맵 생성 (라벨 크기 조정)\n",
    "    sns.heatmap(corr_df, \n",
    "                annot=False,  # 숫자 표시 끄기 (너무 많아서)\n",
    "                cmap='coolwarm', \n",
    "                center=0,\n",
    "                square=True, \n",
    "                fmt='.2f',\n",
    "                xticklabels=True,  # x축 라벨 표시\n",
    "                yticklabels=True,  # y축 라벨 표시\n",
    "                cbar_kws={'shrink': 0.8})\n",
    "    \n",
    "    # 라벨 크기 조정\n",
    "    plt.xticks(rotation=45, ha='right', fontsize=8)\n",
    "    plt.yticks(rotation=0, fontsize=8)\n",
    "    plt.title('Feature Correlation Heatmap (All Features)', fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # 상관관계가 높은 피처들만 별도 히트맵\n",
    "    print(\"\\n=== 높은 상관관계 피처들만 히트맵 ===\")\n",
    "    \n",
    "    # 높은 상관관계를 가진 피처들 찾기\n",
    "    high_corr_features = set()\n",
    "    threshold = 0.7\n",
    "    \n",
    "    for i in range(len(feature_names)):\n",
    "        for j in range(i+1, len(feature_names)):\n",
    "            if abs(corr_array[i][j]) > threshold:\n",
    "                high_corr_features.add(feature_names[i])\n",
    "                high_corr_features.add(feature_names[j])\n",
    "    \n",
    "    if high_corr_features:\n",
    "        high_corr_features = list(high_corr_features)\n",
    "        print(f\"높은 상관관계 피처 수: {len(high_corr_features)}\")\n",
    "        \n",
    "        # 서브셋 히트맵\n",
    "        corr_subset = corr_df.loc[high_corr_features, high_corr_features]\n",
    "        \n",
    "        plt.figure(figsize=(12, 10))\n",
    "        sns.heatmap(corr_subset, \n",
    "                    annot=False, \n",
    "                    cmap='coolwarm', \n",
    "                    center=0,\n",
    "                    square=True, \n",
    "                    fmt='.2f',\n",
    "                    xticklabels=True,\n",
    "                    yticklabels=True)\n",
    "        plt.xticks(rotation=45, ha='right')\n",
    "        plt.yticks(rotation=0)\n",
    "        plt.title(f'High Correlation Features Heatmap (>{threshold})')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"높은 상관관계를 가진 피처가 없습니다.\")\n",
    "        \n",
    "except ImportError:\n",
    "    print(\"matplotlib/seaborn이 없어 히트맵을 생성할 수 없습니다.\")\n",
    "except Exception as e:\n",
    "    print(f\"히트맵 생성 중 오류: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f673c38d-0039-4659-a968-755833b0508c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 6. 다중공선성 검사\n",
    "def check_multicollinearity(corr_matrix, feature_names, threshold=0.9):\n",
    "    \"\"\"\n",
    "    다중공선성 검사\n",
    "    \"\"\"\n",
    "    corr_array = corr_matrix.toArray()\n",
    "    multicollinear_pairs = []\n",
    "    \n",
    "    for i in range(len(feature_names)):\n",
    "        for j in range(i+1, len(feature_names)):\n",
    "            corr_value = abs(corr_array[i][j])\n",
    "            if corr_value > threshold:\n",
    "                multicollinear_pairs.append({\n",
    "                    'feature1': feature_names[i],\n",
    "                    'feature2': feature_names[j],\n",
    "                    'correlation': corr_array[i][j]\n",
    "                })\n",
    "    \n",
    "    return multicollinear_pairs\n",
    "\n",
    "# 다중공선성 검사\n",
    "multicollinear = check_multicollinearity(correlation_matrix, feature_names, 0.9)\n",
    "\n",
    "print(f\"\\n=== 다중공선성 위험 ({len(multicollinear)}개) ===\")\n",
    "for pair in multicollinear:\n",
    "    print(f\"⚠️ {pair['feature1']} ↔ {pair['feature2']}: {pair['correlation']:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3546690b-105a-43b0-aa45-80beabf848a3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### 상관관계 분석 (GPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "edd1122b-66ce-4162-9613-2077286432d8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pyspark.sql.functions import col\n",
    "import time\n",
    "\n",
    "print(\"=== GPU로 상관관계 분석 ===\")\n",
    "\n",
    "# 1. GPU 메모리 최적화 설정\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"사용 디바이스: {device}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    gpu_props = torch.cuda.get_device_properties(0)\n",
    "    total_memory = gpu_props.total_memory / 1024**3\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU 총 메모리: {total_memory:.1f} GB\")\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "# 2. 전체 데이터 로드\n",
    "ps_df = ps_notperiod_df\n",
    "numeric_cols = [col_name for col_name, data_type in ps_df.dtypes\n",
    "                if data_type in ['int', 'bigint', 'float', 'double']]\n",
    "\n",
    "total_rows = ps_df.count()\n",
    "n_features = len(numeric_cols)\n",
    "print(f\"전체 데이터 수: {total_rows:,}\")\n",
    "print(f\"분석할 피처 수: {n_features}\")\n",
    "\n",
    "def compute_large_chunk_correlation(df, numeric_cols, device):\n",
    "    \"\"\"\n",
    "    대용량 청크로 GPU 상관관계 계산 (메모리 활용도 극대화)\n",
    "    \"\"\"\n",
    "    print(\"\\n=== 대용량 청크 GPU 처리 시작 ===\")\n",
    "    \n",
    "    # 훨씬 큰 청크 크기 설정 (Tesla T4 16GB 기준)\n",
    "    if torch.cuda.is_available():\n",
    "        # 16GB GPU에서 안전하게 사용할 수 있는 크기\n",
    "        # 상관관계 매트릭스 계산 시 중간 결과물 고려하여 보수적으로 설정\n",
    "        large_chunk_size = 2000000  # 200만 행부터 시작\n",
    "        \n",
    "        # 메모리 사용량 추정\n",
    "        estimated_memory_gb = (large_chunk_size * n_features * 4) / 1024**3  # float32 기준\n",
    "        print(f\"청크당 예상 메모리 사용량: {estimated_memory_gb:.2f} GB\")\n",
    "        \n",
    "        # GPU 메모리의 70% 이상 사용하도록 조정\n",
    "        target_memory_usage = total_memory * 0.7  # 70% 사용 목표\n",
    "        optimal_chunk_size = int((target_memory_usage * 1024**3) / (n_features * 4 * 3))  # 안전 마진\n",
    "        \n",
    "        # 최종 청크 크기 결정 (최소 100만, 최대 500만)\n",
    "        final_chunk_size = max(1000000, min(optimal_chunk_size, 5000000))\n",
    "        \n",
    "    else:\n",
    "        final_chunk_size = 1000000  # CPU의 경우\n",
    "    \n",
    "    print(f\"최종 청크 크기: {final_chunk_size:,} 행\")\n",
    "    \n",
    "    # 청크 개수 계산\n",
    "    n_rows = df.count()\n",
    "    n_chunks = max(1, (n_rows + final_chunk_size - 1) // final_chunk_size)\n",
    "    \n",
    "    print(f\"총 {n_chunks}개 청크로 분할\")\n",
    "    \n",
    "    if n_chunks > 10:\n",
    "        print(\"⚠️ 청크 개수가 많습니다. 청크 크기를 더 늘려보겠습니다.\")\n",
    "        final_chunk_size = max(final_chunk_size, n_rows // 5)  # 최대 5개 청크로 제한\n",
    "        n_chunks = max(1, (n_rows + final_chunk_size - 1) // final_chunk_size)\n",
    "        print(f\"조정된 청크 크기: {final_chunk_size:,} 행\")\n",
    "        print(f\"조정된 청크 개수: {n_chunks}개\")\n",
    "    \n",
    "    # 상관관계 매트릭스 누적을 위한 변수들\n",
    "    correlation_sum = None\n",
    "    total_weight = 0\n",
    "    \n",
    "    for chunk_idx in range(n_chunks):\n",
    "        chunk_start_time = time.time()\n",
    "        \n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"청크 {chunk_idx + 1}/{n_chunks} 처리 중...\")\n",
    "        \n",
    "        # 청크 데이터 추출 (더 효율적인 방법)\n",
    "        if n_chunks == 1:\n",
    "            # 전체 데이터를 한 번에 처리\n",
    "            chunk_df = df\n",
    "        else:\n",
    "            # 분할 처리\n",
    "            chunk_fraction = 1.0 / n_chunks\n",
    "            chunk_df = df.sample(fraction=chunk_fraction, seed=42 + chunk_idx)\n",
    "        \n",
    "        # Pandas로 변환\n",
    "        print(\"  PySpark → Pandas 변환 중...\")\n",
    "        conversion_start = time.time()\n",
    "        chunk_pdf = chunk_df.select(*numeric_cols).fillna(0).toPandas()\n",
    "        conversion_time = time.time() - conversion_start\n",
    "        \n",
    "        actual_chunk_size = len(chunk_pdf)\n",
    "        print(f\"  실제 청크 크기: {actual_chunk_size:,} x {len(chunk_pdf.columns)}\")\n",
    "        print(f\"  변환 시간: {conversion_time:.2f}초\")\n",
    "        \n",
    "        if actual_chunk_size == 0:\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            # GPU 메모리 상태 확인\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "                initial_memory = torch.cuda.memory_allocated() / 1024**3\n",
    "                available_memory = (torch.cuda.get_device_properties(0).total_memory - torch.cuda.memory_allocated()) / 1024**3\n",
    "                print(f\"  사용 가능 GPU 메모리: {available_memory:.2f} GB\")\n",
    "            \n",
    "            # GPU 텐서로 변환\n",
    "            print(\"  GPU 텐서 변환 중...\")\n",
    "            tensor_start = time.time()\n",
    "            chunk_tensor = torch.tensor(chunk_pdf.values, dtype=torch.float32).to(device)\n",
    "            tensor_time = time.time() - tensor_start\n",
    "            \n",
    "            if torch.cuda.is_available():\n",
    "                after_tensor_memory = torch.cuda.memory_allocated() / 1024**3\n",
    "                memory_used = after_tensor_memory - initial_memory\n",
    "                print(f\"  GPU 메모리 사용량: {memory_used:.2f} GB\")\n",
    "                print(f\"  텐서 변환 시간: {tensor_time:.2f}초\")\n",
    "            \n",
    "            # 상관관계 계산\n",
    "            print(\"  상관관계 계산 중...\")\n",
    "            corr_start = time.time()\n",
    "            chunk_correlation = torch.corrcoef(chunk_tensor.T)\n",
    "            corr_time = time.time() - corr_start\n",
    "            print(f\"  상관관계 계산 시간: {corr_time:.2f}초\")\n",
    "            \n",
    "            # CPU로 이동하여 누적\n",
    "            chunk_corr_cpu = chunk_correlation.cpu().numpy()\n",
    "            \n",
    "            # 가중 평균으로 누적\n",
    "            weight = actual_chunk_size\n",
    "            if correlation_sum is None:\n",
    "                correlation_sum = chunk_corr_cpu * weight\n",
    "            else:\n",
    "                correlation_sum += chunk_corr_cpu * weight\n",
    "            total_weight += weight\n",
    "            \n",
    "            # 메모리 정리\n",
    "            del chunk_tensor, chunk_correlation, chunk_pdf\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "            \n",
    "            chunk_total_time = time.time() - chunk_start_time\n",
    "            print(f\"  청크 총 처리 시간: {chunk_total_time:.2f}초\")\n",
    "            print(f\"  진행률: {(chunk_idx + 1) / n_chunks * 100:.1f}%\")\n",
    "            \n",
    "            # 남은 시간 추정\n",
    "            if chunk_idx > 0:\n",
    "                avg_time_per_chunk = (time.time() - start_time) / (chunk_idx + 1)\n",
    "                remaining_time = avg_time_per_chunk * (n_chunks - chunk_idx - 1)\n",
    "                print(f\"  예상 남은 시간: {remaining_time:.1f}초 ({remaining_time/60:.1f}분)\")\n",
    "            \n",
    "        except RuntimeError as e:\n",
    "            if \"out of memory\" in str(e):\n",
    "                print(f\"  ❌ GPU 메모리 부족! 현재 청크 크기: {actual_chunk_size:,}\")\n",
    "                print(\"  더 작은 청크로 재시도하거나 CPU로 fallback이 필요합니다.\")\n",
    "                raise e\n",
    "            else:\n",
    "                raise e\n",
    "    \n",
    "    # 최종 상관관계 매트릭스 계산\n",
    "    if total_weight > 0:\n",
    "        final_correlation = correlation_sum / total_weight\n",
    "        return final_correlation, numeric_cols\n",
    "    else:\n",
    "        return None, numeric_cols\n",
    "\n",
    "# 3. 대용량 청크 상관관계 분석 실행\n",
    "print(f\"\\n🚀 대용량 청크로 전체 데이터 {total_rows:,}행 분석 시작\")\n",
    "start_time = time.time()\n",
    "\n",
    "try:\n",
    "    correlation_matrix, feature_names = compute_large_chunk_correlation(\n",
    "        ps_df, \n",
    "        numeric_cols, \n",
    "        device\n",
    "    )\n",
    "    \n",
    "    end_time = time.time()\n",
    "    total_time = end_time - start_time\n",
    "    print(f\"\\n⏱️ 전체 처리 시간: {total_time:.2f}초 ({total_time/60:.1f}분)\")\n",
    "    print(f\"📊 처리 속도: {total_rows/total_time:,.0f} 행/초\")\n",
    "    \n",
    "    # 4. 결과 분석\n",
    "    if correlation_matrix is not None:\n",
    "        print(f\"\\n=== ✅ 대용량 청크 상관관계 분석 결과 ✅ ===\")\n",
    "        print(f\"분석된 데이터: {total_rows:,}행\")\n",
    "        print(f\"분석된 피처: {len(feature_names)}개\")\n",
    "        print(f\"상관관계 매트릭스 크기: {correlation_matrix.shape}\")\n",
    "        \n",
    "        # 높은 상관관계 분석\n",
    "        def analyze_correlations(corr_matrix, features, threshold=0.7):\n",
    "            high_corr = []\n",
    "            n = len(features)\n",
    "            for i in range(n):\n",
    "                for j in range(i+1, n):\n",
    "                    corr_val = corr_matrix[i, j]\n",
    "                    if abs(corr_val) > threshold:\n",
    "                        high_corr.append({\n",
    "                            'feature1': features[i],\n",
    "                            'feature2': features[j],\n",
    "                            'correlation': float(corr_val)\n",
    "                        })\n",
    "            return high_corr\n",
    "        \n",
    "        # 높은 상관관계 출력\n",
    "        high_correlations = analyze_correlations(correlation_matrix, feature_names, 0.7)\n",
    "        print(f\"\\n높은 상관관계 (|r| > 0.7): {len(high_correlations)}개\")\n",
    "        \n",
    "        for pair in sorted(high_correlations, key=lambda x: abs(x['correlation']), reverse=True)[:20]:\n",
    "            print(f\"  {pair['feature1']} ↔ {pair['feature2']}: {pair['correlation']:.3f}\")\n",
    "        \n",
    "        # 다중공선성 위험\n",
    "        multicollinear = analyze_correlations(correlation_matrix, feature_names, 0.9)\n",
    "        print(f\"\\n다중공선성 위험 (|r| > 0.9): {len(multicollinear)}개\")\n",
    "        \n",
    "        for pair in sorted(multicollinear, key=lambda x: abs(x['correlation']), reverse=True):\n",
    "            print(f\"  ⚠️ {pair['feature1']} ↔ {pair['feature2']}: {pair['correlation']:.3f}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"❌ 오류 발생: {str(e)}\")\n",
    "    print(\"CPU로 fallback을 시도하거나 청크 크기를 더 줄여보세요.\")\n",
    "\n",
    "# 5. 메모리 정리\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    final_memory = torch.cuda.memory_allocated() / 1024**3\n",
    "    peak_memory = torch.cuda.max_memory_allocated() / 1024**3\n",
    "    print(f\"\\n🧹 최종 GPU 메모리 사용량: {final_memory:.2f} GB\")\n",
    "    print(f\"📈 최대 GPU 메모리 사용량: {peak_memory:.2f} GB\")\n",
    "\n",
    "print(\"\\n✅ GPU 상관관계 분석 완료!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9b654c77-3f24-4f87-9144-7a58f29ff7d0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "### 데이터 베이스 사용 설정\n",
    "spark.sql(\"USE database_03_cache\")\n",
    "print(\"현재 데이터베이스를 'database_03_cache'로 설정\")\n",
    "\n",
    "ps_notperiod_df.write.mode(\"overwrite\").saveAsTable(\"count_notperiod_df_proc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f8faab95-0ce5-40a7-bd84-b1f021b21a6c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [
    {
     "elements": [
      {
       "dashboardResultIndex": null,
       "elementNUID": "6abccc6f-e2ca-4cd7-a214-5b377c0fb3ba",
       "elementType": "command",
       "guid": "2d443798-4368-421f-a80f-292495a65e2a",
       "options": null,
       "position": {
        "height": 9,
        "width": 12,
        "x": 0,
        "y": 0,
        "z": null
       },
       "resultIndex": null
      },
      {
       "dashboardResultIndex": null,
       "elementNUID": "5b40b8c9-1434-41fd-93a5-ddbffd22221d",
       "elementType": "command",
       "guid": "68696a03-7229-4f52-8287-9387fc7a4fad",
       "options": null,
       "position": {
        "height": 6,
        "width": 12,
        "x": 0,
        "y": 9,
        "z": null
       },
       "resultIndex": null
      }
     ],
     "globalVars": {},
     "guid": "",
     "layoutOption": {
      "grid": true,
      "stack": false
     },
     "nuid": "46ef278e-af8b-4845-a8f4-990babf3c9f6",
     "origId": 952016186377752,
     "title": "컬럼별 분포",
     "version": "DashboardViewV1",
     "width": 2560
    }
   ],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "03_3. count_v2",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
