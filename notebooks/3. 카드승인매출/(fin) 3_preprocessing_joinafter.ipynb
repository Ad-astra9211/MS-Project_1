{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "78f0e8ca-0e92-4491-8a97-5696be032912",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 클러스터링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "693c0575-93bf-44d1-a99c-e90bafc3d9ed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 라이브러리 임포트\n",
    "from kmeans_pytorch import kmeans\n",
    "from pyspark.sql import Row, SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.ml.feature import VectorAssembler, PCA, StandardScaler\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import font_manager\n",
    "\n",
    "font_dirs = [\"/usr/share/fonts/truetype/nanum/\"]\n",
    "font_files = font_manager.findSystemFonts(fontpaths=font_dirs)\n",
    " \n",
    "for font_file in font_files:\n",
    "    font_manager.fontManager.addfont(font_file)\n",
    " \n",
    "plt.rc('font', family='NanumGothic')\n",
    "plt.rc('axes', unicode_minus=False)\n",
    " \n",
    "# pd.Series([-1,2,3]).plot(title='테스트', figsize=(3,2))\n",
    "# pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6549c967-c673-4f1f-b81a-db703f90ed40",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### 1. 데이터 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0ee3f4e2-9eeb-4a28-9e73-50332703c151",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Databricks data profile. Run in Databricks to view."
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1.subcommand+json": {
       "baseErrorDetails": null,
       "bindings": {},
       "collapsed": false,
       "command": "%python\nif hasattr(dbutils, \"data\") and hasattr(dbutils.data, \"summarize\"):\n  # setup\n  __data_summary_display_orig = display\n  __data_summary_dfs = []\n  def __data_summary_display_new(df):\n    # add only when result is going to be table type\n    __data_summary_df_modules = [\"pandas.core.frame\", \"databricks.koalas.frame\", \"pyspark.sql.dataframe\", \"pyspark.pandas.frame\", \"pyspark.sql.connect.dataframe\", \"pyspark.sql.classic.dataframe\"]\n    if (type(df).__module__ in __data_summary_df_modules and type(df).__name__ == 'DataFrame') or isinstance(df, list):\n      __data_summary_dfs.append(df)\n  display = __data_summary_display_new\n\n  def __data_summary_user_code_fn():\n    import base64\n    exec(base64.standard_b64decode(\"IyDrjbDsnbTthLAg66Gc65OcCmRmX3NhbXBsZSA9IHNwYXJrLnJlYWQuZm9ybWF0KCJkZWx0YSIpLnRhYmxlKCJkYXRhYmFzZV9wanQuM191c2VfZW5jb2Rpbmdfc2FtcGxlIikKcHJpbnQoZiLrjbDsnbTthLAg7YGs6riwOiB7ZGZfc2FtcGxlLmNvdW50KCl96rCcIO2WiSwge2xlbihkZl9zYW1wbGUuY29sdW1ucyl96rCcIOy7rOufvCIpCmRpc3BsYXkoZGZfc2FtcGxlLmhlYWQoMykp\").decode())\n\n  try:\n    # run user code\n    __data_summary_user_code_fn()\n\n    # run on valid tableResultIndex\n    if len(__data_summary_dfs) > 0:\n      # run summarize\n      if type(__data_summary_dfs[0]).__module__ == \"databricks.koalas.frame\":\n        # koalas dataframe\n        dbutils.data.summarize(__data_summary_dfs[0].to_spark())\n      elif type(__data_summary_dfs[0]).__module__ == \"pandas.core.frame\":\n        # pandas dataframe\n        dbutils.data.summarize(spark.createDataFrame(__data_summary_dfs[0]))\n      else:\n        dbutils.data.summarize(__data_summary_dfs[0])\n    else:\n        displayHTML(\"dataframe no longer exists. If you're using dataframe.display(), use display(dataframe) instead.\")\n\n  finally:\n    display = __data_summary_display_orig\n    del __data_summary_display_new\n    del __data_summary_display_orig\n    del __data_summary_dfs\n    del __data_summary_user_code_fn\nelse:\n  print(\"This DBR version does not support data profiles.\")",
       "commandTitle": "데이터 프로필 1",
       "commandType": "auto",
       "commandVersion": 0,
       "commentThread": [],
       "commentsVisible": false,
       "contentSha256Hex": null,
       "customPlotOptions": {},
       "datasetPreviewNameToCmdIdMap": {},
       "diffDeletes": [],
       "diffInserts": [],
       "displayType": "table",
       "error": null,
       "errorDetails": null,
       "errorSummary": null,
       "errorTraceType": null,
       "finishTime": 0,
       "globalVars": {},
       "guid": "",
       "height": "auto",
       "hideCommandCode": false,
       "hideCommandResult": false,
       "iPythonMetadata": null,
       "inputWidgets": {},
       "isLockedInExamMode": false,
       "latestUser": "a user",
       "latestUserId": null,
       "listResultMetadata": null,
       "metadata": {
        "byteLimit": 2048000,
        "rowLimit": 10000
       },
       "nuid": "4d664761-839d-41b5-895a-bbf3512e0230",
       "origId": 0,
       "parentHierarchy": [],
       "pivotAggregation": null,
       "pivotColumns": null,
       "position": 14.0,
       "resultDbfsErrorMessage": null,
       "resultDbfsStatus": "INLINED_IN_TREE",
       "results": null,
       "showCommandTitle": false,
       "startTime": 0,
       "state": "input",
       "streamStates": {},
       "subcommandOptions": null,
       "submitTime": 0,
       "subtype": "tableResultSubCmd.dataSummary",
       "tableResultIndex": 0,
       "tableResultSettingsMap": {},
       "useConsistentColors": false,
       "version": "CommandV1",
       "width": "auto",
       "workflows": null,
       "xColumns": null,
       "yColumns": null
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 데이터 로드\n",
    "df_sample = spark.read.format(\"delta\").table(\"database_pjt.3_use_encoding_sample\")\n",
    "print(f\"데이터 크기: {df_sample.count()}개 행, {len(df_sample.columns)}개 컬럼\")\n",
    "display(df_sample.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a825699d-d1e3-4b20-89d0-786a83f4ecaf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "len(df_sample.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "80c7d0ef-3908-4288-9d33-539d1463f94c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### 2. 피처 선택"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "caf33be8-d55f-4ab4-bd43-cac1a3a80f89",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 수치형 컬럼만 추출 (클러스터링에 사용할 변수들)\n",
    "from pyspark.sql.types import DoubleType, IntegerType, LongType, FloatType\n",
    "\n",
    "numeric_cols = [\n",
    "    f.name for f in df_sample.schema.fields\n",
    "    if isinstance(f.dataType, (DoubleType, IntegerType, LongType, FloatType))\n",
    "    and f.name not in ['row_id']  # ID 컬럼 제외\n",
    "]\n",
    "\n",
    "print(f\"클러스터링에 사용할 수치형 컬럼 개수: {len(numeric_cols)}\")\n",
    "print(\"주요 컬럼들:\", numeric_cols[:10])  # 처음 10개만 출력\n",
    "\n",
    "# 결측값 확인\n",
    "df_sample.select([F.count(F.when(F.col(c).isNull(), c)).alias(c) for c in numeric_cols[:5]]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b2aae6e4-e4b3-4df0-98dc-984e1f63c2e2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 제외된 수치형 컬럼\n",
    "excluded_numeric_cols = list(set(df_sample.columns) - set(numeric_cols))\n",
    "\n",
    "print(f\"❗ 제외된 수치형 컬럼 수: {len(excluded_numeric_cols)}\")\n",
    "print(\"제외된 컬럼 목록:\", excluded_numeric_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "07d8fa9a-2afc-42c1-9eec-1b1adad2b2c9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### 3. 벡터화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1972920a-4b76-46c4-8087-f781fa0820a8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 모든 수치형 컬럼을 하나의 벡터로 결합\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=numeric_cols,\n",
    "    outputCol=\"features\",\n",
    "    handleInvalid=\"skip\"  # 결측값이 있는 행은 제외\n",
    ")\n",
    "\n",
    "df_vectorized = assembler.transform(df_sample)\n",
    "print(\"벡터화 완료!\")\n",
    "\n",
    "# 벡터 차원 확인\n",
    "first_vector = df_vectorized.select(\"features\").first()[0]\n",
    "print(f\"벡터 차원: {first_vector.size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "59fb2aaf-122b-4365-8fbf-eb088e074032",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### 4. 스케일링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a5a8aefd-3866-4f67-9979-f448607c68d7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 표준화 (평균 0, 분산 1로 조정)\n",
    "scaler = StandardScaler(\n",
    "    inputCol=\"features\",\n",
    "    outputCol=\"scaled_features\",\n",
    "    withStd=True,\n",
    "    withMean=True\n",
    ")\n",
    "\n",
    "scaler_model = scaler.fit(df_vectorized)\n",
    "df_scaled = scaler_model.transform(df_vectorized)\n",
    "\n",
    "print(\"스케일링 완료!\")\n",
    "df_scaled.select(\"scaled_features\").show(3, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3440fe48-484d-4655-8ef0-7c52aeb98421",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_scaled.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1cfc3696-b53a-4da1-9184-8c7e827bc4c7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Databricks data profile. Run in Databricks to view."
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1.subcommand+json": {
       "baseErrorDetails": null,
       "bindings": {},
       "collapsed": false,
       "command": "%python\nif hasattr(dbutils, \"data\") and hasattr(dbutils.data, \"summarize\"):\n  # setup\n  __data_summary_display_orig = display\n  __data_summary_dfs = []\n  def __data_summary_display_new(df):\n    # add only when result is going to be table type\n    __data_summary_df_modules = [\"pandas.core.frame\", \"databricks.koalas.frame\", \"pyspark.sql.dataframe\", \"pyspark.pandas.frame\", \"pyspark.sql.connect.dataframe\", \"pyspark.sql.classic.dataframe\"]\n    if (type(df).__module__ in __data_summary_df_modules and type(df).__name__ == 'DataFrame') or isinstance(df, list):\n      __data_summary_dfs.append(df)\n  display = __data_summary_display_new\n\n  def __data_summary_user_code_fn():\n    import base64\n    exec(base64.standard_b64decode(\"ZGlzcGxheShkZl9zY2FsZWQp\").decode())\n\n  try:\n    # run user code\n    __data_summary_user_code_fn()\n\n    # run on valid tableResultIndex\n    if len(__data_summary_dfs) > 0:\n      # run summarize\n      if type(__data_summary_dfs[0]).__module__ == \"databricks.koalas.frame\":\n        # koalas dataframe\n        dbutils.data.summarize(__data_summary_dfs[0].to_spark())\n      elif type(__data_summary_dfs[0]).__module__ == \"pandas.core.frame\":\n        # pandas dataframe\n        dbutils.data.summarize(spark.createDataFrame(__data_summary_dfs[0]))\n      else:\n        dbutils.data.summarize(__data_summary_dfs[0])\n    else:\n        displayHTML(\"dataframe no longer exists. If you're using dataframe.display(), use display(dataframe) instead.\")\n\n  finally:\n    display = __data_summary_display_orig\n    del __data_summary_display_new\n    del __data_summary_display_orig\n    del __data_summary_dfs\n    del __data_summary_user_code_fn\nelse:\n  print(\"This DBR version does not support data profiles.\")",
       "commandTitle": "데이터 프로필 1",
       "commandType": "auto",
       "commandVersion": 0,
       "commentThread": [],
       "commentsVisible": false,
       "contentSha256Hex": null,
       "customPlotOptions": {},
       "datasetPreviewNameToCmdIdMap": {},
       "diffDeletes": [],
       "diffInserts": [],
       "displayType": "table",
       "error": null,
       "errorDetails": null,
       "errorSummary": null,
       "errorTraceType": null,
       "finishTime": 0,
       "globalVars": {},
       "guid": "",
       "height": "auto",
       "hideCommandCode": false,
       "hideCommandResult": false,
       "iPythonMetadata": null,
       "inputWidgets": {},
       "isLockedInExamMode": false,
       "latestUser": "a user",
       "latestUserId": null,
       "listResultMetadata": null,
       "metadata": {
        "byteLimit": 2048000,
        "rowLimit": 10000
       },
       "nuid": "9f8f2832-fc31-432e-aff6-60ce0141c419",
       "origId": 0,
       "parentHierarchy": [],
       "pivotAggregation": null,
       "pivotColumns": null,
       "position": 17.25,
       "resultDbfsErrorMessage": null,
       "resultDbfsStatus": "INLINED_IN_TREE",
       "results": null,
       "showCommandTitle": false,
       "startTime": 0,
       "state": "input",
       "streamStates": {},
       "subcommandOptions": null,
       "submitTime": 0,
       "subtype": "tableResultSubCmd.dataSummary",
       "tableResultIndex": 0,
       "tableResultSettingsMap": {},
       "useConsistentColors": false,
       "version": "CommandV1",
       "width": "auto",
       "workflows": null,
       "xColumns": null,
       "yColumns": null
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(df_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "17738fce-1b99-4e3d-a509-cca8ea88c631",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### 5. 상관관계"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ab28b57b-0ba6-488e-a78a-f2ab4570061c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# PCA 차원 축소"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a0721e3d-a15f-4111-a394-742805b2ec1d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import PCA\n",
    "\n",
    "# PCA 모델을 k=6으로 설정하여 생성\n",
    "pca = PCA(k=6, inputCol=\"scaled_features\", outputCol=\"pcaFeatures\")\n",
    "\n",
    "# PCA 모델 학습 및 데이터 변환\n",
    "pca_model = pca.fit(df_scaled)\n",
    "pca_df = pca_model.transform(df_scaled)\n",
    "\n",
    "print(\"PCA 적용 후 데이터 스키마:\")\n",
    "pca_df.printSchema()\n",
    "\n",
    "# pcaFeatures 컬럼이 추가된 것을 확인할 수 있습니다.\n",
    "pca_df.select(\"pcaFeatures\").show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2e77a538-a3d4-41aa-84bd-4cb24026aa6e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import PercentFormatter\n",
    "import collections.abc  # To check for sequence type\n",
    "\n",
    "# seaborn 스타일을 사용하여 그래프를 더 깔끔하게 만듭니다.\n",
    "# plt.style.use('seaborn-v0_8-whitegrid')  # seaborn이 설치된 경우 활성화\n",
    "# 한글 폰트가 깨질 경우를 대비한 설정 (환경에 맞게 설치 필요)\n",
    "# import koreanize_matplotlib\n",
    "\n",
    "def analyze_and_plot_pca_variance(pca_model, threshold=0.80):\n",
    "    \"\"\"\n",
    "    PySpark 또는 Scikit-learn PCA 모델의 설명 분산을 분석하고 시각화합니다.\n",
    "\n",
    "    Args:\n",
    "        pca_model: 학습이 완료된 PySpark 또는 Scikit-learn PCA 모델 객체.\n",
    "        threshold (float): 누적 설명력 기준값 (0.0 ~ 1.0).\n",
    "    \"\"\"\n",
    "    # 1. 데이터 추출 및 검증\n",
    "    explained_variance_ratio = None\n",
    "    try:\n",
    "        # PySpark 모델의 경우 .explainedVariance가 Vector 형태이므로 toArray()로 변환\n",
    "        explained_variance_ratio = np.array(pca_model.explainedVariance.toArray())\n",
    "    except AttributeError:\n",
    "        # Scikit-learn 모델 호환성을 위함\n",
    "        try:\n",
    "            explained_variance_ratio = pca_model.explained_variance_ratio_\n",
    "        except AttributeError:\n",
    "            raise ValueError(\"입력된 객체에 'explainedVariance' 또는 'explained_variance_ratio_' 속성이 없습니다. 올바른 PCA 모델 객체인지 확인해주세요.\")\n",
    "\n",
    "    # [오류 방지 코드] 추출된 데이터가 배열/리스트 형태인지 확인\n",
    "    if not isinstance(explained_variance_ratio, (np.ndarray, list)):\n",
    "        raise ValueError(\n",
    "            f\"설명 분산 데이터가 배열 형태가 아닙니다 (현재 타입: {type(explained_variance_ratio)}). \"\n",
    "            f\"PCA 모델이 올바르게 학습되었는지, 혹은 단일 주성분만 포함하고 있는지 확인해주세요.\"\n",
    "        )\n",
    "\n",
    "    cumulative_variance = np.cumsum(explained_variance_ratio)\n",
    "    n_total_components = len(explained_variance_ratio)\n",
    "\n",
    "    # 2. 목표 기준점을 넘는 지점 확인\n",
    "    if np.any(cumulative_variance >= threshold):\n",
    "        n_components_needed = np.argmax(cumulative_variance >= threshold) + 1\n",
    "        actual_variance_at_threshold = cumulative_variance[n_components_needed - 1]\n",
    "    else:\n",
    "        n_components_needed = n_total_components\n",
    "        actual_variance_at_threshold = cumulative_variance[-1]\n",
    "\n",
    "    # 3. 결과 출력\n",
    "    print(f\"[✅ PCA 설명 분산 분석 결과]\")\n",
    "    print(f\"총 {n_total_components}개의 주성분 중,\")\n",
    "    print(f\"선택된 주성분 개수: {n_components_needed}개\")\n",
    "    print(f\"이때의 누적 설명력: {actual_variance_at_threshold:.2%}\")\n",
    "    print(f\"(목표 기준: {threshold:.0%} 이상)\")\n",
    "\n",
    "    # 4. 시각화\n",
    "    fig, ax1 = plt.subplots(figsize=(12, 7))\n",
    "    x_components = range(1, n_total_components + 1)\n",
    "    \n",
    "    ax1.bar(x_components, explained_variance_ratio, alpha=0.6, color='skyblue', label='개별 주성분 설명력')\n",
    "    ax1.set_xlabel('주성분 개수', fontsize=12)\n",
    "    ax1.set_ylabel('개별 주성분 설명력', fontsize=12, color='skyblue')\n",
    "    ax1.tick_params(axis='y', labelcolor='skyblue')\n",
    "    ax1.yaxis.set_major_formatter(PercentFormatter(1.0))\n",
    "\n",
    "    ax2 = ax1.twinx()\n",
    "    ax2.plot(x_components, cumulative_variance, 'o-', color='royalblue', label='누적 설명력')\n",
    "    ax2.set_ylabel('누적 설명력', fontsize=12, color='royalblue')\n",
    "    ax2.tick_params(axis='y', labelcolor='royalblue')\n",
    "    ax2.yaxis.set_major_formatter(PercentFormatter(1.0))\n",
    "\n",
    "    ax2.axhline(y=threshold, color='red', linestyle='--', label=f'목표 기준선 ({threshold:.0%})')\n",
    "    ax2.axvline(x=n_components_needed, color='green', linestyle='--', label=f'필요 주성분 개수 ({n_components_needed}개)')\n",
    "    ax2.plot(n_components_needed, actual_variance_at_threshold, 'ro', markersize=10, label=f'결과 지점 ({actual_variance_at_threshold:.2%})')\n",
    "\n",
    "    plt.title('주성분 개수에 따른 설명 분산', fontsize=16, pad=20)\n",
    "    lines, labels = ax1.get_legend_handles_labels()\n",
    "    lines2, labels2 = ax2.get_legend_handles_labels()\n",
    "    ax2.legend(lines + lines2, labels + labels2, loc='best')\n",
    "    \n",
    "    plt.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# --- 예제 사용법 ---\n",
    "from unittest.mock import Mock\n",
    "mock_pca_model = Mock()\n",
    "# Scikit-learn 스타일\n",
    "mock_pca_model.explained_variance_ratio_ = np.array(\n",
    "     [0.25, 0.20, 0.15, 0.10, 0.08, 0.05, 0.04, 0.03, 0.02, 0.01] + [0.005]*10\n",
    ")\n",
    "# PySpark 스타일\n",
    "mock_pca_model.explainedVariance.toArray.return_value = mock_pca_model.explained_variance_ratio_\n",
    "\n",
    "# 함수 호출\n",
    "analyze_and_plot_pca_variance(mock_pca_model, threshold=0.80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9746bff2-bfff-4eb8-af79-5573895f62c7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pc_matrix = pca_model.pc.toArray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "48b86688-77eb-42dd-9e7b-4b2b58428551",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pc1_loadings = pc_matrix[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "da28fcba-a5ef-4631-bb85-6cb71a6c190a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pc2_loadings = pc_matrix[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c4e953d2-172b-4f38-8b75-acfca96fcf57",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 추가\n",
    "pc3_loadings = pc_matrix[:, 2] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9f0d81b0-8740-4c4a-a7de-873633fde6dd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "PC50_loading = pc_matrix[:, 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8c5f0d39-379a-4241-a945-9acfd4fbaaee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 원본 특성 이름 리스트 (numeric_cols)가 이미 정의되어 있다고 가정합니다.\n",
    "# 예시: numeric_cols = ['feature1', 'feature2', ..., 'feature196']\n",
    "\n",
    "loadings_df = pd.DataFrame({\n",
    "    'feature': numeric_cols,\n",
    "    'PC1_loading': pc1_loadings,\n",
    "    'PC2_loading': pc2_loadings,\n",
    "    'PC3_loading': pc3_loadings,\n",
    "    'PC50_loading': PC50_loading\n",
    "\n",
    "    # 필요하다면 다른 주성분들도 추가할 수 있습니다 (예: PC3_loading = pc_matrix[:, 2])\n",
    "})\n",
    "\n",
    "# PC1에 대해 절대값이 큰 순서대로 정렬하여 상위 N개 특성 확인\n",
    "print(\"--- PC1 Loadings (Top N) ---\")\n",
    "print(loadings_df.reindex(loadings_df.PC1_loading.abs().sort_values(ascending=False).index).head(10)) # 상위 10개\n",
    "\n",
    "# PC2에 대해 절대값이 큰 순서대로 정렬하여 상위 N개 특성 확인\n",
    "print(\"\\n--- PC2 Loadings (Top N) ---\")\n",
    "print(loadings_df.reindex(loadings_df.PC2_loading.abs().sort_values(ascending=False).index).head(10)) # 상위 10개"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "950a9913-4ab9-4c87-a3ee-dfa752aacb3e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# PC2에 대해 절대값이 큰 순서대로 정렬하여 상위 N개 특성 확인\n",
    "print(\"\\n--- PC50 Loadings (Top N) ---\")\n",
    "print(loadings_df.reindex(loadings_df.PC50_loading.abs().sort_values(ascending=False).index).head(10)) # 상위 10개"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7bc20b72-fec1-4493-8680-d1c71498d0ba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# 클러스터링 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5dff20b7-64ce-44ee-938d-132b273fbe06",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# PCA 결과를 NumPy 배열로 변환\n",
    "pca_vectors = pca_df.select(\"pcaFeatures\").collect()\n",
    "pca_array = np.array([row.pcaFeatures.toArray() for row in pca_vectors])\n",
    "\n",
    "print(f\"NumPy 배열 크기: {pca_array.shape}\")\n",
    "\n",
    "# GPU 텐서로 변환\n",
    "pca_tensor = torch.from_numpy(pca_array).float()\n",
    "\n",
    "# GPU 사용 가능하면 GPU로, 아니면 CPU 사용\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "pca_tensor = pca_tensor.to(device)\n",
    "\n",
    "print(f\"사용 디바이스: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "841b78fa-0ece-45f1-895b-abe347804221",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### 7. 최적의 클러스터 개수 찾기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9552c59e-6228-4990-ae07-95fffe7b37a7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml.evaluation import ClusteringEvaluator\n",
    "import numpy as np\n",
    "\n",
    "# 테스트할 K값의 범위 설정\n",
    "k_values = range(2, 12) # 2부터 11까지 테스트\n",
    "\n",
    "# 결과를 저장할 리스트\n",
    "wcss_scores = []\n",
    "silhouette_scores = []\n",
    "\n",
    "print(\"최적의 K를 찾기 위한 탐색을 시작합니다...\")\n",
    "for k in k_values:\n",
    "    # K-Means 모델 생성 (중요: featuresCol을 'pcaFeatures'로 설정)\n",
    "    kmeans = KMeans(featuresCol=\"pcaFeatures\", k=k, seed=42)\n",
    "    \n",
    "    # 모델 학습\n",
    "    model = kmeans.fit(pca_df)\n",
    "    \n",
    "    # 1. WCSS (오차제곱합) 계산\n",
    "    wcss = model.summary.trainingCost\n",
    "    wcss_scores.append(wcss)\n",
    "    \n",
    "    # 2. 실루엣 점수 계산\n",
    "    predictions = model.transform(pca_df)\n",
    "    evaluator = ClusteringEvaluator(featuresCol=\"pcaFeatures\", metricName=\"silhouette\", distanceMeasure=\"squaredEuclidean\")\n",
    "    silhouette = evaluator.evaluate(predictions)\n",
    "    silhouette_scores.append(silhouette)\n",
    "    \n",
    "    print(f\"K = {k} | WCSS = {wcss:.2f} | Silhouette Score = {silhouette:.4f}\")\n",
    "\n",
    "print(\"\\n탐색 완료!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4f03f838-b19e-44da-85c5-63e7b6f5e1f6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(k_values, wcss_scores, 'bo-')\n",
    "plt.title('Elbow Method (엘보우 방법)')\n",
    "plt.xlabel('클러스터 개수 (K)')\n",
    "plt.ylabel('오차제곱합 (WCSS)')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b1b1cc83-bfa7-40b6-9a7f-d5363aaf8f2c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(k_values, silhouette_scores, 'go-')\n",
    "plt.title('Silhouette Scores (실루엣 점수)')\n",
    "plt.xlabel('클러스터 개수 (K)')\n",
    "plt.ylabel('실루엣 점수')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8e6cb6e4-f14a-4754-9f3f-1dcf4c25d703",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### 8. 최종 클러스터링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8f645d6c-54f2-43d3-8ccc-1564d405b5bb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# 앞 단계에서 결정한 최적의 K값입니다. (실용적 관점에서 4를 사용)\n",
    "best_k = 4 \n",
    "\n",
    "print(f\"최적 K={best_k}로 최종 클러스터링을 실행합니다...\")\n",
    "\n",
    "# 1. KMeans 모델 객체를 만듭니다. (아직 실행은 안 합니다)\n",
    "#    - featuresCol: 클러스터링할 데이터가 담긴 컬럼명 (PCA 결과 컬럼)\n",
    "#    - k: 클러스터 개수\n",
    "#    - seed: 실행할 때마다 결과가 바뀌지 않도록 고정\n",
    "final_kmeans = KMeans(featuresCol=\"pcaFeatures\", k=best_k, seed=42)\n",
    "\n",
    "\n",
    "# 2. .fit() 으로 모델을 학습시킵니다.\n",
    "#    pca_df는 PCA 변환까지 완료된 데이터프레임입니다.\n",
    "print(\"모델을 학습 중입니다...\")\n",
    "final_model = final_kmeans.fit(pca_df)\n",
    "\n",
    "\n",
    "# 3. .transform() 으로 클러스터 ID('prediction' 컬럼)를 추가합니다.\n",
    "#    이 한 줄로 기존 데이터에 클러스터링 결과가 자동으로 합쳐집니다.\n",
    "print(\"각 데이터에 클러스터 ID를 할당 중입니다...\")\n",
    "df_final_result = final_model.transform(pca_df)\n",
    "\n",
    "\n",
    "# 4. 최종 결과 확인\n",
    "print(\"\\n클러스터링 완료!\")\n",
    "\n",
    "# 이제 df_final_result에는 원본 컬럼 + 모든 변환 컬럼 + prediction 컬럼이 다 들어있습니다.\n",
    "# 에러 없이 원하는 컬럼을 선택해서 볼 수 있습니다.\n",
    "print(\"클러스터링 결과 샘플 확인 (상위 10개):\")\n",
    "df_final_result.select(\"기준년월\", \"prediction\").show(10) # '기준년월' 등 원본 컬럼명을 넣어 확인해보세요.\n",
    "\n",
    "# 클러스터별 데이터가 몇 개씩 있는지 확인\n",
    "print(\"클러스터별 분포 확인:\")\n",
    "df_final_result.groupBy(\"prediction\").count().orderBy(\"prediction\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3798a17f-5265-4c79-96f0-368ccf2799de",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# --- 이 코드를 K-Means 실행 후, t-SNE 시각화 전에 추가하세요 ---\n",
    "\n",
    "print(\"시각화를 위해 Spark 데이터프레임을 Pandas로 변환합니다...\")\n",
    "\n",
    "# 1. df_final_result에서 시각화에 필요한 두 컬럼('prediction', 'pcaFeatures')을 선택합니다.\n",
    "# 2. .toPandas()를 사용해 Spark 데이터프레임을 Pandas 데이터프레임으로 변환합니다.\n",
    "#    (데이터가 매우 클 경우 메모리 문제가 생길 수 있지만, 현재는 샘플링된 데이터라 괜찮습니다.)\n",
    "pdf_for_viz = df_final_result.select(\"prediction\", \"pcaFeatures\").toPandas()\n",
    "\n",
    "\n",
    "# 3. 'prediction' 컬럼을 NumPy 배열로 변환합니다.\n",
    "#    이것이 시각화 코드에서 사용할 'cluster_results'가 됩니다.\n",
    "cluster_results = pdf_for_viz['prediction'].to_numpy()\n",
    "\n",
    "\n",
    "# 4. 'pcaFeatures' 컬럼도 t-SNE가 사용할 수 있도록 NumPy 배열로 변환합니다.\n",
    "pca_array = np.array(pdf_for_viz['pcaFeatures'].apply(lambda vec: vec.toArray()).tolist())\n",
    "\n",
    "\n",
    "print(\"\\n데이터 준비 완료!\")\n",
    "print(f\" - pca_array 형태: {pca_array.shape}\")\n",
    "print(f\" - cluster_results 형태: {cluster_results.shape}\")\n",
    "print(f\" - cluster_results 고유값: {np.unique(cluster_results)}\") # [0 1 2 3] 이 출력되는지 확인\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d58a2fe5-2768-4309-a8b9-b841765777ee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# t-SNE로 2D 축소\n",
    "tsne_result = TSNE(n_components=2, perplexity=30, random_state=42).fit_transform(pca_array)\n",
    "\n",
    "# Define colors\n",
    "colors = ['b', 'g', 'r', 'c', 'm', 'y', 'k']\n",
    "\n",
    "# 시각화\n",
    "plt.figure(figsize=(10, 8))\n",
    "for cluster_id in range(best_k):\n",
    "    mask = cluster_results == cluster_id\n",
    "    plt.scatter(tsne_result[mask, 0], tsne_result[mask, 1],\n",
    "                c=colors[cluster_id % len(colors)],\n",
    "                label=f'Cluster {cluster_id}', alpha=0.6)\n",
    "\n",
    "plt.title('t-SNE 기반 클러스터 시각화')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3521d63c-b499-4039-b813-489c67e8f6d4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# best_k 변수와 cluster_results의 내용물을 직접 확인합니다.\n",
    "print(f\"시각화에 사용된 best_k: {best_k}\")\n",
    "\n",
    "unique_clusters, counts = np.unique(cluster_results, return_counts=True)\n",
    "\n",
    "print(\"\\n'cluster_results' 변수의 내용:\")\n",
    "for cluster_id, count in zip(unique_clusters, counts):\n",
    "    print(f\"  - 클러스터 {cluster_id}: {count} 개\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2a23edc9-6631-4bab-a90d-18bb6072186a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### 9. 결과"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2188fb42-da65-4a7d-a0ad-a976c9ebaa0a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 클러스터별 데이터 분포\n",
    "cluster_counts = df_final_result.groupBy(\"Prediction\").count().orderBy(\"Prediction\")\n",
    "cluster_counts.show()\n",
    "\n",
    "# 클러스터별 주요 특성 분석\n",
    "agg_exprs = []\n",
    "for col in numeric_cols[:10]:  # 주요 컬럼 10개만\n",
    "    agg_exprs.append(F.mean(col).alias(f\"avg_{col}\"))\n",
    "    agg_exprs.append(F.stddev(col).alias(f\"std_{col}\"))\n",
    "\n",
    "cluster_profiles = df_final_result.groupBy(\"Prediction\").agg(*agg_exprs)\n",
    "cluster_profiles.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2a75c37b-572c-4f82-8f3c-f791c3ddf841",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### 10. 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7f4867b0-881c-4d47-ab15-539b45259be3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA as SklearnPCA\n",
    "\n",
    "# 2D 시각화를 위한 PCA\n",
    "pca_2d = SklearnPCA(n_components=3)\n",
    "pca_2d_result = pca_2d.fit_transform(pca_array)\n",
    "\n",
    "# 클러스터별 색상으로 시각화\n",
    "plt.figure(figsize=(10, 8))\n",
    "colors = ['red', 'blue', 'green', 'orange', 'purple', 'brown', 'pink']\n",
    "\n",
    "for cluster_id in range(best_k):\n",
    "    mask = cluster_results == cluster_id\n",
    "    plt.scatter(\n",
    "        pca_2d_result[mask, 0], \n",
    "        pca_2d_result[mask, 1], \n",
    "        c=colors[cluster_id % len(colors)], \n",
    "        label=f'Cluster {cluster_id}',\n",
    "        alpha=0.6\n",
    "    )\n",
    "\n",
    "plt.xlabel('PCA Component 1')\n",
    "plt.ylabel('PCA Component 2')\n",
    "plt.title('소비정보 데이터 클러스터링 결과')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4eb5067a-a7d1-4378-baf0-0993a4fd15ff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import DoubleType\n",
    "from pyspark.ml.linalg import Vector\n",
    "\n",
    "# --- 이 코드를 사용하세요 ---\n",
    "\n",
    "# 1. Vector에서 특정 인덱스의 값을 추출하는 Python 함수를 정의합니다.\n",
    "def get_element(vector, index):\n",
    "    try:\n",
    "        # Vector 타입을 Python 리스트로 변환 후, 해당 인덱스의 값을 반환\n",
    "        return float(vector.toArray()[index])\n",
    "    except (IndexError, AttributeError):\n",
    "        # 혹시 모를 오류 방지\n",
    "        return None\n",
    "\n",
    "# 2. 위 Python 함수를 Spark이 사용할 수 있는 UDF(사용자 정의 함수)로 등록합니다.\n",
    "#    - 첫 번째 인자: 사용할 Python 함수\n",
    "#    - 두 번째 인자: 이 함수가 반환하는 값의 데이터 타입 (실수형)\n",
    "get_element_udf = F.udf(get_element, DoubleType())\n",
    "\n",
    "\n",
    "# 3. UDF를 사용하여 'pc1'과 'pc2' 컬럼을 안전하게 생성합니다.\n",
    "print(\"UDF를 사용하여 각 주성분(PC) 값을 추출합니다...\")\n",
    "df_with_components = df_final_result.withColumn(\"pc1\", get_element_udf(F.col(\"pcaFeatures\"), F.lit(0))) \\\n",
    "                                    .withColumn(\"pc2\", get_element_udf(F.col(\"pcaFeatures\"), F.lit(1)))\n",
    "\n",
    "print(\"pc1, pc2 컬럼 생성 완료!\")\n",
    "\n",
    "\n",
    "# 4. 이제 pc1, pc2 컬럼이 정상적으로 생성되었으므로, 원래 하려던 집계를 실행합니다.\n",
    "print(\"\\n클러스터별 PCA 좌표 범위 계산:\")\n",
    "cluster_bounds = df_with_components.groupBy(\"cluster\") \\\n",
    "    .agg(\n",
    "        F.min(\"pc1\").alias(\"pc1_min\"),\n",
    "        F.max(\"pc1\").alias(\"pc1_max\"),\n",
    "        F.min(\"pc2\").alias(\"pc2_min\"),\n",
    "        F.max(\"pc2\").alias(\"pc2_max\")\n",
    "    ).orderBy(\"cluster\")\n",
    "\n",
    "cluster_bounds.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eb940e5c-0ffa-41ad-8846-7c077b50847e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D  # 3D 플롯을 위한 모듈\n",
    "from sklearn.decomposition import PCA as SklearnPCA\n",
    "\n",
    "# PCA로 3차원 축소\n",
    "pca_3d = SklearnPCA(n_components=3)\n",
    "pca_3d_result = pca_3d.fit_transform(pca_array)\n",
    "\n",
    "# 클러스터별 색상 시각화\n",
    "fig = plt.figure(figsize=(12, 10))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "colors = ['red', 'blue', 'green', 'orange', 'purple', 'brown', 'pink']\n",
    "\n",
    "for cluster_id in range(best_k):\n",
    "    mask = cluster_results == cluster_id\n",
    "    ax.scatter(\n",
    "        pca_3d_result[mask, 0], \n",
    "        pca_3d_result[mask, 1], \n",
    "        pca_3d_result[mask, 2], \n",
    "        c=colors[cluster_id % len(colors)],\n",
    "        label=f'Cluster {cluster_id}',\n",
    "        alpha=0.6\n",
    "    )\n",
    "\n",
    "ax.set_xlabel('PCA Component 1')\n",
    "ax.set_ylabel('PCA Component 2')\n",
    "ax.set_zlabel('PCA Component 3')\n",
    "ax.set_title('소비정보 데이터 클러스터링 결과 (3D)')\n",
    "ax.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "04e68eae-d037-4a0a-890c-4b4609bcc340",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn.decomposition import PCA as SklearnPCA\n",
    "\n",
    "# 2D 시각화를 위한 PCA\n",
    "pca_3d = SklearnPCA(n_components=3)\n",
    "pca_3d_result = pca_3d.fit_transform(pca_array)\n",
    "\n",
    "# 클러스터별 색상으로 시각화\n",
    "fig = plt.figure(figsize=(10, 8))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "colors = ['red', 'blue', 'green', 'orange', 'purple', 'brown', 'pink']\n",
    "\n",
    "for cluster_id in range(best_k):\n",
    "    mask = cluster_results == cluster_id\n",
    "    plt.scatter(\n",
    "        pca_3d_result[mask, 0], \n",
    "        pca_3d_result[mask, 1], \n",
    "        pca_3d_result[mask, 2], \n",
    "        c=colors[cluster_id % len(colors)], \n",
    "        label=f'Cluster {cluster_id}',\n",
    "        alpha=0.6\n",
    "    )\n",
    "\n",
    "ax.set_xlabel('PCA 1')\n",
    "ax.set_ylabel('PCA 2')\n",
    "ax.set_zlabel('PCA 3')\n",
    "plt.title('소비정보 데이터 클러스터링 결과')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7ecbfa34-6814-4250-a06f-5304d829fa5d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bc4f36d3-6ada-412d-a685-320bcdc4e419",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# df_sample은 가장 처음에 불러온 원본 데이터프레임입니다.\n",
    "df_with_id = df_sample.withColumn(\"row_id\", F.monotonically_increasing_id())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c4e6b357-e9a3-4e7f-9d57-5ccb48f7db63",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# 지금까지 사용했던 각 단계의 모델 객체들을 순서대로 리스트에 담습니다.\n",
    "# 각 변수명(assembler, scaler, pca, final_kmeans)은 실제 노트북에서 사용하신 이름으로 확인 후 맞춰주세요.\n",
    "pipeline = Pipeline(stages=[\n",
    "    assembler, \n",
    "    scaler, \n",
    "    pca, \n",
    "    final_kmeans\n",
    "])\n",
    "\n",
    "print(\"ML 파이프라인이 성공적으로 구성되었습니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "827c5062-5e53-4a5d-ab05-9d3c8abc8c41",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 파이프라인 전체를 데이터에 학습(fit)시킵니다.\n",
    "# df_with_id는 원본 데이터에 row_id가 추가된, 가장 초기 단계의 데이터프레임입니다.\n",
    "pipeline_model = pipeline.fit(df_with_id)\n",
    "\n",
    "# 학습된 파이프라인 모델을 원하는 경로에 저장합니다.\n",
    "# 경로는 DBFS(Databricks File System) 경로를 사용하시면 됩니다.\n",
    "# 여기서는 'customer_cluster_pipeline_model'이라는 이름으로 저장하겠습니다.\n",
    "pipeline_path = \"/FileStore/models/database_pjt/customer_cluster_pipeline_model\"\n",
    "pipeline_model.write().overwrite().save(pipeline_path)\n",
    "\n",
    "print(f\"학습된 파이프라인 모델이 여기에 저장되었습니다: {pipeline_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ba84d799-3800-40d2-a593-49a808de83c7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# df_final_result는 클러스터링 결과가 포함된 최종 데이터프레임입니다.\n",
    "# 원하는 테이블 이름을 지정합니다. (예: database_pjt.customer_clusters_final)\n",
    "table_name = \"database_pjt.customer_clusters_final\"\n",
    "\n",
    "# 데이터프레임을 Delta Table로 저장합니다.\n",
    "# .mode(\"overwrite\")는 기존에 같은 이름의 테이블이 있으면 덮어쓰는 옵션입니다.\n",
    "df_final_result.write.format(\"delta\").mode(\"overwrite\").saveAsTable(table_name)\n",
    "\n",
    "print(f\"클러스터링 최종 결과가 Delta Table로 저장되었습니다: {table_name}\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [
    {
     "elements": [
      {
       "dashboardResultIndex": null,
       "elementNUID": "9f8f2832-fc31-432e-aff6-60ce0141c419",
       "elementType": "command",
       "guid": "304ca28c-986f-4a4c-95b1-24ad16904e4d",
       "options": null,
       "position": {
        "height": 13,
        "width": 18,
        "x": 0,
        "y": 0,
        "z": null
       },
       "resultIndex": null
      }
     ],
     "globalVars": {},
     "guid": "",
     "layoutOption": {
      "grid": true,
      "stack": true
     },
     "nuid": "01fe2509-5256-43c1-9e31-29d64d4c5f87",
     "origId": 1371802259337790,
     "title": "Untitled",
     "version": "DashboardViewV1",
     "width": 2560
    }
   ],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "(fin) 3_preprocessing_joinafter",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
