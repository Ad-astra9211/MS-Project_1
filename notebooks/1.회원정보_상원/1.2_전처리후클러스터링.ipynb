{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ae7cfd19-587b-4199-9a2e-22adfcf61ad0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# 1_fina(고객정보데이터) 클러스터링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1b1041ed-36ab-41b9-b85f-e46f6ae87cfb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "!pip install kmeans-pytorch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1bbc511e-9720-475c-bcde-0c538938afec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from kmeans_pytorch import kmeans\n",
    "from pyspark.sql import Row\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d3ddc7d5-933e-46c9-a498-1f1ce43199a0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "df_final = spark.read.format(\"delta\").table(\"database_pjt.1_fina_entire\")\n",
    "df_final.show(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "6ef39561-ed3b-4834-ae92-4f10f822940d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_final.printSchema()\n",
    "print(df_final.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0488a25b-7277-4ac9-9059-0a7b50883dd9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 첫 번째 row의 벡터 길이(차원) 확인\n",
    "df_final.select(\"scaled_log_features\").first()[0].size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "271e318a-455c-4446-8db2-d2d16d6319ce",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#벡터화는 되어있으니 pca는 해야함. 최적의 k값도 찾아야해\n",
    "from pyspark.ml.feature import PCA\n",
    "\n",
    "# k: 축소할 차원 수, inputCol: 벡터 컬럼명, outputCol: 결과 컬럼명\n",
    "pca = PCA(k=5, inputCol=\"scaled_log_features\", outputCol=\"pca_features\")\n",
    "pca_model = pca.fit(df_final)\n",
    "df_final = pca_model.transform(df_final)\n",
    "\n",
    "# 결과 확인\n",
    "df_final.select(\"pca_features\").show(3, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bd5ae90a-0cf1-4400-8c9c-7aa81d9d5a17",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#드라이버 메모리 부담 최소화를 위해 블럭 나눔\n",
    "# (1) PySpark에서 10% 샘플링 후, PCA 벡터만 추출\n",
    "import numpy as np\n",
    "\n",
    "df_final_sample = df_final.sample(fraction=0.1, seed=42)\n",
    "pca_sample_np = np.array([row.pca_features.toArray() for row in df_final_sample.select(\"pca_features\").collect()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c36906bc-1c95-48a6-8f3f-e582d3f61756",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 회원정보 전체 데이터 수 계산 -> 클러스터링 처리 연산 관련 데이터량 추정\n",
    "df_final.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1525d3c6-af9a-4b0d-a130-50112843aee2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# 샘플 데이터만 GPU로 변환\n",
    "pca_sample_tensor = torch.from_numpy(pca_sample_np).float().cuda()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bbfc5073-fa4b-4464-b221-05c02f8b62a4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#kmeans(gpu) 샘플데이터로 클러스터링\n",
    "#이거 하고 최단연결, 최장연결, 평균연결,중심연결 4개 비교도 해보고싶긴한데\n",
    "from kmeans_pytorch import kmeans\n",
    "\n",
    "# 샘플 데이터로 KMeans (유클리드)\n",
    "kmeans_cluster_ids, kmeans_centers = kmeans(\n",
    "    X=pca_sample_tensor,\n",
    "    num_clusters=4,\n",
    "    distance='euclidean',\n",
    "    device=torch.device('cuda')\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "467d7646-24eb-425a-b44e-9cd1a4e768c3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 샘플 데이터로 KMeans (코사인)\n",
    "kmeans2_cluster_ids, kmeans2_centers = kmeans(\n",
    "    X=pca_sample_tensor,\n",
    "    num_clusters=5,\n",
    "    distance='cosine',\n",
    "    device=torch.device('cuda')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4f5861b9-f618-45a7-8da4-1a0e21d88be8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 4. 전체 데이터 예측 (배치 처리 권장, 메모리 부족시 limit 사용)\n",
    "# 4. 전체 데이터 예측 함수/ 데이터가 어느 클러스터 속하는지 예측할당\n",
    "import torch\n",
    "def assign_clusters(X, centers, distance='euclidean'):\n",
    "    centers = centers.to(X.device)\n",
    "    if distance == 'euclidean':\n",
    "        dists = torch.cdist(X, centers)\n",
    "    elif distance == 'cosine':\n",
    "        X_norm = X / X.norm(dim=1, keepdim=True)\n",
    "        centers_norm = centers / centers.norm(dim=1, keepdim=True)\n",
    "        dists = 1 - torch.mm(X_norm, centers_norm.t())\n",
    "    else:\n",
    "        raise ValueError(\"지원하지 않는 distance\")\n",
    "    return torch.argmin(dists, dim=1)\n",
    "\n",
    "# 5. 전체 데이터 배치 처리 (70,000개 제한 예시)\n",
    "df_final_limit = df_final.limit(1000000)\n",
    "pca_features_np = np.array([row.pca_features.toArray() for row in df_final_limit.select(\"pca_features\").collect()])\n",
    "pca_features_tensor = torch.from_numpy(pca_features_np).float().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "9442c455-da87-427a-ba60-8f4d3e6f2a8d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 6. 클러스터 할당 즉 행이 어떤 클러스터 속하는지 실제 계산후 라벨링\n",
    "kmeans_cluster_ids_full = assign_clusters(pca_features_tensor, kmeans_centers, 'euclidean')\n",
    "kmeans2_cluster_ids_full = assign_clusters(pca_features_tensor, kmeans2_centers, 'cosine')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "27a6e9bd-36e6-4270-b61c-09620a36a6e8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 7. 메타 클러스터링/ 유클리드랑 코사인거리등 보고, 통합해서 한번더 군집화\n",
    "meta_features = torch.stack([kmeans_cluster_ids_full, kmeans2_cluster_ids_full], dim=1).float()\n",
    "meta_cluster_ids, meta_centers = kmeans(\n",
    "    X=meta_features,\n",
    "    num_clusters=3,\n",
    "    distance='euclidean',\n",
    "    device=torch.device('cuda'),\n",
    "    tol=1e-4\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "184890fc-c8b8-4ca3-9dbd-1324a9c9c8c0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "클러스터는 시험적으로 3개로 하는건가요 ?\n",
    "5개를 햇었다가 오류났어서 그 오류 수정하는 방식에서 바꿨던거라 \n",
    "숫자를 더 늘려보긴 할겁니다\n",
    "\n",
    "알겠습니다!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "431caa64-7dbc-410d-bbe2-9b842e4b4bea",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(torch.unique(meta_cluster_ids, return_counts=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b5065284-35bb-4ca0-892d-44f98d66b67d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(torch.isnan(meta_features).any(), torch.isinf(meta_features).any())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bdbd835b-faf5-4d2f-ae38-689757f42950",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 8. 결과 변환\n",
    "final_cluster_ids_np = meta_cluster_ids.cpu().numpy().astype(int)\n",
    "rows = [Row(id=idx, final_cluster=int(cluster)) for idx, cluster in enumerate(final_cluster_ids_np)]\n",
    "df_final_cluster = spark.createDataFrame(rows)\n",
    "df_final_cluster.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9106bba7-9ab9-4a59-888c-4ba381355a8c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "# 리미트개수에서 1,0000개만 무작위 샘플링\n",
    "idx = np.random.choice(len(pca_sample_np), 30000, replace=False)\n",
    "sample_np = pca_sample_np[idx]\n",
    "sample_labels = kmeans_cluster_ids.cpu().numpy()[idx]\n",
    "\n",
    "score = silhouette_score(sample_np, sample_labels, metric='euclidean')\n",
    "print(f\"Silhouette Score (Euclidean, 30000개 샘플): {score:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "68eef603-9564-46f2-ba91-9e4a62ae5004",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#전체 코드\n",
    "\n",
    "#드라이버 메모리 부담 최소화를 위해 블럭 나눔\n",
    "# (1) PySpark에서 10% 샘플링 후, PCA 벡터만 추출\n",
    "import numpy as np\n",
    "\n",
    "df_final_sample = df_final.sample(fraction=0.1, seed=42)\n",
    "pca_sample_np = np.array([row.pca_features.toArray() for row in df_final_sample.select(\"pca_features\").collect()])\n",
    "import torch\n",
    "\n",
    "# 샘플 데이터만 GPU로 변환\n",
    "pca_sample_tensor = torch.from_numpy(pca_sample_np).float().cuda()\n",
    "#kmeans(gpu) 샘플데이터로 클러스터링\n",
    "#이거 하고 최단연결, 최장연결, 평균연결,중심연결 4개 비교도 해보고싶긴한데\n",
    "from kmeans_pytorch import kmeans\n",
    "\n",
    "# 샘플 데이터로 KMeans (유클리드)\n",
    "kmeans_cluster_ids, kmeans_centers = kmeans(\n",
    "    X=pca_sample_tensor,\n",
    "    num_clusters=7,\n",
    "    distance='euclidean',\n",
    "    device=torch.device('cuda')\n",
    ")\n",
    "\n",
    "# 샘플 데이터로 KMeans (코사인)\n",
    "kmeans2_cluster_ids, kmeans2_centers = kmeans(\n",
    "    X=pca_sample_tensor,\n",
    "    num_clusters=7,\n",
    "    distance='cosine',\n",
    "    device=torch.device('cuda')\n",
    ")\n",
    "# 4. 전체 데이터 예측 (배치 처리 권장, 메모리 부족시 limit 사용)\n",
    "# 4. 전체 데이터 예측 함수/ 데이터가 어느 클러스터 속하는지 예측할당\n",
    "import torch\n",
    "def assign_clusters(X, centers, distance='euclidean'):\n",
    "    centers = centers.to(X.device)\n",
    "    if distance == 'euclidean':\n",
    "        dists = torch.cdist(X, centers)\n",
    "    elif distance == 'cosine':\n",
    "        X_norm = X / X.norm(dim=1, keepdim=True)\n",
    "        centers_norm = centers / centers.norm(dim=1, keepdim=True)\n",
    "        dists = 1 - torch.mm(X_norm, centers_norm.t())\n",
    "    else:\n",
    "        raise ValueError(\"지원하지 않는 distance\")\n",
    "    return torch.argmin(dists, dim=1)\n",
    "\n",
    "# 5. 전체 데이터 배치 처리 (10,000개 제한 예시)\n",
    "#df_final_limit = df_final.limit(10000)\n",
    "#pca_features_np = np.array([row.pca_features.toArray() for row in df_final_limit.select(\"pca_features\").collect()])\n",
    "#pca_features_tensor = torch.from_numpy(pca_features_np).float().cuda()\n",
    "# 6. 클러스터 할당 즉 행이 어떤 클러스터 속하는지 실제 계산후 라벨링\n",
    "kmeans_cluster_ids_full = assign_clusters(pca_features_tensor, kmeans_centers, 'euclidean')\n",
    "kmeans2_cluster_ids_full = assign_clusters(pca_features_tensor, kmeans2_centers, 'cosine')\n",
    "\n",
    "# 7. 메타 클러스터링/ 유클리드랑 코사인거리등 보고, 통합해서 한번더 군집화\n",
    "meta_features = torch.stack([kmeans_cluster_ids_full, kmeans2_cluster_ids_full], dim=1).float()\n",
    "meta_cluster_ids, meta_centers = kmeans(\n",
    "    X=meta_features,\n",
    "    num_clusters=3,\n",
    "    distance='euclidean',\n",
    "    device=torch.device('cuda'),\n",
    "    tol=1e-4\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ea020078-90c1-4046-82db-82473df2cb7c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from kmeans_pytorch import kmeans\n",
    "from pyspark.sql import Row\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# 1. 10% 샘플링 후, PCA 벡터만 추출\n",
    "df_final_sample = df_final.sample(fraction=0.1, seed=42)\n",
    "pca_sample_np = np.array([row.pca_features.toArray() for row in df_final_sample.select(\"pca_features\").collect()])\n",
    "pca_sample_tensor = torch.from_numpy(pca_sample_np).float().cuda()\n",
    "\n",
    "# 2. 샘플 데이터로 KMeans (유클리드 거리만)\n",
    "kmeans_cluster_ids, kmeans_centers = kmeans(\n",
    "    X=pca_sample_tensor,\n",
    "    num_clusters=5,  # 원하는 군집 개수\n",
    "    distance='euclidean',\n",
    "    device=torch.device('cuda')\n",
    ")\n",
    "\n",
    "# 3. 전체 데이터(10,000개 제한) 클러스터 할당\n",
    "df_final_limit = df_final.limit(1000000)\n",
    "pca_features_np_limit = np.array([row.pca_features.toArray() for row in df_final_limit.select(\"pca_features\").collect()])\n",
    "pca_features_tensor_limit = torch.from_numpy(pca_features_np_limit).float().cuda()\n",
    "\n",
    "# 유클리드 거리로 각 행의 클러스터 할당\n",
    "kmeans_cluster_ids_full = torch.argmin(torch.cdist(pca_features_tensor_limit, kmeans_centers.to(pca_features_tensor_limit.device)), dim=1)\n",
    "\n",
    "# 4. 결과 변환\n",
    "final_cluster_ids_np = kmeans_cluster_ids_full.cpu().numpy().astype(int)\n",
    "rows = [Row(id=idx, final_cluster=int(cluster)) for idx, cluster in enumerate(final_cluster_ids_np)]\n",
    "df_final_cluster = spark.createDataFrame(rows)\n",
    "df_final_cluster.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c084f21d-fdb2-458d-9cf1-405d00c9f25c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "# 리미트개수에서 1,0000개만 무작위 샘플링\n",
    "idx = np.random.choice(len(pca_sample_np), 30000, replace=False)\n",
    "sample_np = pca_sample_np[idx]\n",
    "sample_labels = kmeans_cluster_ids.cpu().numpy()[idx]\n",
    "\n",
    "score = silhouette_score(sample_np, sample_labels, metric='euclidean')\n",
    "print(f\"Silhouette Score (Euclidean, 30000개 샘플): {score:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "78cbdf92-b379-45d3-8d2a-e64a4f02920a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 분석 방향성"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0eefebbf-5ee0-4727-9896-02cdf44b9e88",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "- 군집의 분포 및 특성 분석\n",
    " \n",
    "군집별 데이터 수(분포)\n",
    "각 군집에 데이터가 얼마나 분포되어 있는지 확인. 한쪽 군집에 쏠림이 심하면 해석에 주의해야 합니다.\n",
    " \n",
    "군집 중심(centroid) 값 해석\n",
    "각 군집의 중심값(평균 벡터)을 확인하여, 군집별 대표적 특성(변수별 평균, 패턴 등)을 파악합니다.\n",
    " \n",
    "군집별 주요 변수 요약\n",
    "군집별로 주요 피처(변수)의 평균, 분산, 범위 등을 요약해 군집의 특성을 설명합니다.\n",
    " \n",
    "- 군집 간/내 거리 및 분리도\n",
    " \n",
    "군집 내 거리(Compactness)\n",
    "각 군집 내부의 데이터들이 얼마나 밀집해 있는지(=군집 내 거리의 평균/분산).\n",
    " \n",
    "군집 간 거리(Separation)\n",
    "서로 다른 군집 중심점(centroid) 간의 거리. 군집 간 거리가 크면 클수록 군집이 잘 분리된 것."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "341290d0-ee6a-4e66-a387-523e88b59578",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 시각화"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e6ce0a1a-c874-480e-b209-5415921f3f15",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    " \n",
    "- 차원 축소 후 시각화(PCA) 2~3차원으로 차원 축소 후, 군집별로 색을 달리해 데이터 분포와 군집 경계를 시각적으로 확인합니다.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5df0cd15-16f4-4f2a-9830-d9728fdfb649",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "pca_features_np_limit_scaled = scaler.fit_transform(pca_features_np_limit)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e083c8c3-1b04-44fc-921b-68466ebbddc0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from pyspark.sql.functions import col\n",
    "from sklearn.decomposition import PCA\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "\n",
    "# 클러스터링 결과를 원본 df_final_limit와 join\n",
    "df_with_cluster = df_final_limit.withColumn(\"row_id\", F.monotonically_increasing_id())\n",
    "df_final_cluster_with_index = df_final_cluster.withColumnRenamed(\"id\", \"row_id\")\n",
    "df_joined = df_with_cluster.join(df_final_cluster_with_index, on=\"row_id\")\n",
    "\n",
    "# PCA 2D 적용을 위한 numpy array 추출\n",
    "pca_features_np_vis = np.array([row.pca_features.toArray() for row in df_joined.select(\"pca_features\").collect()])\n",
    "cluster_ids_vis = np.array([row.final_cluster for row in df_joined.select(\"final_cluster\").collect()])\n",
    "\n",
    "# PCA 차원 축소 (2D)\n",
    "pca_2d = PCA(n_components=2)\n",
    "pca_2d_result = pca_2d.fit_transform(pca_features_np_vis)\n",
    "\n",
    "# 시각화\n",
    "plt.figure(figsize=(10, 7))\n",
    "for cluster_id in np.unique(cluster_ids_vis):\n",
    "    idxs = cluster_ids_vis == cluster_id\n",
    "    plt.scatter(pca_2d_result[idxs, 0], pca_2d_result[idxs, 1], label=f'Cluster {cluster_id}', alpha=0.6)\n",
    "\n",
    "plt.title('PCA 2D Visualization of Clusters')\n",
    "plt.xlabel('PCA 1')\n",
    "plt.ylabel('PCA 2')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0a568b53-3dac-463f-ae4f-61266e8242f5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "from mpl_toolkits.mplot3d import Axes3D  # 꼭 필요합니다\n",
    "\n",
    "# pca_features_np_limit: (1000000, n_features)\n",
    "# final_cluster_ids_np: (1000000,)\n",
    "\n",
    "# 1. 3차원 PCA로 축소\n",
    "pca_3d = PCA(n_components=3)\n",
    "pca_3d_result = pca_3d.fit_transform(pca_features_np_limit)\n",
    "\n",
    "# 2. 3만 개 샘플링\n",
    "sample_idx = np.random.choice(len(pca_3d_result), 30000, replace=False)\n",
    "pca_3d_sample = pca_3d_result[sample_idx]\n",
    "cluster_sample = final_cluster_ids_np[sample_idx]\n",
    "\n",
    "# 3. 시각화\n",
    "colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd']\n",
    "\n",
    "fig = plt.figure(figsize=(10, 8))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "for cluster_id in np.unique(cluster_sample):\n",
    "    idx = (cluster_sample == cluster_id)\n",
    "    ax.scatter(\n",
    "        pca_3d_sample[idx, 0],\n",
    "        pca_3d_sample[idx, 1],\n",
    "        pca_3d_sample[idx, 2],\n",
    "        s=10,\n",
    "        color=colors[cluster_id % len(colors)],\n",
    "        label=f'Cluster {cluster_id}',\n",
    "        alpha=0.6\n",
    "    )\n",
    "\n",
    "ax.set_title('3D PCA Scatter Plot by Cluster')\n",
    "ax.set_xlabel('PCA Component 1')\n",
    "ax.set_ylabel('PCA Component 2')\n",
    "ax.set_zlabel('PCA Component 3')\n",
    "ax.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "28f3d4a0-6b7a-4f32-a68a-09a37254ce43",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "3D PCA 군집 시각화 해석\n",
    "\n",
    "이 그래프는 전체 데이터(100만 건) 중 3만 건을 무작위로 샘플링한 뒤,  \n",
    "PCA로 3차원 축소하여 각 군집별로 색을 다르게 표시한 3D 산점도입니다.\n",
    "\n",
    "---\n",
    "\n",
    "주요 해석\n",
    "\n",
    "- **군집 0 (파란색)**\n",
    "  - 3D 공간에서 가장 넓고 오른쪽 영역에 분포\n",
    "  - 데이터가 많이 몰려 있고, 퍼짐이 큼\n",
    "  - 전체적으로 주요 분산 방향을 따라 넓게 펼쳐져 있음\n",
    "\n",
    "- **군집 1 (주황색)**\n",
    "  - 위쪽 평면에 띠 형태로 뚜렷하게 분리되어 있음\n",
    "  - 다른 군집과 거의 중첩 없이 독립적으로 존재\n",
    "  - 응집도가 높고, 경계가 명확함\n",
    "\n",
    "- **군집 2 (초록색), 군집 3 (빨간색), 군집 4 (보라색)**\n",
    "  - 왼쪽 및 중앙 하단 영역에 위치\n",
    "  - 군집 2(초록)와 군집 3(빨강)은 나란히 붙어 있고, 군집 4(보라)는 이들과 일부 중첩됨\n",
    "  - 군집 3은 특히 한정된 공간에 조밀하게 모여 있음\n",
    "\n",
    "---\n",
    "\n",
    "전체적 분포 및 군집 품질\n",
    "\n",
    "- **군집 간 분리도(Separation)**\n",
    "  - 3D 공간에서 각 군집이 명확히 분리되어 있음\n",
    "  - 군집 0과 군집 1은 서로 뚜렷하게 떨어져 있음\n",
    "  - 군집 2, 3, 4는 경계가 일부 맞닿거나 중첩되어 있지만, 대체로 구분 가능\n",
    "\n",
    "- **군집 내 응집도(Compactness)**\n",
    "  - 군집 3, 2는 내부 데이터가 조밀하게 모여 있음\n",
    "  - 군집 0, 1은 상대적으로 넓게 퍼져 있음\n",
    "\n",
    "- **분석 결론**\n",
    "  - 고차원 데이터의 군집 구조가 3D에서도 잘 보존됨\n",
    "  - 군집별 특성이 뚜렷하게 구분되는 구조\n",
    "  - 군집 0(파랑)이 가장 큰 집단이며, 군집 1(주황)은 명확한 띠 형태, 나머지 군집은 각각 특정 영역에 분포\n",
    "\n",
    "\n",
    "\n",
    "- 3D PCA 시각화는 2D보다 군집 간 분리와 내부 구조를 더 잘 보여줌\n",
    "- 군집별 특성(프로파일링) 분석을 통해 각 군집이 실제로 어떤 속성을 갖는지 추가 해석 필요\n",
    "- PCA는 데이터의 주요 분산 방향만 보존하므로, 실제 고차원 구조와 일부 차이가 있을 수 있음\n",
    "\n",
    "---\n",
    "\n",
    "**요약:**  \n",
    "각 군집이 3D 공간에서 명확하게 분리되어 있고, 군집별 분포와 경계가 뚜렷하게 나타남.  \n",
    "군집 0이 가장 크고, 군집 1은 띠 형태로 독립적, 군집 2·3·4는 일부 중첩되어 있으나 대체로 구분 가능.  \n",
    "전체적으로 군집화 품질이 우수하게 시각화된 결과임.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f5aba432-33c5-40e1-966e-8533431f7f45",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    " \n",
    "# pca_features_np_limit: (1000000, n_features) - 전체 데이터의 PCA 벡터 (numpy 배열)\n",
    "# final_cluster_ids_np: (1000000,) - 각 데이터의 군집 ID (numpy 배열)\n",
    " \n",
    "# 1. 2차원 PCA로 추가 축소\n",
    "pca_2d = PCA(n_components=2)\n",
    "pca_2d_result = pca_2d.fit_transform(pca_features_np_limit)\n",
    " \n",
    "# 2. 3만개 샘플링 (시각화 성능 및 가독성 위해)\n",
    "sample_idx = np.random.choice(len(pca_2d_result), 30000, replace=False)\n",
    "pca_2d_sample = pca_2d_result[sample_idx]\n",
    "cluster_sample = final_cluster_ids_np[sample_idx]\n",
    "\n",
    "# PCA 1, 2에 가장 기여한 상위 3개 feature 추출\n",
    "top_features_pc1 = np.argsort(np.abs(pca_2d.components_[0]))[::-1][:3]\n",
    "top_features_pc2 = np.argsort(np.abs(pca_2d.components_[1]))[::-1][:3]\n",
    "\n",
    "print(\"PCA Component 1 주요 feature:\", [feature_names[i] for i in top_features_pc1])\n",
    "print(\"PCA Component 2 주요 feature:\", [feature_names[i] for i in top_features_pc2])\n",
    " \n",
    "xlabel = \"PCA Component 1\\n(Main: \" + \", \".join([feature_names[i] for i in top_features_pc1]) + \")\"\n",
    "ylabel = \"PCA Component 2\\n(Main: \" + \", \".join([feature_names[i] for i in top_features_pc2]) + \")\"\n",
    "\n",
    "# 3. 군집별 색상 지정\n",
    "colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd']\n",
    " \n",
    "plt.figure(figsize=(10, 8))\n",
    "for cluster_id in np.unique(cluster_sample):\n",
    "    idx = (cluster_sample == cluster_id)\n",
    "    plt.scatter(\n",
    "        pca_2d_sample[idx, 0],\n",
    "        pca_2d_sample[idx, 1],\n",
    "        s=10,\n",
    "        color=colors[cluster_id],\n",
    "        label=f'Cluster {cluster_id}',\n",
    "        alpha=0.6\n",
    "    )\n",
    " \n",
    "plt.title('2D PCA Scatter Plot by Cluster')\n",
    "plt.xlabel(xlabel)\n",
    "plt.ylabel(ylabel)\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    " \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fca292d8-f659-4874-ae8b-0b9f4fff7c20",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pip install umap-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4d919001-8106-46ca-bb2d-7ba5e7a49c1c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pyspark.sql.functions import col\n",
    "import umap.umap_ as umap  # pip install umap-learn\n",
    "\n",
    "# 1. PySpark DataFrame에서 NumPy 배열로 변환\n",
    "pca_features_np_vis = np.array([row.pca_features.toArray() for row in df_joined.select(\"pca_features\").collect()])\n",
    "cluster_ids_vis = np.array([row.final_cluster for row in df_joined.select(\"final_cluster\").collect()])\n",
    "\n",
    "# 2. UMAP 적용 (2차원으로 차원 축소)\n",
    "umap_2d = umap.UMAP(n_components=2)\n",
    "umap_2d_result = umap_2d.fit_transform(pca_features_np_vis)\n",
    "\n",
    "# 3. 시각화\n",
    "plt.figure(figsize=(10, 7))\n",
    "for cluster_id in np.unique(cluster_ids_vis):\n",
    "    idxs = cluster_ids_vis == cluster_id\n",
    "    plt.scatter(umap_2d_result[idxs, 0], umap_2d_result[idxs, 1], label=f'Cluster {cluster_id}', alpha=0.6)\n",
    "\n",
    "plt.title('UMAP 2D Visualization of Clusters')\n",
    "plt.xlabel('UMAP 1')\n",
    "plt.ylabel('UMAP 2')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "23029aa8-d5e7-478d-b1a8-7688f7670731",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "- 군집별 대표 샘플 추출\n",
    "각 군집 중심에 가장 가까운 데이터(대표 샘플)를 뽑아 실제 특성을 확인합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "162312d6-33fa-4a61-b594-243e7dbe926a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 클러스터 중심 좌표\n",
    "centers_np = kmeans_centers.cpu().numpy()\n",
    "\n",
    "# 모든 데이터 벡터\n",
    "features_np_all = pca_features_tensor_limit.cpu().numpy()\n",
    "\n",
    "# 각 샘플과 중심 거리 계산\n",
    "distances = np.linalg.norm(features_np_all[:, None, :] - centers_np[None, :, :], axis=2)\n",
    "\n",
    "# 가장 가까운 점 인덱스 추출 (각 클러스터별)\n",
    "representative_idxs = np.argmin(distances, axis=0)  # shape: (num_clusters,)\n",
    "\n",
    "# Spark DataFrame에서 대표 샘플 추출\n",
    "df_with_row_idx = df_final_limit.withColumn(\"row_id\", F.monotonically_increasing_id())\n",
    "representative_rows = df_with_row_idx.filter(F.col(\"row_id\").isin([int(i) for i in representative_idxs]))\n",
    "\n",
    "display(representative_rows.show(truncate=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ade763e6-5361-4aa0-850b-6be013ce88e5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 결과분석"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4424e144-0b71-4355-8e13-c2ccb1011cc6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 군집화 품질 평가\n",
    "\n",
    "데이터 분포 확인 및 군집 내 / 외 거리, 실루엣 점수 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6af7f946-d11c-40ae-9612-2d11f85fb1fa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# final_cluster_ids_np는 이미 100만개 데이터에 대해 군집 ID가 할당된 numpy 배열\n",
    "# 군집 개수는 5로 가정\n",
    "unique, counts = np.unique(final_cluster_ids_np, return_counts=True)\n",
    "for cluster_id, count in zip(unique, counts):\n",
    "    print(f\"군집 {cluster_id}: {count}개\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "23371ee6-b733-4841-ba5c-4356e7d71e38",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    " 1. 군집별 데이터 분포\n",
    "- **가장 큰 군집**: 군집 0 (약 32만 개, 전체의 약 32%)\n",
    "- **가장 작은 군집**: 군집 1 (약 8만 개, 전체의 약 8%)\n",
    "- **나머지 군집**: 군집 2, 3, 4는 각각 16~22% 비중을 차지\n",
    "---\n",
    " 2. 해석\n",
    "- **불균형 분포**  \n",
    "  군집별 데이터 개수가 균등하지 않고, 군집 0에 데이터가 많이 몰려 있음.  \n",
    "  군집 1은 상대적으로 매우 적은 데이터가 할당됨.\n",
    "- **의미**  \n",
    "  - 군집 0은 데이터에서 가장 흔한(대표적인) 특성을 가진 집단일 가능성이 높음\n",
    "  - 군집 1은 특이하거나 극단적인 특성을 가진 소수 집단일 수 있음\n",
    "  - 군집 2, 3, 4는 중간 규모의 집단으로, 각각 다른 특성의 하위 그룹일 가능성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "7bec8adc-9f5b-48b8-867d-92a231e6d489",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# kmeans_centers: (5, n_features) 텐서\n",
    "# pca_features_np_limit: (1000000, n_features) numpy 배열\n",
    "# final_cluster_ids_np: (1000000,) numpy 배열\n",
    "\n",
    "# 각 데이터별로 본인 군집의 중심점과 거리 계산\n",
    "distances = np.linalg.norm(\n",
    "    pca_features_np_limit - kmeans_centers.cpu().numpy()[final_cluster_ids_np], axis=1\n",
    ")\n",
    "inertia = np.sum(distances ** 2)\n",
    "print(f\"Inertia(SSE): {inertia:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3bef7b7f-193a-4f45-a08a-238de32ffd9c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "# 군집 간 중심점 거리 행렬\n",
    "center_distances = cdist(kmeans_centers.cpu().numpy(), kmeans_centers.cpu().numpy())\n",
    "print(\"군집 간 중심점 거리 행렬:\")\n",
    "print(center_distances)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "27f6d94b-3fce-4deb-828c-dfd4e854556c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    " 군집 간 중심점 거리 행렬 해석\n",
    "\n",
    "|       | 군집 0 | 군집 1 | 군집 2 | 군집 3 | 군집 4 |\n",
    "|-------|--------|--------|--------|--------|--------|\n",
    "| 군집 0| 0.00   | 4.94   | 3.37   | 3.01   | 3.29   |\n",
    "| 군집 1| 4.94   | 0.00   | 3.88   | 4.08   | 3.14   |\n",
    "| 군집 2| 3.37   | 3.88   | 0.00   | 1.95   | 2.13   |\n",
    "| 군집 3| 3.01   | 4.08   | 1.95   | 0.00   | 1.60   |\n",
    "| 군집 4| 3.29   | 3.14   | 2.13   | 1.60   | 0.00   |\n",
    "\n",
    "---\n",
    "\n",
    "#### 1. 해석\n",
    "\n",
    "- **군집 중심점 간 거리**는 각 군집의 중심(centroid) 벡터들 사이의 유클리드 거리입니다.\n",
    "- **가장 멀리 떨어진 군집**: 군집 0과 군집 1 (4.94)\n",
    "- **가장 가까운 군집**: 군집 3과 군집 4 (1.60), 군집 2와 군집 3 (1.95)\n",
    "- **의미**:  \n",
    "  - 거리가 클수록 해당 군집들이 데이터 특성상 명확하게 분리되어 있다는 뜻.\n",
    "  - 거리가 작을수록 두 군집이 비슷한 특성을 가질 가능성이 높음.\n",
    "---\n",
    "\n",
    "#### 2. 활용 방향\n",
    "\n",
    "- 군집 간 거리가 충분히 크면 군집화가 잘 분리된 것.\n",
    "- 거리가 작은 군집 쌍(예: 군집 3-4, 2-3)은 특성이 유사할 수 있으니 군집별 특성 분석 시 주의.\n",
    "- 군집별 대표 특성, 변수 평균 등을 추가로 비교하면 더 명확한 해석 가능.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5f17d109-43df-4699-aebb-b2ab14514db8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "for cluster_id in range(5):\n",
    "    idxs = np.where(final_cluster_ids_np == cluster_id)[0]\n",
    "    cluster_points = pca_features_np_limit[idxs]\n",
    "    center = kmeans_centers.cpu().numpy()[cluster_id]\n",
    "    mean_dist = np.mean(np.linalg.norm(cluster_points - center, axis=1))\n",
    "    print(f\"군집 {cluster_id} 내 평균 거리: {mean_dist:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d4bd7670-a068-488e-b7a9-0e0f22f9299d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "# pca_features_np_limit: (1000000, n_features)\n",
    "# final_cluster_ids_np: (1000000,)\n",
    "sample_idx = np.random.choice(len(pca_features_np_limit), 30000, replace=False)\n",
    "sample_np = pca_features_np_limit[sample_idx]\n",
    "sample_labels = final_cluster_ids_np[sample_idx]\n",
    "score = silhouette_score(sample_np, sample_labels, metric='euclidean')\n",
    "print(f\"Silhouette Score (Euclidean, 3만개 샘플): {score:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "97aa3f38-9f1a-435f-a46e-0b1755ec062a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "\n",
    "# pca_features_np_limit: (1000000, n_features) - 전체 데이터의 PCA 벡터 (numpy 배열)\n",
    "# final_cluster_ids_np: (1000000,) - 각 데이터의 군집 ID (numpy 배열)\n",
    "\n",
    "# 1. 2차원 PCA로 추가 축소\n",
    "pca_2d = PCA(n_components=2)\n",
    "pca_2d_result = pca_2d.fit_transform(pca_features_np_limit)\n",
    "\n",
    "# 2. 3만개 샘플링 (시각화 성능 및 가독성 위해)\n",
    "sample_idx = np.random.choice(len(pca_2d_result), 30000, replace=False)\n",
    "pca_2d_sample = pca_2d_result[sample_idx]\n",
    "cluster_sample = final_cluster_ids_np[sample_idx]\n",
    "\n",
    "# 3. 군집별 색상 지정\n",
    "colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd']\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "for cluster_id in np.unique(cluster_sample):\n",
    "    idx = (cluster_sample == cluster_id)\n",
    "    plt.scatter(\n",
    "        pca_2d_sample[idx, 0], \n",
    "        pca_2d_sample[idx, 1], \n",
    "        s=10, \n",
    "        color=colors[cluster_id], \n",
    "        label=f'Cluster {cluster_id}', \n",
    "        alpha=0.6\n",
    "    )\n",
    "\n",
    "plt.title('2D PCA Scatter Plot by Cluster')\n",
    "plt.xlabel('PCA Component 1')\n",
    "plt.ylabel('PCA Component 2')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6b798aae-b8b9-49f7-9c39-2855d7f8d284",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "2D PCA 군집 시각화 해석\n",
    "이 그래프는 전체 데이터(100만 건)에서 3만 건을 무작위로 샘플링한 뒤,  \n",
    "PCA로 2차원 축소하여 각 군집(클러스터)별로 색을 다르게 표시한 산점도입니다.\n",
    "\n",
    "주요 해석\n",
    "- **군집 0 (파란색)**  \n",
    "  - 오른쪽에 가장 넓게 분포\n",
    "  - 데이터가 가장 많이 몰려 있고, 분산(퍼짐)도 큼\n",
    "  - PCA 주성분 기준으로, 이 군집이 가장 큰 집단임을 시각적으로 확인 가능\n",
    "\n",
    "- **군집 1 (주황색)**  \n",
    "  - 왼쪽 상단에 뚜렷하게 분리되어 있음\n",
    "  - 경계가 명확하고, 비교적 응집도가 높음\n",
    "  - 다른 군집과 중첩이 거의 없음\n",
    "\n",
    "- **군집 2 (초록색)**  \n",
    "  - 왼쪽 하단에 위치\n",
    "  - 데이터가 선형적으로 퍼진 형태가 보임\n",
    "  - 군집 1과 일부 경계가 맞닿아 있지만, 군집 0과는 명확히 분리\n",
    "\n",
    "- **군집 3 (빨간색)**  \n",
    "  - 중앙 하단에 위치\n",
    "  - 비교적 작은 영역에 조밀하게 모여 있음\n",
    "  - 군집 2, 4와 경계가 맞닿아 있음\n",
    "\n",
    "- **군집 4 (보라색)**  \n",
    "  - 중앙~좌상단에 분포\n",
    "  - 군집 1, 2, 3과 경계가 일부 중첩\n",
    "  - 분산이 크고, 다양한 방향으로 퍼져 있음\n",
    "\n",
    "---\n",
    "전체적 분포 및 군집 품질\n",
    "\n",
    "- **군집 간 분리도(Separation)**  \n",
    "  - 각 군집은 PCA 2D 공간에서 명확히 분리되어 있음\n",
    "  - 특히 군집 0(파랑)과 군집 1(주황)은 서로 뚜렷하게 떨어져 있음\n",
    "\n",
    "- **군집 내 응집도(Compactness)**  \n",
    "  - 군집 3, 2는 내부 데이터가 조밀하게 모여 있음\n",
    "  - 군집 0, 4는 상대적으로 퍼져 있음\n",
    "\n",
    "- **경계 중첩**  \n",
    "  - 군집 4는 여러 군집과 경계가 일부 겹침. PCA 축소 과정에서 정보 손실로 인한 현상일 수 있음\n",
    "\n",
    "- **분석 결론**  \n",
    "  - 고차원 데이터의 군집 구조가 2D에서도 잘 보존됨\n",
    "  - 군집별로 특성이 뚜렷하게 구분되는 구조\n",
    "  - 군집 0(파랑)이 가장 큰 집단이고, 군집 1(주황), 2(초록), 3(빨강), 4(보라)는 각각 특정 영역에 분포\n",
    "\n",
    "---\n",
    "- PCA는 데이터의 주요 분산 방향만 2D로 투영하므로, 실제 고차원 구조와 일부 차이가 있을 수 있음\n",
    "- 군집별 특성(프로파일링) 분석을 통해 각 군집이 실제로 어떤 속성을 갖는지 추가 해석 필요\n",
    "**요약:**  \n",
    "각 군집이 2D 공간에서 명확하게 분리되어 있고, 군집별 분포와 경계가 뚜렷하게 나타남.  \n",
    "군집 0이 가장 크고, 군집 1과 2는 경계가 명확, 군집 4는 여러 군집과 일부 중첩되어 있음.  \n",
    "전체적으로 군집화 품질이 우수하게 시각화된 결과임.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "271937b5-61e9-46eb-923a-6388e842ab19",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=['log_입회경과개월수_신용', 'log__1순위카드이용금액', 'log__1순위카드이용건수', 'log__2순위카드이용금액', 'log__2순위카드이용건수'],\n",
    "    outputCol='scaled_log_features'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "19acab51-f830-4846-8d5f-57859f818206",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# PCA 모델의 주성분 가중치 행렬 추출 (k=5, n_features=5)\n",
    "pc_weights = pca_model.pc.toArray()  # shape: (5, 5)\n",
    "\n",
    "# 실제 inputCols 변수명과 매칭\n",
    "input_cols = ['log_입회경과개월수_신용', 'log__1순위카드이용금액', 'log__1순위카드이용건수', 'log__2순위카드이용금액', 'log__2순위카드이용건수']\n",
    "\n",
    "for i, component in enumerate(pc_weights):\n",
    "    print(f\"\\nPCA Component {i+1}:\")\n",
    "    for var, weight in zip(input_cols, component):\n",
    "        print(f\"{var}: {weight:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "25f3ff1c-9bae-4f79-b565-45d495ecd9f9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "# 군집 중심점 (kmeans_centers): (num_clusters, n_features)\n",
    "# 전체 데이터의 PCA 벡터 (pca_features_np_limit): (n_samples, n_features)\n",
    "# 각 데이터의 군집 ID (final_cluster_ids_np): (n_samples,)\n",
    "\n",
    "num_clusters = kmeans_centers.shape[0]\n",
    "\n",
    "# 1. 군집 내 거리(Compactness): 각 군집별로 중심점과의 평균 거리\n",
    "intra_distances = []\n",
    "for cluster_id in range(num_clusters):\n",
    "    idxs = np.where(final_cluster_ids_np == cluster_id)[0]\n",
    "    cluster_points = pca_features_np_limit[idxs]\n",
    "    center = kmeans_centers.cpu().numpy()[cluster_id]\n",
    "    # 각 점과 중심점 사이의 거리\n",
    "    dists = np.linalg.norm(cluster_points - center, axis=1)\n",
    "    mean_dist = np.mean(dists)\n",
    "    std_dist = np.std(dists)\n",
    "    intra_distances.append((cluster_id, mean_dist, std_dist))\n",
    "    print(f\"군집 {cluster_id} 내 평균 거리: {mean_dist:.4f}, 표준편차: {std_dist:.4f}\")\n",
    "\n",
    "# 2. 군집 간 거리(Separation): 중심점들 사이의 거리 행렬\n",
    "center_distances = cdist(kmeans_centers.cpu().numpy(), kmeans_centers.cpu().numpy())\n",
    "print(\"\\n군집 간 중심점 거리 행렬:\")\n",
    "print(center_distances)\n",
    "\n",
    "# 3. 군집 간 최소/최대/평균 거리 요약\n",
    "upper_tri = center_distances[np.triu_indices(num_clusters, k=1)]\n",
    "print(f\"\\n군집 간 최소 거리: {upper_tri.min():.4f}\")\n",
    "print(f\"군집 간 최대 거리: {upper_tri.max():.4f}\")\n",
    "print(f\"군집 간 평균 거리: {upper_tri.mean():.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8ba1d9b0-4082-483d-92fe-0f6ea57171ee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "91a22fa1-ce7e-441e-afa1-68bbfd2eaf1f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(df_with_cluster.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c0afa02a-0227-4442-9df3-92d9d7bbc72b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.types import DoubleType, IntegerType, LongType, FloatType\n",
    "\n",
    "# 1. row_id 컬럼이 없으면 생성\n",
    "if 'row_id' not in df_final_limit.columns:\n",
    "    df_final_limit = df_final_limit.withColumn('row_id', F.monotonically_increasing_id())\n",
    "\n",
    "# 2. 군집 ID DataFrame 생성\n",
    "rows = [Row(row_id=int(idx), cluster=int(cluster)) for idx, cluster in enumerate(final_cluster_ids_np)]\n",
    "df_cluster = spark.createDataFrame(rows)\n",
    "\n",
    "# 3. row_id로 join\n",
    "df_with_cluster = df_final_limit.join(df_cluster, on='row_id')\n",
    "\n",
    "# 4. 수치형 컬럼 자동 추출 (row_id, cluster 제외)\n",
    "numeric_cols = [\n",
    "    f.name for f in df_with_cluster.schema.fields\n",
    "    if isinstance(f.dataType, (DoubleType, IntegerType, LongType, FloatType))\n",
    "    and f.name not in ['cluster', 'row_id']\n",
    "]\n",
    "\n",
    "# 5. 집계 함수 생성\n",
    "agg_exprs = []\n",
    "for c in numeric_cols:\n",
    "    agg_exprs.append(F.mean(c).alias(f'mean_{c}'))\n",
    "    agg_exprs.append(F.stddev(c).alias(f'std_{c}'))\n",
    "    agg_exprs.append(F.min(c).alias(f'min_{c}'))\n",
    "    agg_exprs.append(F.max(c).alias(f'max_{c}'))\n",
    "\n",
    "# 6. 군집별 통계 집계\n",
    "result = df_with_cluster.groupBy('cluster').agg(*agg_exprs)\n",
    "result.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "0cc66225-20f3-468d-ac22-5de79f4e3b6b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "!pip install tabulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "ba4d66ab-a8d9-446c-b680-3d1c58d2531a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "!pip install fpdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3f3c4084-e616-496b-b5f5-e1b607c65196",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "!pip install reportlab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "e8e5e707-5aed-4a13-b7c9-11d4ebe4a7f8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from tabulate import tabulate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4775bf88-d683-45e6-8deb-2eaad2e27a3a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from reportlab.lib.pagesizes import letter, landscape\n",
    "from reportlab.pdfgen import canvas\n",
    "\n",
    "pdf_result = result.toPandas()\n",
    "\n",
    "c = canvas.Canvas(\"cluster_profile.pdf\", pagesize=landscape(letter))\n",
    "c.setFont(\"Helvetica\", 8)\n",
    "\n",
    "row_height = 15\n",
    "x_offset = 40\n",
    "y_offset = 550\n",
    "col_width = 80\n",
    "\n",
    "# 컬럼명 출력\n",
    "for i, col in enumerate(pdf_result.columns):\n",
    "    c.drawString(x_offset + i * col_width, y_offset, str(col))\n",
    "\n",
    "# 데이터 출력\n",
    "for row_idx, row in enumerate(pdf_result.values):\n",
    "    for col_idx, value in enumerate(row):\n",
    "        c.drawString(x_offset + col_idx * col_width, y_offset - row_height * (row_idx + 1), str(value))\n",
    "\n",
    "c.save()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8478b7a1-7aee-4ae0-acdf-97d880c40132",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pdf_file_path = \"cluster_profile.pdf\"\n",
    "pdf_file_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "10a43efe-488c-4017-862b-737aec6c72f1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "pdf_file_path = \"cluster_profile.pdf\"\n",
    "file_exists = os.path.isfile(pdf_file_path)\n",
    "print(file_exists)  # True면 파일 있음, False면 없음\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8512a25d-7770-48a1-b55a-37797cccb8fe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# 1 클러스터링결과 상세분석\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8c122d6d-ecb5-41f5-bcab-7628e2e09a5e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 📊 군집별 특성 분석 결과 (Clusters Characteristics Analysis)\n",
    "\n",
    "제공된 Spark DataFrame 집계 결과는 K-Means 군집화 이후 각 군집(Cluster)이 어떤 통계적 특성을 가지는지 보여주는 매우 중요한 자료입니다.  \n",
    "이를 통해 각 군집의 '페르소나'를 파악하고 비즈니스적 의미를 도출할 수 있습니다.\n",
    "\n",
    "아래는 각 군집의 주요 특징을 요약하고 해석한 내용입니다.  \n",
    "이 결과를 바탕으로 군집의 의미를 이해하고, 비즈니스 전략에 활용할 수 있습니다.\n",
    "\n",
    "---\n",
    "\n",
    "## 군집 0: **오래된 VIP & 활동성 높은 일반 고객**\n",
    "- **규모:** 가장 큰 규모 (319,498명)\n",
    "- **주요 특징:**\n",
    "  - **오랜 고객:** 입회경과개월수_신용 (평균 84개월), 최종카드발급경과월 (평균 17.5개월) 모두 가장 오래된 고객임을 시사\n",
    "  - **활발한 이용:** 소지카드수_유효_신용 (평균 1.74개), 이용카드수_신용 (평균 2.26개)이 가장 많음\n",
    "  - **높은 지출:** 총이용금액 (평균 339만) 및 관련 log_ 변수들이 다른 군집에 비해 가장 높음\n",
    "  - **낮은 이탈 이력:** 탈회횟수_누적 (평균 0.47회) 등 탈회 이력이 낮은 편\n",
    "  - **VIP 비중:** VIP등급코드_idx (평균 0.56)가 다른 군집에 비해 높음\n",
    "- **해석:**  \n",
    "  오랜 기간 서비스를 이용하며, 활발하게 카드를 소지/이용하고 높은 지출을 하는 충성 고객이 다수 포함된 군집입니다. VIP 고객 비중도 높아 이탈 방지 및 프리미엄 서비스 제공이 중요합니다.\n",
    "\n",
    "---\n",
    "\n",
    "## 군집 1: **저활동/초기 이탈 가능성 고객**\n",
    "- **규모:** 중간 규모 (80,700명)\n",
    "- **주요 특징:**\n",
    "  - **오래된 발급:** 최종카드발급경과월 (평균 25.4개월)은 군집 0, 4보다 길어, 최근에 카드를 발급받았을 가능성이 낮음\n",
    "  - **극도로 낮은 이용:** 이용카드수_신용 (평균 0.014개), 총이용금액 (평균 261원) 모두 매우 낮음\n",
    "  - **일부 탈회 이력:** 탈회횟수_누적 (평균 0.52회) 등 탈회 이력이 있음\n",
    "- **해석:**  \n",
    "  카드를 소지하고 있지만 거의 사용하지 않거나, 과거에 탈회 이력이 있고 현재도 비활성 상태인 고객이 많음. 잠재적 이탈 고객이거나 유령 고객일 가능성이 높으므로, 재활성화 캠페인 또는 효율적 관리를 위한 전략이 필요합니다.\n",
    "\n",
    "---\n",
    "\n",
    "## 군집 2: **신규 고객 또는 빠르게 이탈한 고객**\n",
    "- **규모:** 중간 규모 (217,678명)\n",
    "- **주요 특징:**\n",
    "  - **최근 가입:** 입회경과개월수_신용 (평균 7.78개월), 최종카드발급경과월 (평균 11.3개월) 모두 가장 짧음\n",
    "  - **높은 카드 신청:** 카드신청건수 (평균 0.28건)가 가장 높음\n",
    "  - **높은 탈회 이력:** 탈회횟수_누적 (평균 0.70회), 탈회횟수_발급6개월이내 (평균 0.056회) 등 이탈 이력이 가장 높음\n",
    "- **해석:**  \n",
    "  최근에 가입했거나 카드 발급 후 빠르게 이탈한 이력이 있는 고객들이 많음. 신규 고객이면서도 이탈 위험이 높은 그룹으로, 온보딩 프로세스 강화 및 초기 이탈 방지 프로그램이 시급합니다.\n",
    "\n",
    "---\n",
    "\n",
    "## 군집 3: **적극적인 신규 또는 중급 이용 고객**\n",
    "- **규모:** 중간 규모 (223,079명)\n",
    "- **주요 특징:**\n",
    "  - **오랜 고객 & 중간 이용:** 입회경과개월수_신용 (평균 108.45개월)이 가장 길고, 총이용금액 (평균 198만) 및 관련 log_ 변수들이 군집 0 다음으로 높음\n",
    "  - **가장 낮은 탈회 이력:** 탈회횟수_누적 (평균 0.39회)이 가장 낮음\n",
    "- **해석:**  \n",
    "  오랜 기간 고객이었지만, 군집 0만큼의 이용 빈도나 금액은 아니지만 꾸준히 카드를 이용하는 고객이 많음. 탈회 이력이 가장 적어 비교적 안정적인 고객군으로, VIP 군집(군집 0)으로 성장시킬 수 있는 잠재력이 있습니다.\n",
    "\n",
    "---\n",
    "\n",
    "## 군집 4: **중저가 이용/평범한 고객**\n",
    "- **규모:** 가장 작은 규모 (159,045명)\n",
    "- **주요 특징:**\n",
    "  - **오랜 고객 & 낮은 이용:** 입회경과개월수_신용 (평균 100.67개월)이 군집 3과 유사하게 오래된 고객이나, 총이용금액 (평균 52만) 및 관련 log_ 변수들이 군집 1 다음으로 가장 낮음\n",
    "  - **낮은 탈회 이력:** 탈회횟수_누적 (평균 0.42회)가 군집 3 다음으로 낮음\n",
    "- **해석:**  \n",
    "  오랜 기간 고객이었지만, 카드 이용량이나 금액이 낮은 '평범한' 또는 '잠자는' 고객일 수 있음. 이탈 가능성은 낮지만, 추가적인 가치 창출을 위해 맞춤형 프로모션이나 혜택을 통한 이용 유도가 필요합니다.\n",
    "\n",
    "---\n",
    "\n",
    "## 비즈니스적 시사점 및 다음 단계\n",
    "\n",
    "이러한 군집별 특성 분석은 단순히 숫자를 보는 것을 넘어, 각 군집이 어떤 고객층을 대표하는지 이해하는 데 큰 도움이 됩니다.\n",
    "\n",
    "- **군집 0:** 최우수/충성 고객 → 이탈 방지 및 프리미엄 서비스 유지 전략\n",
    "- **군집 1:** 비활성/이탈 가능성 높은 고객 → 재활성화 캠페인 또는 효율적 고객 관리\n",
    "- **군집 2:** 신규/초기 이탈 위험 고객 → 초기 고객 경험 개선 및 이탈 방지 프로그램 강화\n",
    "- **군집 3:** 안정적인 중급 이용 고객 → VIP로의 성장 유도를 위한 상향 판매/교차 판매 기회 모색\n",
    "- **군집 4:** 저활동 장기 고객 → 추가적인 이용 유도를 위한 맞춤형 혜택 제공\n",
    "\n",
    "이 분석 결과는 이전에 PCA 시각화에서 보았던 군집 4의 넓은 분포(그리고 데이터 개수는 가장 크지 않음)가, 실제로 이 군집이 '다양한' 특성을 가진 '평범한' 고객을 포함하고 있기 때문일 가능성을 시사합니다.  \n",
    "즉, 시각적으로 넓게 퍼진 것이 단순히 차원 축소의 왜곡일 수도 있지만, 군집 4 내의 데이터들이 특정 특징으로 강하게 뭉치기보다 전반적으로 넓게 분포하는 경향이 있음을 통계적으로 보여주는 것입니다.\n",
    "\n",
    "---\n",
    "\n",
    "**다음 단계:**  \n",
    "이러한 통계적 특성 분석과 시각화 결과를 종합하여 군집화 모델의 적합성을 최종적으로 판단하고,  \n",
    "필요하다면 군집 개수(K)를 재조정하거나 다른 군집화 알고리즘을 시도해 볼 수 있습니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bbba9717-9121-41ed-9042-ec398ce203cf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## ✅ 군집별 소비 패턴 해석 및 펀드 추천 전략\n",
    "\n",
    "## 1. PCA 축 해석 및 소비 패턴 요약\n",
    "\n",
    "- **PCA1:** 소비 횟수 중심 지표 (값이 클수록 소비 횟수 낮음)\n",
    "- **PCA2:** 소비 금액 중심 지표 (값이 클수록 소비 금액 높음)\n",
    "\n",
    "---\n",
    "\n",
    "## 2. 클러스터별 소비 해석 및 펀드 추천\n",
    "\n",
    "### 🔹 클러스터 0\n",
    "- **PCA1:** 0 ~ 3.5 (소비 횟수 적음)\n",
    "- **PCA2:** -2 ~ 2.5 (금액 중간~낮음)\n",
    "- **소비 해석:**  \n",
    "  소비 빈도와 금액이 모두 낮은 편. 카드 사용이 적은 비활동 고객, 또는 특정 시기만 사용하는 소극적 소비자.\n",
    "- **추천 펀드:**  \n",
    "  안정성 중심, 로우볼, TDF, 국공채, MMF  \n",
    "  → 클러스터 2/7번(실버·연금형, 안정지향형)과 유사\n",
    "\n",
    "---\n",
    "\n",
    "### 🔹 클러스터 1\n",
    "- **PCA1:** -3.5 ~ -1.7 (소비 횟수 많음)\n",
    "- **PCA2:** -0.5 ~ 3.5 (금액도 많음)\n",
    "- **소비 해석:**  \n",
    "  활발한 소비자. 횟수와 금액 모두 많음. 카드 중심의 생활형 소비자, 고소득 또는 재무 관심이 큰 고객.\n",
    "- **추천 펀드:**  \n",
    "  테마형, 글로벌 투자형, 성장주, 로보어드바이저, 퀀트, 커버드콜  \n",
    "  → 클러스터 1/5번(트렌드 소비형, 투자 관심형)과 유사\n",
    "\n",
    "---\n",
    "\n",
    "### 🔹 클러스터 2\n",
    "- **PCA1:** -2.5 ~ 1 (보통 횟수)\n",
    "- **PCA2:** -3 ~ -0.5 (소비 금액 낮음)\n",
    "- **소비 해석:**  \n",
    "  사용 횟수는 보통이나 지출 금액이 매우 적음. 가성비·실속형, 사회 초년생 가능성.\n",
    "- **추천 펀드:**  \n",
    "  ETF, 중소형주, 단기 투자형  \n",
    "  → 클러스터 4번(젊은 실속형)과 유사\n",
    "\n",
    "---\n",
    "\n",
    "### 🔹 클러스터 3\n",
    "- **PCA1:** -0.8 ~ 0.5 (횟수 중간)\n",
    "- **PCA2:** -1.5 ~ 1 (금액도 중간)\n",
    "- **소비 해석:**  \n",
    "  일반적인 소비 성향, 중산층 패턴. 일정한 생활 리듬 내 소비, 가족 소비 가능성.\n",
    "- **추천 펀드:**  \n",
    "  혼합형펀드, ESG, 자산배분형  \n",
    "  → 클러스터 6번(가족 중심형)과 유사\n",
    "\n",
    "---\n",
    "\n",
    "### 🔹 클러스터 4\n",
    "- **PCA1:** -2 ~ -0.5 (소비 횟수 많음)\n",
    "- **PCA2:** -0.5 ~ 2.5 (금액도 보통 이상)\n",
    "- **소비 해석:**  \n",
    "  소비 빈도 많고 금액도 적당. 활발한 생활 소비자, 젊은 소비층, 디지털 서비스·배달앱 등 사용률 높음.\n",
    "- **추천 펀드:**  \n",
    "  ETF, 중소형주, 팩터, 벤처, 공모주  \n",
    "  → 클러스터 4번(젊은 실속형) 또는 1번(트렌드 소비형)과 유사\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Spark DataFrame 기반 군집별 통계적 특성 요약\n",
    "\n",
    "### 📊 군집별 특성 분석 결과\n",
    "\n",
    "- **군집 0: 오래된 VIP & 활동성 높은 일반 고객**  \n",
    "  - 규모: 319,498명 (최대)\n",
    "  - 오랜 고객, 활발한 이용, 높은 지출, 낮은 이탈 이력, VIP 비중 높음  \n",
    "  - **해석:** 충성 고객 다수, 프리미엄 서비스 및 이탈 방지 중요\n",
    "\n",
    "- **군집 1: 저활동/초기 이탈 가능성 고객**  \n",
    "  - 규모: 80,700명\n",
    "  - 오래된 발급, 극도로 낮은 이용, 일부 탈회 이력  \n",
    "  - **해석:** 비활성·유령 고객, 재활성화 캠페인 필요\n",
    "\n",
    "- **군집 2: 신규 고객 또는 빠르게 이탈한 고객**  \n",
    "  - 규모: 217,678명\n",
    "  - 최근 가입, 높은 카드 신청, 높은 탈회 이력  \n",
    "  - **해석:** 신규·이탈 위험, 온보딩 및 이탈 방지 강화 필요\n",
    "\n",
    "- **군집 3: 적극적인 신규 또는 중급 이용 고객**  \n",
    "  - 규모: 223,079명\n",
    "  - 오랜 고객, 중간 이용, 가장 낮은 탈회 이력  \n",
    "  - **해석:** 안정적 고객, VIP 성장 유도 가능\n",
    "\n",
    "- **군집 4: 중저가 이용/평범한 고객**  \n",
    "  - 규모: 159,045명 (최소)\n",
    "  - 오랜 고객, 낮은 이용, 낮은 탈회 이력  \n",
    "  - **해석:** 평범·잠자는 고객, 맞춤형 혜택 통한 이용 유도 필요\n",
    "\n",
    "---\n",
    "\n",
    "## 4. 비즈니스적 시사점 및 전략\n",
    "\n",
    "- **군집 0:** 최우수/충성 고객 → 이탈 방지, 프리미엄 서비스 유지\n",
    "- **군집 1:** 비활성/이탈 가능성 고객 → 재활성화 캠페인, 효율적 관리\n",
    "- **군집 2:** 신규/초기 이탈 위험 고객 → 초기 경험 개선, 이탈 방지 프로그램\n",
    "- **군집 3:** 안정적 중급 이용 고객 → VIP 성장 유도, 상향/교차 판매\n",
    "- **군집 4:** 저활동 장기 고객 → 맞춤형 혜택 제공, 추가 이용 유도\n",
    "\n",
    "---\n",
    "\n",
    "### 💡 추가 해석\n",
    "- 군집 4의 넓은 PCA 분포는 실제로 다양한 특성을 가진 '평범한' 고객이 포함되어 있기 때문일 수 있음.\n",
    "- 시각적으로 넓게 퍼진 것은 차원 축소의 왜곡일 수도 있지만, 군집 4 내 데이터가 특정 특징으로 강하게 뭉치기보다 넓게 분포하는 경향을 통계적으로 보여줌.\n",
    "\n",
    "---\n",
    "\n",
    "**이 요약은 PCA 기반 소비 패턴, 군집별 통계, 실제 비즈니스 전략까지 한 번에 파악할 수 있도록 통합 정리한 내용입니다.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ff3da0c1-165a-4d48-bbe7-b5a393ee19c0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 5개 클러스터별 펀드 추천 유형 및 매칭 근거\n",
    "\n",
    "| 클러스터 | 추천 펀드 유형                  | 매칭 근거                        |\n",
    "|:--------:|:-------------------------------|:---------------------------------|\n",
    "| **0**    | 성장형, 트렌드형, 프리미엄형    | 활동성 높음, 금액 큼, VIP 비중   |\n",
    "| **1**    | 안정지향형, 실버형              | 저활동, 이탈 경험                |\n",
    "| **2**    | 젊은 실속형, 실속형             | 신규, 이탈 위험, 실용성          |\n",
    "| **3**    | 가족형, 혼합형, 자산배분형      | 꾸준함, 가족 가능성, 중산층      |\n",
    "| **4**    | 연금형, 안정지향형              | 장기 고객, 저활동, 평범          |\n",
    "\n",
    "---\n",
    "\n",
    "### 클러스터별 해석 및 펀드 추천 요약\n",
    "\n",
    "- **클러스터 0:**  \n",
    "  소비·이용이 많고 VIP 비중 높음 → 프리미엄, 성장형, 트렌드형 펀드와 가장 유사 (고위험·고수익 선호)\n",
    "\n",
    "- **클러스터 1:**  \n",
    "  거의 사용 안 하거나, 이탈 경험 있음 → 안정지향, 실버, 연금형 등 보수적·안정형 상품이 적합\n",
    "\n",
    "- **클러스터 2:**  \n",
    "  신규, 이탈 위험, 실속형 → 젊은 실속형, 단기형, ETF, 중소형주 등 실용적·저위험 상품\n",
    "\n",
    "- **클러스터 3:**  \n",
    "  꾸준한 중간이용, 가족 가능성 → 가족형, 혼합형, 자산배분형 등 안정+성장 혼합 상품\n",
    "\n",
    "- **클러스터 4:**  \n",
    "  장기고객이나 활동성 낮음, 평범 → 연금형, 실버형, 안정지향형 등 장기·저위험 상품\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d69b9a87-4912-4208-9990-5e81f516c09c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# 분류모델\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2c6809b0-a074-4c95-8b4b-7c34df74b161",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "from pyspark.sql import Row\n",
    "\n",
    "# 1. 원본 데이터에 row_id 컬럼 생성 (샘플링과 동일한 방식으로)\n",
    "if 'row_id' not in df_final.columns:\n",
    "    df_final = df_final.withColumn('row_id', monotonically_increasing_id())\n",
    "\n",
    "# 2. 클러스터 결과 DataFrame 생성 (final_cluster_ids_np는 원본 데이터 순서와 반드시 일치해야 함)\n",
    "rows = [Row(row_id=int(idx), cluster=int(cluster)) for idx, cluster in enumerate(final_cluster_ids_np)]\n",
    "df_final_cluster = spark.createDataFrame(rows)\n",
    "\n",
    "# 3. 원본 데이터와 클러스터 결과를 row_id 기준으로 join\n",
    "df_final_with_cluster = df_final.join(df_final_cluster, on='row_id', how='left')\n",
    "\n",
    "# 4. 결과 확인\n",
    "df_final_with_cluster.select('row_id', 'cluster').show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d09cdd78-79ae-42e7-9e5e-51aa93cb4e6e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_final_with_cluster.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "532e8b84-d3f2-4fdf-9df4-6b23b95f6fab",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# 1. 데이터 정렬 후 순차적 인덱스 부여 (예: 발급회원번호 기준)\n",
    "df_sorted = df_final.orderBy(F.asc(\"발급회원번호\")).withColumn(\"index\", F.monotonically_increasing_id())\n",
    "\n",
    "# 2. 상위 5개 회원번호 추출 (정렬 상태에서)\n",
    "member_ids_head = [row['발급회원번호'] for row in df_sorted.limit(5).collect()]\n",
    "cluster_ids_head = final_cluster_ids_np[:5]\n",
    "\n",
    "# 3. 하위 5개 회원번호 추출 (정렬 상태에서)\n",
    "total_count = df_sorted.count()\n",
    "member_ids_tail = [row['발급회원번호'] for row in df_sorted.filter(F.col(\"index\") >= total_count-5).collect()]\n",
    "cluster_ids_tail = final_cluster_ids_np[-5:]\n",
    "\n",
    "# 4. 결과 출력\n",
    "print(\"[정렬된 데이터 기준]\")\n",
    "print(\"상위 5개 회원번호:\", member_ids_head)\n",
    "print(\"상위 5개 클러스터ID:\", cluster_ids_head)\n",
    "print(\"하위 5개 회원번호:\", member_ids_tail)\n",
    "print(\"하위 5개 클러스터ID:\", cluster_ids_tail)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9bdc1b49-14ec-4ea8-af91-df1977bb2f34",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# final_cluster_ids_np와 df_final의 행 순서가 동일하다는 전제 하에\n",
    "member_ids = [row['발급회원번호'] for row in df_final.select('발급회원번호').collect()]\n",
    "rows = [Row(발급회원번호=member_id, cluster=int(cluster)) for member_id, cluster in zip(member_ids, final_cluster_ids_np)]\n",
    "df_final_cluster = spark.createDataFrame(rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1e405419-53d5-47f3-ac51-fb8c55b3576b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_final_with_cluster = df_final.join(df_final_cluster, on='발급회원번호', how='left')\n",
    "df_final_with_cluster.select('발급회원번호', 'cluster').show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "ee706620-7165-45de-95f6-b3f08bcbb22e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_final_with_cluster.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "11cb5aac-d1fc-4d11-9f96-a8aa004dd777",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 클러스터별 인원수 집계\n",
    "df_final_with_cluster.groupBy('cluster').count().orderBy('cluster').show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "49b63825-e285-41b6-9cbc-4c97d64c5a5a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# 1. 원본 전체 데이터의 PCA 벡터 추출\n",
    "pca_features_np = np.array([row.pca_features.toArray() for row in df_final.select(\"pca_features\").collect()])\n",
    "pca_features_tensor = torch.from_numpy(pca_features_np).float().cuda()\n",
    "\n",
    "# 2. 기존에 학습한 클러스터 중심(centers) 사용\n",
    "# kmeans_centers: (num_clusters, feature_dim)  # 이미 샘플로부터 구해진 값\n",
    "\n",
    "# 3. 각 행별로 클러스터 할당 (유클리드 거리 기준)\n",
    "assigned_clusters = torch.argmin(torch.cdist(pca_features_tensor, kmeans_centers.to(pca_features_tensor.device)), dim=1)\n",
    "\n",
    "# 4. 결과를 numpy로 변환\n",
    "final_cluster_ids_np = assigned_clusters.cpu().numpy().astype(int)\n",
    "\n",
    "# 5. 발급회원번호와 클러스터 번호를 매칭\n",
    "member_ids = [row['발급회원번호'] for row in df_final.select('발급회원번호').collect()]\n",
    "rows = [Row(발급회원번호=member_id, cluster=int(cluster)) for member_id, cluster in zip(member_ids, final_cluster_ids_np)]\n",
    "df_final_cluster = spark.createDataFrame(rows)\n",
    "\n",
    "# 6. 원본 데이터와 join\n",
    "df_final_with_cluster = df_final.join(df_final_cluster, on='발급회원번호', how='left')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5c43e849-1aae-4375-8bfb-cff7f616bbf1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_final_with_cluster.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e8daee8e-5ca0-4759-a5e6-33d9d4a4a788",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 발급회원번호 중복 여부 확인\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "df_final_with_cluster.groupBy('발급회원번호').count().filter(F.col('count') > 1).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "fd5ef236-cac5-4504-8575-228a34b2ab42",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 클러스터별 인원수 집계\n",
    "df_final_with_cluster.groupBy('cluster').count().orderBy('cluster').show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d631c1c8-eb2f-415c-b242-cbb8e4fa1b6d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 중복제거 중\n",
    "df_final_dedup = df_final.dropDuplicates(['발급회원번호'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ecb5a6c2-1918-43ef-b721-25d11918a167",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "df_final_dedup.groupBy('발급회원번호').count().filter(F.col('count') > 1).count()\n",
    "# 결과가 0이면 중복 없음\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3a4585fa-7361-41df-ab19-35286a8076f6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#수정된 변수로 다시 예측분류 모델 시작\n",
    "from pyspark.sql import Row\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# 1. 중복 제거된 데이터프레임 사용\n",
    "pca_features_np = np.array([row.pca_features.toArray() for row in df_final_dedup.select(\"pca_features\").collect()])\n",
    "pca_features_tensor = torch.from_numpy(pca_features_np).float().cuda()\n",
    "\n",
    "# 2. 기존에 학습한 클러스터 중심(centers) 사용\n",
    "# kmeans_centers는 이미 존재한다고 가정\n",
    "\n",
    "# 3. 각 행별로 클러스터 할당 (유클리드 거리 기준)\n",
    "assigned_clusters = torch.argmin(torch.cdist(pca_features_tensor, kmeans_centers.to(pca_features_tensor.device)), dim=1)\n",
    "\n",
    "# 4. 결과를 numpy로 변환\n",
    "final_cluster_ids_np = assigned_clusters.cpu().numpy().astype(int)\n",
    "\n",
    "# 5. 발급회원번호와 클러스터 번호를 매칭\n",
    "member_ids = [row['발급회원번호'] for row in df_final_dedup.select('발급회원번호').collect()]\n",
    "rows = [Row(발급회원번호=member_id, cluster=int(cluster)) for member_id, cluster in zip(member_ids, final_cluster_ids_np)]\n",
    "df_final_cluster = spark.createDataFrame(rows)\n",
    "\n",
    "# 6. 원본 데이터(df_final)와 join (중복 제거된 df_final_dedup가 아닌 원본과 join)\n",
    "df_final_with_cluster = df_final.join(df_final_cluster, on='발급회원번호', how='left')\n",
    "\n",
    "# 결과 일부 확인\n",
    "df_final_with_cluster.select('발급회원번호', 'cluster').show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "62d2226d-8df7-48b2-998c-6f34dcb0eb7a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 클러스터별 인원수 집계\n",
    "df_final_with_cluster.groupBy('cluster').count().orderBy('cluster').show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8dd57b00-0cd9-4a07-96b5-c0e697c3e71f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 컬럼명과 데이터 타입 전체 출력\n",
    "for field in df_final_with_cluster.schema.fields:\n",
    "    print(f\"{field.name}: {field.dataType}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "d0157b94-7752-46b3-905a-a46891975d54",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#진짜 최종 1300만건관련 군집분석\n",
    "df_final_with_cluster.show(2, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "baba2fb2-575f-42f2-9025-e6039983cc2e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#수치형 데이터설명\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# numeric_cols 리스트 예시\n",
    "numeric_cols = [\n",
    "    '입회경과개월수_신용', '최종카드발급경과월', '탈회횟수_누적', '카드신청건수', '소지카드수_유효_신용',\n",
    "    '유효카드수_체크', '이용카드수_신용', '이용카드수_체크', '청구금액_기본연회비_B0M', '청구금액_제휴연회비_B0M',\n",
    "    '최종탈회후경과월', '탈회횟수_발급6개월이내', '탈회횟수_발급1년이내',\n",
    "    'log_입회경과개월수_신용', 'log__1순위카드이용금액', 'log__1순위카드이용건수',\n",
    "    'log__2순위카드이용금액', 'log__2순위카드이용건수', '총이용금액', 'log_총이용금액',\n",
    "    'VIP등급코드_idx', '연령_idx', '최상위카드등급코드_idx', 'row_id'\n",
    "]\n",
    "\n",
    "# 백틱으로 감싸는 함수\n",
    "def backtick_col(col_name):\n",
    "    return f\"`{col_name}`\"\n",
    "\n",
    "agg_exprs = []\n",
    "for col in numeric_cols:\n",
    "    agg_exprs.extend([\n",
    "        F.max(backtick_col(col)).alias(f\"max_{col}\"),\n",
    "        F.min(backtick_col(col)).alias(f\"min_{col}\"),\n",
    "        F.mean(backtick_col(col)).alias(f\"mean_{col}\"),\n",
    "        F.expr(f'percentile_approx({backtick_col(col)}, 0.5)').alias(f\"median_{col}\")\n",
    "    ])\n",
    "\n",
    "# 집계 실행\n",
    "cluster_stats = df_final_with_cluster.groupBy(\"cluster\").agg(*agg_exprs)\n",
    "cluster_stats.orderBy(\"cluster\").show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2f9f6b0f-3b64-4097-aaca-47b640989792",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 최빈값추가 반복\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "for col in numeric_cols:\n",
    "    if col != 'cluster':\n",
    "        window_spec = Window.partitionBy(\"cluster\").orderBy(F.desc(\"count\"))\n",
    "        mode_df = (\n",
    "            df_final_with_cluster.groupBy(\"cluster\", col)\n",
    "            .count()\n",
    "            .withColumn(\"rank\", F.row_number().over(window_spec))\n",
    "            .filter(F.col(\"rank\") == 1)\n",
    "            .select(\"cluster\", F.col(col).alias(f\"mode_{col}\"))\n",
    "        )\n",
    "        cluster_stats = cluster_stats.join(mode_df, on=\"cluster\", how=\"left\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "f42eebb2-b0db-43a7-a40d-916851d69f73",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#결과확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ecee205c-a19d-46f2-804c-1c9e5f0455a1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 1. 기존 테이블이 있다면 삭제 (덮어쓰기 안전 보장)\n",
    "spark.sql(\"DROP TABLE IF EXISTS database_pjt.df_final_with_cluster\")\n",
    "\n",
    "# 2. Delta Lake 포맷으로 관리형 테이블로 저장 (웨어하우스/SQL/ML/시각화 모두 호환)\n",
    "df_final_with_cluster.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"database_pjt.df_final_with_cluster\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "21997bed-927e-456b-9d85-918db159f8fe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score\n",
    "import numpy as np\n",
    "\n",
    "# 1. PCA 피처와 클러스터 레이블을 numpy 배열로 추출\n",
    "pca_features_np = np.array([row.pca_features.toArray() for row in df_final_with_cluster.select('pca_features').collect()])\n",
    "cluster_labels_np = np.array([row.cluster for row in df_final_with_cluster.select('cluster').collect()])\n",
    "\n",
    "# 2. 샘플링 (예: 3만개)\n",
    "sample_idx = np.random.choice(len(pca_features_np), 30000, replace=False)\n",
    "sample_np = pca_features_np[sample_idx]\n",
    "sample_labels = cluster_labels_np[sample_idx]\n",
    "\n",
    "# 3. 실루엣 점수 계산\n",
    "score = silhouette_score(sample_np, sample_labels, metric='euclidean')\n",
    "print(f\"Silhouette Score (Euclidean, 3만개 샘플): {score:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "39a68eca-cd2e-4170-b17e-2cf2d8f7ef99",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_final_with_cluster = spark.table(\"database_pjt.df_final_with_cluster\")\n",
    "df_final_with_cluster.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7e98fbe6-ca2a-492f-ade1-44ecfd194eeb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "#ml 등록\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a0937f01-b676-4433-9bb0-246411f2cf50",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#mlflow 설치 및 임포트\n",
    "%pip install mlflow\n",
    "import mlflow\n",
    "import mlflow.pytorch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d801ddd9-9527-4919-98dd-9c6b65b49813",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#클러스터 모델 저장(파이토치기준)\n",
    "import torch\n",
    "\n",
    "class ClusterModel(torch.nn.Module):\n",
    "    def __init__(self, centers):\n",
    "        super().__init__()\n",
    "        self.centers = torch.nn.Parameter(centers)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return torch.argmin(torch.cdist(x, self.centers), dim=1)\n",
    "\n",
    "model = ClusterModel(kmeans_centers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ae91bd12-10ed-4709-abd4-e346aede3653",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#3. MLflow로 모델 로깅 및 등록\n",
    "#(실루엣 점수, 파라미터, 클러스터 중심 시각화까지 포함)\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "with mlflow.start_run(run_name=\"ClusterModelRun\") as run:\n",
    "    # 파라미터 로깅\n",
    "    mlflow.log_param(\"num_clusters\", kmeans_centers.shape[0])\n",
    "    mlflow.log_param(\"distance_metric\", \"euclidean\")\n",
    "    \n",
    "    # 모델 등록 (레지스트리 이름 반드시 지정)\n",
    "    mlflow.pytorch.log_model(\n",
    "        model,\n",
    "        \"cluster_model\",\n",
    "        registered_model_name=\"Classification_Member_Info\"\n",
    "    )\n",
    "    \n",
    "    # (선택) 실루엣 점수 등 메트릭 기록\n",
    "    mlflow.log_metric(\"silhouette_score\", score)\n",
    "    \n",
    "    # (선택) 클러스터 중심 시각화\n",
    "    plt.scatter(kmeans_centers[:,0].cpu(), kmeans_centers[:,1].cpu())\n",
    "    plt.title(\"Cluster Centers\")\n",
    "    plt.savefig(\"cluster_centers.png\")\n",
    "    mlflow.log_artifact(\"cluster_centers.png\")\n",
    "    \n",
    "    run_id = run.info.run_id\n",
    "\n",
    "print(\"MLflow run_id:\", run_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "234a3293-00a2-4d8b-afd4-b37a899a5a37",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "모델 버전 관리\n",
    "\n",
    "데이터브릭스 워크스페이스 → \"Models\" 탭에서 버전별 관리 가능\n",
    "\n",
    "Staging → Production 단계 승격 가능\n",
    "\n",
    "배포 옵션\n",
    "\n",
    "배치 추론: spark_udf 생성하여 대규모 데이터 처리 가능\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c2445328-6d4b-46aa-a47d-1684094da453",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#최종 저장 구조\n",
    "\n",
    "#text\n",
    "#dbfs:/databricks/mlflow-tracking/experiments/\n",
    "  └─ <experiment_id>/\n",
    "     └─ <run_id>/\n",
    "        ├─ artifacts/\n",
    "        │  └─ cluster_model/\n",
    "        │     ├─ model.pth\n",
    "        │     └─ conda.yaml\n",
    "        ├─ metrics/\n",
    "        └─ params/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "438ff9cd-1a88-4e2a-b7dd-5145b9c4394a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#추가적인 배포옵션?\n",
    "#배치추론 : spark_udf 생성해서 대규모 데이터 처리가증\n",
    "cluster_udf = mlflow.pyfunc.spark_udf(spark, f\"models:/{model_name}/Production\")\n",
    "df_with_pred = df_final.withColumn(\"cluster\", cluster_udf(\"pca_features\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1094d208-6f98-45ff-aaba-979c5219c3d7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "모델 모니터링\n",
    "\n",
    "입력 데이터 분포, 예측값 분포 등을 MLflow에 추가 로깅 가능\n",
    "\n",
    "모델 드리프트 감지 설정 가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9780412e-a95f-4809-905d-a80ed677af57",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#배포옵션\n",
    "# 모델 로드\n",
    "model_name = \"Classification_Member_Info\"\n",
    "loaded_model = mlflow.pytorch.load_model(f\"models:/{model_name}/Production\")\n",
    "\n",
    "# 배치 예측 예시\n",
    "pca_tensor = torch.from_numpy(pca_features_np).float().cuda()\n",
    "predictions = loaded_model(pca_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b48cd4d6-6ddb-4cf8-b173-b5e3b5b8d86f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# 모델을 GPU로 이동\n",
    "loaded_model = loaded_model.to('cuda')\n",
    "\n",
    "# 입력 데이터도 GPU로 이동\n",
    "pca_tensor = torch.from_numpy(pca_features_np).float().cuda()\n",
    "\n",
    "# 예측 수행\n",
    "predictions = loaded_model(pca_tensor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1eca4263-582b-4255-bc9d-8ad0fdebbf00",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 예측 결과를 numpy 배열로 변환\n",
    "pred_np = predictions.cpu().numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "36c09f6c-605b-4698-8c5a-4a265acbcf2f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 예측결과를 df에 추가하기\n",
    "from pyspark.sql import Row\n",
    "\n",
    "member_ids = [row['발급회원번호'] for row in df_final_dedup.select('발급회원번호').collect()]\n",
    "rows = [Row(발급회원번호=member_id, cluster=int(cluster)) for member_id, cluster in zip(member_ids, pred_np)]\n",
    "df_pred_cluster = spark.createDataFrame(rows)\n",
    "\n",
    "# 원본 데이터와 join\n",
    "df_final_with_pred = df_final_dedup.join(df_pred_cluster, on='발급회원번호', how='left')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "720880de-3f61-4568-a4fc-31d8cd01c601",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#3. 클러스터별 통계 및 분포 분석\n",
    "#groupBy, agg 등을 사용해 클러스터별 인원수, 평균값, 분포 등을 분석합니다.\n",
    "\n",
    "df_final_with_pred.groupBy('cluster').count().orderBy('cluster').show()\n",
    "df_final_with_pred.groupBy('cluster').avg('총이용금액').orderBy('cluster').show()#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0b4ae09e-0d21-4e9d-a332-b70c74de754b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#4. 결과 저장\n",
    "#분석 결과나 예측 결과를 Delta Lake 또는 Hive 테이블로 저장하여 팀원과 공유할 수 있습니다.\n",
    "\n",
    "df_final_with_pred.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"database_pjt.df_final_with_pred\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ff7bfe79-e86a-4971-8308-268863f5dd14",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "5. 추가 작업\n",
    "실루엣 점수 등 군집 품질 평가\n",
    "\n",
    "UMAP, PCA 등 시각화\n",
    "\n",
    "후속 ML 파이프라인 구축 및 모델 재사용, 배포 등"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "22554263-dad3-488e-9c10-cbcdb5dabfbd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# 3.소비정보 데이터 클러스터링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e0e722ae-3558-4b8e-9ab4-014459838b9e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "!pip install kmeans-pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "798aa3d8-30b7-4e35-a47c-f1bddd137c46",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from kmeans_pytorch import kmeans\n",
    "from pyspark.sql import Row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aac8e3d7-2c64-4007-afb4-1a58a454a0af",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "df_use = spark.read.format(\"delta\").table(\"database_pjt.3_all\")\n",
    "df_use.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0fadf9c8-179d-46eb-b921-cf3d44391350",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "df_use.printSchema()\n",
    "print(df_use.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b1694724-293e-42dc-84f3-f48c222a1baa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 회원정보 전체 데이터 수 계산 -> 클러스터링 처리 연산 관련 데이터량 추정\n",
    "df_use.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b91eb18f-0df3-41bf-9485-593642668781",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "from pyspark.sql.functions import to_date, col\n",
    "\n",
    "# 예시 데이터\n",
    "# df = spark.createDataFrame([('202401',), ('202402',), ('202403',)], ['기준년월'])\n",
    "\n",
    "# 문자열을 datetime으로 변환 (일자를 1일로 지정)\n",
    "df_use = df_use.withColumn('기준년월', to_date(col('기준년월'), 'yyyyMM'))\n",
    "\n",
    "# datetime 타입에서 date 타입으로 변환\n",
    "df_use = df_use.withColumn('기준년월', col('기준년월').cast('date'))\n",
    "\n",
    "display(df_use)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eefcafcf-91d0-443c-84ce-455d2b8850fd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# 1. 인코딩 대상 컬럼 정의\n",
    "label_cols = [\"_1순위업종\", \"_2순위업종\",\"_3순위업종\",\"_1순위쇼핑업종\",\"_2순위쇼핑업종\",\"_3순위쇼핑업종\",\"_1순위교통업종\",\"_2순위교통업종\",\n",
    "              \"_3순위교통업종\",\"_1순위여유업종\",\"_2순위여유업종\",\"_3순위여유업종\",\"_1순위납부업종\",\"_2순위납부업종\",\"_3순위납부업종\"]\n",
    "\n",
    "# 2. Indexing + One-Hot Encoding 단계 정의\n",
    "indexers = [\n",
    "    StringIndexer(inputCol=col, outputCol=f\"{col}_idx\", handleInvalid=\"keep\")\n",
    "    for col in label_cols\n",
    "]\n",
    "\n",
    "# 3. 파이프라인 실행\n",
    "pipeline = Pipeline(stages=indexers)\n",
    "encoded_model = pipeline.fit(df_use)\n",
    "df_encoded = encoded_model.transform(df_use)\n",
    "\n",
    "# 4. 사용할 컬럼 정리\n",
    "encoded_cols = [f\"{col}_idx\" for col in label_cols if f\"{col}_idx\" in df_encoded.columns]\n",
    "\n",
    "original_cols = [col for col in df_use.columns if col not in (label_cols)]\n",
    "\n",
    "# 5. 최종 df 구성\n",
    "df_final = df_encoded.select(original_cols + encoded_cols)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "97efbcc4-46fc-4cff-9432-3cd9984bdd50",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "범주형 데이터 인코딩 완료, 허나 수치형 데이터가 너무 많아서 더 쳐낼 필요가 있겠음\n",
    "일단 상관관계를 다시보고 0.8 이상인 칼럼이 있는지 확인하고, 있으면 쳐냄"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d41bb8a1-a9d1-4afa-a562-c287da3e6a8d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.stat import Correlation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# 1. 수치형 변수 리스트 추출\n",
    "numeric_cols = [c for c, t in df_final.dtypes if t in ['double', 'int']]\n",
    "\n",
    "# 2. VectorAssembler로 features 컬럼 생성\n",
    "assembler = VectorAssembler(inputCols=numeric_cols, outputCol=\"features\")\n",
    "df_vec = assembler.transform(df_final).select(\"features\")\n",
    "\n",
    "# 3. 피어슨 상관계수 행렬 계산\n",
    "corr_matrix = Correlation.corr(df_vec, \"features\", \"pearson\").head()[0].toArray()\n",
    "\n",
    "# 4. pandas DataFrame으로 변환\n",
    "corr_df = pd.DataFrame(corr_matrix, index=numeric_cols, columns=numeric_cols)\n",
    "\n",
    "# 5. 자기 자신과의 상관관계 제외 (1.0 제거)\n",
    "mask = np.triu(np.ones(corr_df.shape), k=1).astype(bool)  # 상삼각 행렬 마스크\n",
    "corr_filtered = corr_df.where(mask)\n",
    "\n",
    "# 6. 0.8 이상인 상관계수 필터링\n",
    "high_corr = corr_filtered.stack().reset_index()\n",
    "high_corr.columns = ['변수1', '변수2', '상관계수']\n",
    "high_corr = high_corr[high_corr['상관계수'] >= 0.8]\n",
    "\n",
    "# 7. 출력\n",
    "print(\"상관계수가 0.8 이상인 변수 쌍:\")\n",
    "print(high_corr.sort_values(by='상관계수', ascending=False).round(3))\n",
    "\n",
    "# Databricks에서는 표 형태로 보기 좋게 출력\n",
    "display(high_corr.sort_values(by='상관계수', ascending=False).round(3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "365f491c-6701-477f-a540-22d9aefaaeae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 상관관계 0.8 이상 헤비 컬럼 50개 삭제\n",
    "\n",
    "delete_col = [\"이용개월수_부분무이자_R12M\",\"이용횟수_선결제_B0M\",\"이용개월수_페이_오프라인_R6M\",\"이용개월수_카드론_R3M\",\"이용개월수_체크_R12M\",\n",
    "              \"이용금액_A_페이_R6M\",\"이용개월수_페이_온라인_R6M\",\"이용금액_할부_무이자_B0M\",\"이용개월수_체크_R12M\",\"이용개월수_카드론_R12M\",\n",
    "              \"이용개월수_카드론_R6M\",\"이용금액_CA_R3M\",\"이용개월수_신용_R12M\",\"이용개월수_페이_오프라인_R6M\",\"이용금액_신용_R3M\",\"이용금액_B페이_R3M\",\n",
    "              \"이용횟수_연체_R3M\",\"이용개월수_CA_R12M\",\"이용개월수_선결제_R6M\",\"이용개월수_온라인_R6M\",\"이용개월수_CA_R12M\",\"이용금액_선결제_R3M\",\n",
    "              \"이용횟수_연체_R6M\",\"이용금액_CA_B0M\",\"이용횟수_선결제_B0M\",\"이용횟수_연체_B0M\",\"이용개월수_D페이_R6M\",\"이용금액_체크_R6M\",\"이용횟수_선결제_B0M\",\"이용개월수_전체_R6M\",\"이용금액_할부_무이자_R6M\",\"이용개월수_할부_유이자_R12M\",\"이용금액_CA_R3M\",\"이용개월수_CA_R12M\",\"이용횟수_연체_R3M\",\"이용개월수_선결제_R6M\",\"이용개월수_할부_유이자_R6M\",\"이용개월수_할부_유이자_R12M\",\n",
    "              \"이용금액_신용_R3M\",\"이용개월수_전체_R6M\",\"이용개월수_신용_R12M\",\"이용금액_C페이_B0M\",\"이용금액_선결제_R3M\",\"이용횟수_연체_R6M\",\n",
    "              \"이용금액_오프라인_R6M\",\"이용금액_B페이_B0M\",\"이용금액_오프라인_R6M\",\"이용금액_신용_R6M\",\"이용금액_오프라인_R6M\",\"이용금액_할부_무이자_R3M\"]\n",
    "              \n",
    "df_final = df_final.drop(*delete_col)\n",
    "\n",
    "df_final.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1869c788-c656-46ee-944c-14c396e88ff4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_final.write.format(\"delta\").mode(\"error\").saveAsTable(\"database_pjt.3_use_encoding\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ed9f6c96-764b-4025-a815-d9cfbd906484",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 전체 데이터의 0.5% (90만개)만 추출하여 샘플링 및 저장\n",
    "\n",
    "df_final_sample = df_final.sample(fraction=0.005, seed=42)\n",
    "df_final_sample.write.format(\"delta\").mode(\"error\").saveAsTable(\"database_pjt.3_use_encoding_sample\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b7ac51a9-8da0-4f85-8a4a-ff0963a9e444",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_use_final_sample = df_final_sample.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d4069c3a-d6bd-4819-8391-da4987904368",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0ff88dd6-d49f-43c4-8d5c-034ee28245d5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "# (1) 실제 존재하는 one-hot 컬럼만 필터링\n",
    "encoded_feature_cols = [f\"{col}_oh\" for col in one_hot_cols]\n",
    "existing_encoded_cols = [col for col in encoded_feature_cols if col in df_final.columns]\n",
    "\n",
    "# (2) VectorAssembler 구성\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=existing_encoded_cols,\n",
    "    outputCol=\"features\"\n",
    ")\n",
    "\n",
    "df_vectorized = assembler.transform(df_final)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9de06683-e1c2-40cf-b187-b7d7a807dc31",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_final.write.format(\"delta\").mode(\"error\").saveAsTable(\"database_pjt.3_use_encoding\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "75c8e5a1-758c-4156-8030-8b95c6d1908f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(df_final.limit(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "402d7d98-5319-4e5c-aa5c-dddae3466535",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# DataFrame을 numpy 배열로 변환 후 텐서로 변환\n",
    "# 예: 상위 100만건만 추출\n",
    "sampled_df = df_use.limit(1000000).toPandas().select_dtypes(include='number')\n",
    "df_use_tensor = torch.from_numpy(sampled_df.values).float().cuda()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "06f9aad1-43c8-48cd-82c3-473f7c3fed1f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#kmeans(gpu) 샘플데이터로 클러스터링\n",
    "#이거 하고 최단연결, 최장연결, 평균연결,중심연결 4개 비교도 해보고싶긴한데\n",
    "from kmeans_pytorch import kmeans\n",
    "\n",
    "# 샘플 데이터로 KMeans (유클리드)\n",
    "kmeans_cluster_ids, kmeans_centers = kmeans(\n",
    "    X=df_use_tensor,\n",
    "    num_clusters=5,\n",
    "    distance='euclidean',\n",
    "    device=torch.device('cuda')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e03e2b30-d1c9-4e61-a18a-d1bf2fae4680",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 4. 전체 데이터 예측 (배치 처리 권장, 메모리 부족시 limit 사용)\n",
    "# 4. 전체 데이터 예측 함수/ 데이터가 어느 클러스터 속하는지 예측할당\n",
    "import torch\n",
    "def assign_clusters(X, centers, distance='euclidean'):\n",
    "    centers = centers.to(X.device)\n",
    "    if distance == 'euclidean':\n",
    "        dists = torch.cdist(X, centers)\n",
    "    elif distance == 'cosine':\n",
    "        X_norm = X / X.norm(dim=1, keepdim=True)\n",
    "        centers_norm = centers / centers.norm(dim=1, keepdim=True)\n",
    "        dists = 1 - torch.mm(X_norm, centers_norm.t())\n",
    "    else:\n",
    "        raise ValueError(\"지원하지 않는 distance\")\n",
    "    return torch.argmin(dists, dim=1)\n",
    "\n",
    "# 5. 전체 데이터 배치 처리 (70,000개 제한 예시)\n",
    "df_final_limit = df_final.limit(10000)\n",
    "pca_features_np = np.array([row.pca_features.toArray() for row in df_final_limit.select(\"pca_features\").collect()])\n",
    "pca_features_tensor = torch.from_numpy(pca_features_np).float().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5b08cc9f-0af8-48dc-bee4-f4b4af3074cd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 6. 클러스터 할당 즉 행이 어떤 클러스터 속하는지 실제 계산후 라벨링\n",
    "kmeans_cluster_ids_full = assign_clusters(pca_features_tensor, kmeans_centers, 'euclidean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "20428a81-b1ae-4633-83c3-6197b43dd0f9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 7. 메타 클러스터링/ 유클리드랑 코사인거리등 보고, 통합해서 한번더 군집화\n",
    "meta_features = torch.stack([kmeans_cluster_ids_full, kmeans2_cluster_ids_full], dim=1).float()\n",
    "meta_cluster_ids, meta_centers = kmeans(\n",
    "    X=meta_features,\n",
    "    num_clusters=3,\n",
    "    distance='euclidean',\n",
    "    device=torch.device('cuda'),\n",
    "    tol=1e-4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cb940829-7b22-4484-b5d4-3e37f0ba0010",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 8. 결과 변환\n",
    "final_cluster_ids_np = meta_cluster_ids.cpu().numpy().astype(int)\n",
    "rows = [Row(id=idx, final_cluster=int(cluster)) for idx, cluster in enumerate(final_cluster_ids_np)]\n",
    "df_final_cluster = spark.createDataFrame(rows)\n",
    "df_final_cluster.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9912a868-c4dd-4d3a-bffb-5a62a149bbab",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "# 리미트개수에서 1,0000개만 무작위 샘플링\n",
    "idx = np.random.choice(len(pca_sample_np), 30000, replace=False)\n",
    "sample_np = pca_sample_np[idx]\n",
    "sample_labels = kmeans_cluster_ids.cpu().numpy()[idx]\n",
    "\n",
    "score = silhouette_score(sample_np, sample_labels, metric='euclidean')\n",
    "print(f\"Silhouette Score (Euclidean, 30000개 샘플): {score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9e0316f9-941c-4fec-b5c9-40ea33becfc8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Delta Lake 관리형 테이블 불러오기\n",
    "df = spark.sql(\"SELECT * FROM database_pjt.df_final_with_cluster\")\n",
    "# 또는\n",
    "df = spark.table(\"database_pjt.df_final_with_cluster\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bba63e37-61a3-4506-bdad-0bd6210041c6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.clustering import KMeans\n",
    "import numpy as np\n",
    "\n",
    "# 예시: 군집별 중심(centroid) 및 응집도(SSE) 계산\n",
    "# 1. 군집별 중심(centroid) 계산 (예: 연령, 이용금액, 탈회횟수 등 주요 변수 기준)\n",
    "assembler = VectorAssembler(inputCols=[\"연령_idx\", \"이용금액\", \"탈회횟수_누적\"], outputCol=\"features\")\n",
    "df_feat = assembler.transform(df)\n",
    "\n",
    "# 2. 군집별 중심(centroid) 계산 (Spark SQL로 직접 계산)\n",
    "centroids = df_feat.groupBy(\"cluster\").agg(\n",
    "    F.mean(\"연령_idx\").alias(\"mean_age\"),\n",
    "    F.mean(\"이용금액\").alias(\"mean_amount\"),\n",
    "    F.mean(\"탈회횟수_누적\").alias(\"mean_churn\")\n",
    ").orderBy(\"cluster\").collect()\n",
    "\n",
    "# 3. 각 군집의 응집도(SSE, Within-Cluster Sum of Squares) 계산\n",
    "# ※ Spark에서는 직접 계산이 복잡하므로, 샘플링 후 pandas로 계산하는 것이 효율적\n",
    "# 아래는 pandas 예시 (Spark에서 collect 후 처리)\n",
    "import pandas as pd\n",
    "\n",
    "pdf = df_feat.sample(False, 0.01).toPandas()  # 샘플링\n",
    "pdf['cluster'] = pdf['cluster'].astype(int)\n",
    "centroids_pd = pd.DataFrame([(c['cluster'], c['mean_age'], c['mean_amount'], c['mean_churn']) \n",
    "                            for c in centroids], \n",
    "                           columns=['cluster', 'mean_age', 'mean_amount', 'mean_churn'])\n",
    "\n",
    "sse = []\n",
    "for cl in pdf['cluster'].unique():\n",
    "    cl_data = pdf[pdf['cluster'] == cl]\n",
    "    centroid = centroids_pd[centroids_pd['cluster'] == cl].iloc[0]\n",
    "    dist = np.sqrt(\n",
    "        (cl_data['연령_idx'] - centroid['mean_age'])**2 +\n",
    "        (cl_data['이용금액'] - centroid['mean_amount'])**2 +\n",
    "        (cl_data['탈회횟수_누적'] - centroid['mean_churn'])**2\n",
    "    )\n",
    "    sse.append(dist.sum())\n",
    "\n",
    "print(\"군집별 응집도(SSE):\", sse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cdb5a55b-f239-4121-8eba-67a7abf3b29b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 전체 중심(global centroid) 계산\n",
    "global_centroid = pdf[['연령_idx', '이용금액', '탈회횟수_누적']].mean()\n",
    "\n",
    "ssb = []\n",
    "for cl in pdf['cluster'].unique():\n",
    "    centroid = centroids_pd[centroids_pd['cluster'] == cl].iloc[0]\n",
    "    n = len(pdf[pdf['cluster'] == cl])\n",
    "    dist = n * (\n",
    "        (centroid['mean_age'] - global_centroid['연령_idx'])**2 +\n",
    "        (centroid['mean_amount'] - global_centroid['이용금액'])**2 +\n",
    "        (centroid['mean_churn'] - global_centroid['탈회횟수_누적'])**2\n",
    "    )\n",
    "    ssb.append(dist)\n",
    "\n",
    "print(\"군집별 분리도(SSB):\", ssb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8083f6d6-a31f-44e9-9603-1985680b4536",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 예시: 군집별 소속 확률이 비슷한 데이터(중첩 영역) 비율 계산\n",
    "# ※ 실제로는 군집 소속 확률이 필요하지만, hard clustering에서는 불가 → 대안: 군집 경계 근처 데이터 비율 계산\n",
    "\n",
    "# 1. 각 데이터와 가장 가까운 두 군집 중심 거리 차이 계산\n",
    "def get_closest_two(pdf, centroids_pd):\n",
    "    dists = []\n",
    "    for idx, row in pdf.iterrows():\n",
    "        d = []\n",
    "        for _, c in centroids_pd.iterrows():\n",
    "            dist = np.sqrt(\n",
    "                (row['연령_idx'] - c['mean_age'])**2 +\n",
    "                (row['이용금액'] - c['mean_amount'])**2 +\n",
    "                (row['탈회횟수_누적'] - c['mean_churn'])**2\n",
    "            )\n",
    "            d.append(dist)\n",
    "        sorted_d = sorted(d)\n",
    "        dists.append(sorted_d[1] - sorted_d[0])  # 1위와 2위 군집 중심 거리 차이\n",
    "    return dists\n",
    "\n",
    "pdf['dist_diff'] = get_closest_two(pdf, centroids_pd)\n",
    "\n",
    "# 2. 경계 중첩 비율: 1위와 2위 군집 중심 거리 차이가 작은 데이터 비율\n",
    "threshold = 0.3  # 임계값 (실제 데이터에 맞게 조정)\n",
    "overlap_ratio = (pdf['dist_diff'] < threshold).mean()\n",
    "print(\"경계 중첩 비율:\", overlap_ratio)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "babbdc42-83da-40fa-be5c-88511d536550",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_final_with_pred = spark.table(\"database_pjt.df_final_with_pred\")\n",
    "df_final_with_pred.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bbc64cd8-784d-4c31-b272-1b778bc718d6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import mlflow.pytorch\n",
    "\n",
    "# 등록된 모델 이름과 버전\n",
    "model_name = \"Classification_Member_Info\"\n",
    "model_version = 1  # 필요시 변경\n",
    "\n",
    "# MLflow에서 모델 불러오기\n",
    "loaded_model = mlflow.pytorch.load_model(f\"models:/{model_name}/{model_version}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b75e1ea1-887f-4d18-9ce4-fb1a3b3ed967",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# 모델을 GPU로 이동 (GPU 사용 시)\n",
    "loaded_model = loaded_model.to('cuda')\n",
    "\n",
    "# 입력 데이터도 GPU로 이동\n",
    "pca_tensor = torch.from_numpy(pca_features_np).float().cuda()\n",
    "\n",
    "# 예측 수행\n",
    "predictions = loaded_model(pca_tensor)\n",
    "pred_np = predictions.cpu().numpy()  # numpy 배열로 변환\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a2ab6753-2f13-469b-bfda-162cf502776c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "\n",
    "# 발급회원번호 리스트 준비 (원본 데이터프레임에서 추출)\n",
    "member_ids = [row['발급회원번호'] for row in df_final_dedup.select('발급회원번호').collect()]\n",
    "\n",
    "# 예측 결과와 발급회원번호를 묶어서 Row 리스트 생성\n",
    "rows = [Row(발급회원번호=member_id, cluster=int(cluster)) for member_id, cluster in zip(member_ids, pred_np)]\n",
    "\n",
    "# Spark DataFrame으로 변환\n",
    "df_pred_cluster = spark.createDataFrame(rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "15b04e87-0b19-4ce9-88bd-26a31a50fa2f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler, PCA\n",
    "import numpy as np\n",
    "\n",
    "# 예시: 분석에 사용할 피처 리스트 (실제 컬럼명에 맞게 수정 필요)\n",
    "feature_cols = [\n",
    "    '입회경과개월수_신용', '탈회횟수_누적', '이용금액_R3M_신용', '이용금액_R3M_체크',\n",
    "    '_1순위카드이용금액', '_1순위카드이용건수', '_2순위카드이용금액', '_2순위카드이용건수',\n",
    "    '청구금액_기본연회비_B0M', '청구금액_제휴연회비_B0M', '카드신청건수', '최종카드발급경과월'\n",
    "]\n",
    "\n",
    "# 1. VectorAssembler로 피처 벡터 생성\n",
    "assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n",
    "df_features = assembler.transform(df_final_with_pred)\n",
    "\n",
    "# 2. PCA 모델 생성 (예: 3차원으로 축소)\n",
    "pca = PCA(k=3, inputCol=\"features\", outputCol=\"pca_features\")\n",
    "pca_model = pca.fit(df_features)\n",
    "df_pca = pca_model.transform(df_features)\n",
    "\n",
    "# 3. PCA 결과를 numpy 배열로 변환\n",
    "pca_features_np = np.array(df_pca.select(\"pca_features\").rdd.map(lambda row: row.toArray()).collect())\n",
    "\n",
    "# 4. pca_features_np 변수 생성 확인\n",
    "print(f\"pca_features_np shape: {pca_features_np.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b32b9dfa-f9a7-4d1a-80b4-e15d8eade766",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# 예시: 투자위험등급(등급이 있다면) 또는 군집 라벨을 타겟으로 사용\n",
    "# 만약 군집 라벨이 있다면, 'cluster' 컬럼을 타겟으로 사용\n",
    "features = [\n",
    "    '입회경과개월수_신용', '탈회횟수_누적', '이용금액_R3M_신용', '이용금액_R3M_체크',\n",
    "    '_1순위카드이용금액', '_1순위카드이용건수', '_2순위카드이용금액', '_2순위카드이용건수',\n",
    "    '청구금액_기본연회비_B0M', '청구금액_제휴연회비_B0M', '카드신청건수', '최종카드발급경과월'\n",
    "]\n",
    "\n",
    "assembler = VectorAssembler(inputCols=features, outputCol=\"features\")\n",
    "\n",
    "# 예시: 군집 라벨을 타겟으로 사용\n",
    "rf = RandomForestClassifier(featuresCol=\"features\", labelCol=\"cluster\")\n",
    "\n",
    "pipeline = Pipeline(stages=[assembler, rf])\n",
    "model = pipeline.fit(df_final_with_pred)\n",
    "\n",
    "# 피처 중요도 추출\n",
    "rfModel = model.stages[-1]\n",
    "importances = rfModel.featureImportances\n",
    "for i, (col, imp) in enumerate(zip(features, importances)):\n",
    "    print(f\"{i+1}. {col}: {imp:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "1.2_전처리후클러스터링",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
